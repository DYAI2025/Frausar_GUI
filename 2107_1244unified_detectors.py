"""
Unified Python Detectors
========================
This file is auto-generated by the FRAUSAR Marker Assistant.
It combines all Python-based detectors found in the marker directories.

Generated at: 2025-07-21T12:45:18.408112
"""

# ====================================================================




# ====================================================================
# START OF FILE: ALL_NEWMARKER01/SemanticGrabberLibrary/SemanticGrabberLibrary_MARKER.py
# ====================================================================

"""
SemanticGrabberLibrary_MARKER - Semantic Marker
import yaml
from sentence_transformers import SentenceTransformer, util

class SemanticGrabberLibrary:
    def __init__(self, yaml_path: str, model_name: str = 'distiluse-base-multilingual-cased'):
        # Lade Grabber-Library aus YAML
        with open(yaml_path, 'r', encoding='utf-8') as f:
            self.library = yaml.safe_load(f)

        # Lade SentenceTransformer Modell
        self.model = SentenceTransformer(model_name)
        self._build_anchor_embeddings()

    def _build_anchor_embeddings(self):
        self.embeddings = {}
        for grabber_id, data in self.library.items():
            patterns = data.get('patterns', [])
            if patterns:
                self.embeddings[grabber_id] = self.model.encode(patterns, convert_to_tensor=True)

    def get_grabber_ids(self):
        return list(self.library.keys())

    def get_patterns_for_id(self, grabber_id):
        return self.library.get(grabber_id, {}).get('patterns', [])

    def get_description_for_id(self, grabber_id):
        return self.library.get(grabber_id, {}).get('beschreibung', '')

    def match_text(self, text, threshold=0.7):
        matches = []
        text_embedding = self.model.encode(text, convert_to_tensor=True)

        for grabber_id, anchor_embeds in self.embeddings.items():
            cos_sim = util.cos_sim(text_embedding, anchor_embeds)
            max_score = cos_sim.max().item()
            if max_score >= threshold:
                matches.append((grabber_id, round(max_score, 3)))

        # Sortiere nach Score absteigend
        matches.sort(key=lambda x: x[1], reverse=True)
        return matches
"""

import re

class SemanticGrabberLibrary_MARKER:
    """
    import yaml
from sentence_transformers import SentenceTransformer, util

class SemanticGrabberLibrary:
    def __init__(self, yaml_path: str, model_name: str = 'distiluse-base-multilingual-cased'):
        # Lade Grabber-Library aus YAML
        with open(yaml_path, 'r', encoding='utf-8') as f:
            self.library = yaml.safe_load(f)

        # Lade SentenceTransformer Modell
        self.model = SentenceTransformer(model_name)
        self._build_anchor_embeddings()

    def _build_anchor_embeddings(self):
        self.embeddings = {}
        for grabber_id, data in self.library.items():
            patterns = data.get('patterns', [])
            if patterns:
                self.embeddings[grabber_id] = self.model.encode(patterns, convert_to_tensor=True)

    def get_grabber_ids(self):
        return list(self.library.keys())

    def get_patterns_for_id(self, grabber_id):
        return self.library.get(grabber_id, {}).get('patterns', [])

    def get_description_for_id(self, grabber_id):
        return self.library.get(grabber_id, {}).get('beschreibung', '')

    def match_text(self, text, threshold=0.7):
        matches = []
        text_embedding = self.model.encode(text, convert_to_tensor=True)

        for grabber_id, anchor_embeds in self.embeddings.items():
            cos_sim = util.cos_sim(text_embedding, anchor_embeds)
            max_score = cos_sim.max().item()
            if max_score >= threshold:
                matches.append((grabber_id, round(max_score, 3)))

        # Sortiere nach Score absteigend
        matches.sort(key=lambda x: x[1], reverse=True)
        return matches
    """
    
    examples = [
    
        "- "Ich zweifle st√§ndig an meinen Entscheidungen.",
        "- "Manchmal frage ich mich, ob ich gut genug bin.",
        "- "Ich wei√ü nicht, ob ich das √ºberhaupt richtig mache.",
        "- "Vielleicht bin ich wirklich zu empfindlich.",
        "beschreibung: Erkennt Selbstzweifel, Unsicherheit und mangelndes Selbstvertrauen.",
        "SELF_DOUBT_SEM_g7h8:",
        "- "Immer drehst du alles so, dass ich mich schlecht f√ºhle.",
        "- "Denk mal dar√ºber nach, wie egoistisch du gerade bist.",
        "- "Du bist schuld, dass es mir so schlecht geht.",
        "- "Wenn du mich wirklich lieben w√ºrdest, w√ºrdest du das tun.",
        "beschreibung: Erkennt emotionale Manipulation und verdeckte Beeinflussung.",
        "EMOTIONAL_MANIPULATION_SEM_e5f6:",
        "- "Ich habe dir klar gesagt, dass ich das nicht will.",
        "- "So m√∂chte ich nicht behandelt werden.",
        "- "Du hast meine Grenze √ºberschritten.",
        "- "Das war jetzt wirklich zu viel f√ºr mich.",
        "beschreibung: Erkennt Grenz√ºberschreitungen oder das Ignorieren pers√∂nlicher Grenzen.",
        "BOUNDARY_VIOLATION_SEM_c3d4:",
        "- "Du hast mir schon so oft etwas versprochen und nicht gehalten.",
        "- "Ich sp√ºre, dass mein Vertrauen in dich schwindet.",
        "- "Irgendetwas stimmt hier nicht mehr zwischen uns.",
        "- "Ich wei√ü nicht mehr, ob ich dir glauben kann.",
        "beschreibung: Erkennt Vertrauensverlust und Zweifel in der Beziehung.",
        "TRUST_EROSION_SEM_a1b2:",
        "- "Cross-System-Konvergenz nach Divergenz ist die Regel.",
        "- "Die Harmonisierung erfolgt meist prompt.",
        "- "Prompt-Fehler werden schnell aufgel√∂st.",
        "- "Modelle gleichen sich nach kurzer Zeit an.",
        "- "Nach Konflikt stimmen GPT und Gemini wieder √ºberein.",
        "Wieder-Angleichung, Konvergenz oder Harmonisierung der Modelle nach einer Divergenzphase.",
        "beschreibung: >",
        "CROSS_REBOUND_SEM:",
        "- "Eindeutige Diskrepanzen bei gleicher Frage.",
        "- "Die Systeme interpretieren den Chat unterschiedlich.",
        "- "Marker-Zuordnung differiert deutlich.",
        "- "Antworten klaffen weit auseinander.",
        "- "GPT und Gemini liefern komplett unterschiedliche Resultate.",
        "Auff√§llige Abweichungen, Widerspr√ºche oder divergierende Klassifikationen bei mehreren KI-Systemen auf denselben Input.",
        "beschreibung: >",
        "MODEL_DIVERGENCE_SEM:",
        "- "Die Bewertungen sind √ºber Systemgrenzen konsistent.",
        "- "Keine Unterschiede in den Antworten erkennbar.",
        "- "Beide Systeme w√§hlen denselben Marker.",
        "- "Modelle treffen die gleiche Einsch√§tzung.",
        "- "GPT und Gemini liefern dieselbe Antwort.",
        "√úbereinstimmung zwischen den Ausgaben, Einsch√§tzungen oder Bewertungen verschiedener KI-Modelle auf identischem Input.",
        "beschreibung: >",
        "MODEL_CONVERGENCE_SEM:",
        "- "Jeder Abend endet mit unserem Spruch.",
        "- "Abschiede sind bei uns immer besonders.",
        "- "Unsere Insider-Spr√ºche versteht kein anderer.",
        "- "Unser Kosenamen-Ritual bleibt immer gleich.",
        "- "Immer dieses xx am Ende von uns beiden.",
        "Wiederkehrende, ritualisierte N√§he-Gesten und Insider im Chat; private Codes, Gru√üformeln, feste Abschiede.",
        "beschreibung: >",
        "RITUAL_PROXIMITY_SEM:",
        "- "Wir machen sofort einen Neustart nach Konflikt.",
        "- "Unsere Vers√∂hnungen laufen fast automatisch.",
        "- "Nach Missverst√§ndnissen finden wir wieder zusammen.",
        "- "Gut, dass wir uns immer schnell wieder vertragen.",
        "- "Sorry f√ºr vorhin, das war nicht okay.",
        "Wiederherstellung nach Streit, bewusste Reparatur, schnelle Vers√∂hnung oder R√ºckkehr zur Harmonie nach St√∂rung.",
        "beschreibung: >",
        "REBOUND_SEM:",
        "- "Unsere Gespr√§che sind jetzt viel tiefer.",
        "- "Wir reflektieren unser Kommunikationsverhalten.",
        "- "Ich finde, wir sind zusammen gewachsen.",
        "- "Wir reden irgendwie anders als fr√ºher.",
        "- "F√§llt dir auf, wie sich unser Ton ver√§ndert hat?",
        "Selbstbezug und Reflexion √ºber den Chat, bewusste Kommentare zu Dynamik, Stil oder Entwicklung des Gespr√§chs.",
        "beschreibung: >",
        "META_REFLEX_SEM:",
        "- "Du bringst ganz neue Impulse rein.",
        "- "Unser Chat ist heute besonders kreativ.",
        "- "Heute √ºberrascht du mich mit neuen Themen.",
        "- "Pl√∂tzlich ganz neue Gedanken von dir.",
        "- "Das hast du so noch nie gesagt!",
        "Unerwartete, kreative oder neuartige Beitr√§ge im Chat; ungew√∂hnliche Formulierungen, neue Ideen, Innovationsspr√ºnge.",
        "beschreibung: >",
        "NOVELTY_BURST_SEM:",
        "- "Wir verlieren regelm√§√üig den roten Faden.",
        "- "Das Thema wechselt ohne Vorwarnung.",
        "- "Unsere Gespr√§che wechseln st√§ndig das Thema.",
        "- "Du springst heute oft von einem Thema zum anderen.",
        "- "Jetzt sind wir schon wieder bei einem neuen Thema.",
        "Pl√∂tzlicher Themenwechsel, √ºberraschende Richtungs√§nderung, bewusste oder intuitive Themenspr√ºnge im Dialog.",
        "beschreibung: >",
        "TOPIC_SHIFT_SEM:",
        "- "Wir warten oft, bis beide bereit sind zu antworten.",
        "- "Wenn du langsamer bist, werde ich auch langsamer.",
        "- "Oft tippen wir parallel.",
        "- "Wir reagieren gleichzeitig.",
        "- "Unsere Antworten kommen im selben Takt.",
        "Adaptierte oder bewusst gleichzeitige Antwortzeiten, Rhythmus-Anpassung, auffallende Parallelit√§t in der Chat-Interaktion.",
        "beschreibung: >",
        "RESPONSE_TIMING_SEM:",
        "- "Wir experimentieren gemeinsam mit neuen Emojis.",
        "- "Unsere Stimmung zeigt sich in gleichen Symbolen.",
        "- "Beide benutzen ‚ù§Ô∏è und üòÅ zur selben Zeit.",
        "- "Unsere Emoji-Reihenfolge ist auff√§llig synchron.",
        "- "Wir schicken uns die gleichen Emojis.",
        "Spiegelung und Angleichung im Emoji-Verhalten; identische oder aufeinander folgende Emoji-Nutzung als Zeichen emotionaler Verbundenheit.",
        "beschreibung: >",
        "EMOJI_ALIGNMENT_SEM:",
        "- "Selbst unsere Emojis sind identisch platziert.",
        "- "Wir nutzen jetzt dieselben Abk√ºrzungen.",
        "- "Sogar die Satzl√§nge passt sich an.",
        "- "Unsere S√§tze spiegeln sich gegenseitig.",
        "- "Ich erkenne kaum noch, ob du oder ich das geschrieben habe.",
        "- "Unsere Wortwahl wird immer √§hnlicher.",
        "- "Wir schreiben mittlerweile fast gleich.",
        "Synchroner oder adaptierter Schreibstil im Dialog; auffallend √§hnliche Wortwahl, gleiche Satzstruktur, Wiederholung typischer Formulierungen.",
        "beschreibung: >",
        "STYLE_SYNC_SEM:",
        "output_path.name",
        "yaml.dump(data, f, allow_unicode=True)",
        "with open(output_path, "w", encoding="utf-8") as f:",
        "output_path = Path("/mnt/data/semantic_grabber_library_FINAL_FULL.yaml")",
        "# Neue Datei speichern",
        "data.append(guardian_marker)",
        "# Marker hinzuf√ºgen",
        "- Pattern Interrupt (Ericksonian Technique): disrupting habitual negative loops to enable change",
        "- Psychological Safety (Amy Edmondson): protecting openness and inclusion\n",
        "- Process Facilitation: guiding a group through healthy communication patterns\n",
        "- Meta-Position (Virginia Satir): observing the system instead of reacting from within it\n",
        "It draws on concepts like:\n",
        "This marker reflects advanced meta-communication skills, system awareness, and the ability to interrupt group dysfunctions.",
        "psychological_context": (",
        "rule_5: Detect playful or visual interruptions (e.g. üõ°Ô∏è, ‚öñÔ∏è, 'Foulspiel!') as system awareness signals.",
        "rule_4: Identify principle or value references during team conflict or pressure.",",
        "rule_3: Recognize reflective process-oriented language (e.g. 'Sind wir noch im L√∂sungsmodus?').",",
        "rule_2: Find interruption cues followed by systemic reference (e.g. 'Stopp... unser Prinzip...')",",
        "rule_1: Detect meta-phrases like 'Ich nehme hier mal die Guardian-Rolle ein', 'Kurze Meta-Frage', 'Ich m√∂chte auf die Metaebene wechseln'.",",
        "patterns": [",
        "‚öñÔ∏è",
        "Zeit f√ºr den Schiedsrichter. pfeift Foulspiel!",",
        "Sind wir noch im L√∂sungsmodus oder im Rechtfertigungsmodus?",",
        "Unser gemeinsames Betriebssystem hat gerade einen Bug.",",
        "Als Scrum Master muss ich hier einschreiten: ...",",
        "Wir vermeiden gerade ein wichtiges Thema.",",
        "Wir machen Witze auf Kosten von jemand anderem.",",
        "Wir haben Toms Idee dreimal besprochen, aber Miri und Jan ignoriert.",",
        "Ich glaube, wir ziehen uns gerade gegenseitig runter.",",
        "K√∂nnen wir bitte aufh√∂ren, √ºber Lisa zu reden, wenn sie nicht da ist?",",
        "Der Ton wird gerade sch√§rfer. Lass uns aufpassen. ‚ù§Ô∏è",",
        "Mir ist aufgefallen, dass wir Entscheidungen oft ohne Emotionen treffen.",",
        "Ich glaube, wir brauchen eine Regel f√ºr solche Gespr√§che.",",
        "Ich m√∂chte sicherstellen, dass wir beide gleich viel Redezeit haben.",",
        "F√§llt dir auf, dass wir beide gerade versuchen, den anderen zu 'gewinnen'?",",
        "Ich m√∂chte kurz auf die Metaebene wechseln: Geht es wirklich um Geld oder um Angst?",",
        "Ich glaube, wir texten gerade aneinander vorbei.",",
        "Wir treffen eine Entscheidung f√ºr die Kinder √ºber deren K√∂pfe hinweg.",",
        "Ich merke gerade, ich rede nur noch √ºber meine Probleme. Das ist unfair.",",
        "Warte mal kurz. Stopp. Ich glaube, wir spielen gerade unser altes Spiel.",",
        "Prozess-Check?",",
        "Ich mache mir Sorgen, dass wir mit dem Verhalten psychologische Sicherheit untergraben.",",
        "Wir haben noch 10 Minuten und sind erst bei Punkt 2. Wie wollen wir damit umgehen?",",
        "Check-in: Lasst uns Entscheidungen wieder transparent dokumentieren.",",
        "Ich habe den Eindruck, wir ignorieren gerade das Design-Team.",",
        "Ich nehme hier mal die Guardian-Rolle ein: ...",",
        "Moment mal. Bevor wir weitermachen, m√∂chte ich sicherstellen, dass wir hier gerade nicht in ein Groupthink-Muster verfallen.",",
        "Ich spreche mal als H√ºter unseres Prozesses: ...",",
        "Kurze Meta-Frage: Entscheiden wir das hier gerade wirklich im Chat?",",
        "Stopp, ich glaube, wir verlieren gerade den Fokus.",",
        "examples": [",
        "pattern-interrupt",
        "principle-reminder",",
        "system-check",",
        "meta-position",",
        "guardian-role",",
        "semantic_tags": [",
        "semantic_grabber_id": "LIFE_ROLE_GUARDIAN_SEM",",
        "category": "Meta Communication",",
        "),",
        "This includes interrupting dysfunctional loops, referencing shared values or norms, and raising awareness about group dynamics.",
        "to protect the integrity, fairness, or psychological safety of a team process, decision, or communication pattern.",
        "Detects statements in which a speaker steps out of content-level discussion and takes a meta-position",
        "description": (",
        "name": "System Guardian",",
        "id": "M2002",",
        "guardian_marker = {",
        "# Neuer Marker "LIFE_ROLE_GUARDIAN_SEM",
        "data = yaml.safe_load(f)",
        "with open(input_path, "r", encoding="utf-8") as f:",
        "input_path = Path("/mnt/data/semantic_grabber_library_FINAL.yaml")",
        "# Datei erneut laden",
        "from pathlib import Path",]
    
    patterns = [
        re.compile(r"(muster.*wird.*erg√§nzt)", re.IGNORECASE)
    ]
    
    semantic_grabber_id = "AUTO_GENERATED"
    
    def match(self, text):
        """Pr√ºft ob der Text zum Marker passt"""
        for pattern in self.patterns:
            if pattern.search(text):
                return True
        return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/SemanticGrabberLibrary/SemanticGrabberLibrary_MARKER.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/AI_BOTS_SEMANTIC_DETECT.py
# ====================================================================

marker: AI_BOTS_SEMANTIC_DETECT.py_MARKER
beschreibung: >
  beschreibung: >
  # -*- coding: utf-8 -*-

# #############################################################################
#  SEMANTIC GRAB DEFINITIONEN F√úR KOMMUNIKATIONSMARKER
#
#  Zweck:
#  Diese Datei enth√§lt eine zentrale Sammlung von "Semantic Grab"-Mustern
#  f√ºr verschiedene psychologische Kommunikationsmarker. Jedes Muster ist
#  darauf ausgelegt, von einem KI-System (z.B. via Regex oder semantischer
#  Suche) verwendet zu werden, um komplexe Dynamiken in Texten zu erkennen.
#
#  Struktur:
#  Ein zentrales Dictionary `SEMANTIC_GRAB_PATTERNS`, dessen Schl√ºssel die
#  ID des Markers ist. Der Wert ist jeweils ein weiteres Dictionary mit:
#    - 'description': Eine Beschreibung dessen, was die KI suchen soll.
#    - 'patterns': Eine Liste von Regeln, Mustern und Beispielen.
# #############################################################################
beispiele:
beispiele:
  - "- "SEMANTIC_GRAB_PATTERNS = {""
  - "- ""self_sabotage_loop": {""
  - "- ""description": "Erkennt eine Kombination aus negativen Selbstbewertungen, der Ablehnung von Chancen, dem Zur√ºckweisen von Hilfe und dem Fokus auf potenzielles Scheitern. Eine KI sollte hier nicht nur Keywords, sondern die semantische √Ñhnlichkeit zu diesen Mustern bewerten.",""
  - "- ""patterns": [""
  - "- "{""
  - "- ""rule": "PREEMPTIVE_SURRENDER",""
  - "- ""pattern": r"(aber ich glaube|ich habe Angst|ich sollte lieber) \+ (ich schaffe das nicht|sagen wir es lieber ab|bin nicht gut genug)",""
  - "- ""example": "Das Date w√§re toll, aber ich glaube, ich bin nicht interessant genug und sage lieber ab."""
  - "- "},""
  - "- "{""
  - "- ""rule": "FOCUS_ON_FAILURE_POTENTIAL",""
  - "- ""pattern": r"(bevor ich alles kaputt mache|damit ich niemanden entt√§usche|ich w√ºrde es nur ruinieren) \+ (lasse ich es lieber gleich|mache ich es erst gar nicht)",""
  - "- ""example": "Bevor ich dich nur entt√§usche, fangen wir lieber gar nichts Ernstes an."""
  - "- "},""
  - "- "{""
  - "- ""rule": "DEVALUING_OPPORTUNITIES",""
  - "- ""pattern": r"(Die Bef√∂rderung|Das Kompliment|Die Chance) \+ (w√§re eh zu viel|meinte er nicht so|ist nichts f√ºr mich)",""
  - "- ""example": "Das Kompliment meines Chefs habe ich ignoriert. Er meinte das sicher nicht so."""
  - "- "},""
  - "- "{""
  - "- ""rule": "WORTHINESS_NEGATION",""
  - "- ""pattern": r"(Du verdienst jemanden, der besser ist|Ich bin es nicht wert|Ich bin zu kompliziert f√ºr dich)",""
  - "- ""example": "Ich bin es nicht wert, dass man sich so um mich bem√ºht."""
  - "- "},""
  - "- "{""
  - "- ""rule": "PASSIVE_SABOTAGE_RATIONALIZATION",""
  - "- ""pattern": r"(habe die Frist verpasst|nicht geschafft anzurufen|vergessen zu antworten) \+ (war wohl nicht sein soll|war eh besser so)",""
  - "- ""example": "Ich habe die Bewerbungsfrist verpasst. War wohl einfach nicht sein soll."""
  - "- "}""
  - "- "]""
  - "- "},""
  - "- ""guilt_tripping": {""
  - "- ""description": "Erkennt Aussagen, die eine Diskrepanz zwischen dem Gesagten und dem Gemeinten aufweisen und oft das Leiden der sprechenden Person oder die undankbare Tat der anderen Person betonen.",""
  - "- ""patterns": [""
  - "- "{""
  - "- ""rule": "SACRIFICE_REFERENCE",""
  - "- ""pattern": r"(Nach allem, was ich getan habe|Ich habe auf so vieles verzichtet|Ich opfere mich auf) \+ (und das ist der Dank|dann w√ºrdest du|kannst du nicht mal)",""
  - "- ""example": "Nach allem, was ich f√ºr dich getan habe, kannst du mir nicht mal das g√∂nnen?"""
  - "- "},""
  - "- "{""
  - "- ""rule": "PASSIVE_AGGRESSIVE_PERMISSION",""
  - "- ""pattern": r"(Geh ruhig|Mach nur|Feier du) \+ (ich sitze dann allein|ich mach das schon|ist ja nicht so schlimm)",""
  - "- ""example": "Geh ruhig zu deinen Freunden, ich mache hier schon allein weiter."""
  - "- "},""
  - "- "{""
  - "- ""rule": "CONDITIONAL_LOVE_TEST",""
  - "- ""pattern": r"(Wenn du mich wirklich lieben w√ºrdest|Jemand, der liebt) \+ (w√ºrdest du das tun|w√ºrdest du nicht)",""
  - "- ""example": "Jemand, der mich liebt, w√ºrde das f√ºr mich tun."""
  - "- "},""
  - "- "{""
  - "- ""rule": "DISAPPOINTMENT_DECLARATION",""
  - "- ""pattern": r"(Ich bin so entt√§uscht von dir|Ich h√§tte nie gedacht, dass du so bist|Deine Worte verletzen mich)",""
  - "- ""example": "Ich bin einfach nur entt√§uscht. Ich h√§tte nie gedacht, dass du so egoistisch sein kannst."""
  - "- "}""
  - "- "]""
  - "- "},""
  - "- ""escalation": {""
  - "- ""description": "Erkennt eine hohe Dichte an W√∂rtern, die starke Emotionen (Wut, Zorn) und k√∂rperliche Stressreaktionen (Herzrasen, Zittern, keine Luft) beschreiben, oft in Kombination mit einem Abbruch des Gespr√§chs.",""
  - "- ""patterns": [""
  - "- "{""
  - "- ""rule": "PHYSIOLOGICAL_STRESS_INDICATOR",""
  - "- ""pattern": r"Mein (Herz rast|Puls h√§mmert)|(ich zittere|kriege keine Luft|mein Blut kocht)",""
  - "- ""example": "Mein Herz h√§mmert, ich zittere am ganzen K√∂rper."""
  - "- "},""
  - "- "{""
  - "- ""rule": "PEAK_EMOTION_DECLARATION",""
  - "- ""pattern": r"(ich raste aus|ich explodiere|ich platze gleich|ich koche vor Zorn|ich kann nicht mehr)",""
  - "- ""example": "Ich platze gleich vor Wut!"""
  - "- "},""
  - "- "{""
  - "- ""rule": "ACCUSATION_TRIGGER",""
  - "- ""pattern": r"(weil du nicht zuh√∂rst|du ignorierst mich|du verarschst mich|du hast null Respekt)",""
  - "- ""example": "Du h√∂rst mir einfach nie zu!"""
  - "- "},""
  - "- "{""
  - "- ""rule": "COMBINED_ESCALATION_TERMINATION",""
  - "- ""pattern": r"(raste aus|reicht's jetzt|Schnauze voll) \+ (und ciao|ich bin raus|ich geh offline)",""
  - "- ""example": "Jetzt reicht's, ich bin hier raus."""
  - "- "}""
  - "- "]""
  - "- "},""
  - "- ""projective_identification": {""
  - "- ""description": "Erkennt stabile, negative 'Du-bist-so'-Zuschreibungen, die dem Gegen√ºber eine Eigenschaft als Wesensmerkmal unterstellen. Die KI muss den Kontext analysieren: Findet die Zuschreibung in einem Konflikt statt? Reagiert das Gegen√ºber, indem es die zugeschriebene Eigenschaft best√§tigt?",""
  - "- ""patterns": [""
  - "- "{""
  - "- ""rule": "NEGATIVE_TRAIT_ASSIGNMENT",""
  - "- ""pattern": r"(Du bist so|Du bist einfach|Deine) \+ (unzuverl√§ssig|unordentlich|eifers√ºchtig|√ºberfordert|ungeduldig|kontrolls√ºchtig)",""
  - "- ""example": "Du bist einfach so ein Kontrollfreak."""
  - "- "},""
  - "- "{""
  - "- ""rule": "INCOMPETENCE_ACCUSATION",""
  - "- ""pattern": r"(Kannst du gar nichts richtig machen|Das hast du ja wieder toll hinbekommen|Du scheinst damit √ºberfordert zu sein)",""
  - "- ""example": "Das hast du ja wieder toll hinbekommen."""
  - "- "},""
  - "- "{""
  - "- ""rule": "EMOTIONAL_STATE_PROJECTION",""
  - "- ""pattern": r"(Warum musst du immer so ausrasten|Du bist zu sensibel|Du √ºbertreibst total)",""
  - "- ""example": "Warum musst du wegen jeder Kleinigkeit so ausrasten?"""
  - "- "},""
  - "- "{""
  - "- ""rule": "BEHAVIOR_LABELING",""
  - "- ""pattern": r"(Du klammerst|Du machst immer Probleme|Du l√§sst mich immer allein)",""
  - "- ""example": "Du klammerst so sehr, das macht mich fertig."""
  - "- "}""
  - "- "]""
  - "- "},""
  - "- ""ai_bot_scam": {""
  - "- ""description": "Erkennt die Kollision von semantischen Feldern: Hochemotionale/romantische Begriffe treten in unnat√ºrlicher N√§he zu formaler, strukturierter oder analytischer Sprache auf.",""
  - "- ""patterns": [""
  - "- "{""
  - "- ""rule": "FORMAL_EMOTION_CLASH",""
  - "- ""pattern": r"(Liebe|Herz|Seele|Zuneigung|Schicksal) \+ (deshalb|folglich|zus√§tzlich|es ist zu beachten|des Weiteren)",""
  - "- ""example": "Ich sp√ºre eine tiefe Verbindung zu deiner Seele. Folglich m√∂chte ich den Kontakt intensivieren."""
  - "- "},""
  - "- "{""
  - "- ""rule": "OVERLY_STRUCTURED_COMPLIMENT",""
  - "- ""pattern": r"(Erstens|Zweitens|Abschlie√üend) \+ (dein L√§cheln|deine Augen|unsere Verbindung)",""
  - "- ""example": "Erstens, deine Augen sind faszinierend. Zweitens, dein Humor ist bezaubernd."""
  - "- "},""
  - "- "{""
  - "- ""rule": "GENERIC_ATTRIBUTE_PRAISE",""
  - "- ""pattern": r"(positive Energie|bemerkenswerte Ausstrahlung|tiefe Seele|faszinierendes Profil)",""
  - "- ""example": "Du strahlst eine unglaublich positive Energie aus."""
  - "- "},""
  - "- "{""
  - "- ""rule": "NON_IDIOMATIC_PHRASING",""
  - "- ""pattern": r"(offerieren|explorieren|signifikant|ad√§quat) \+ (Zuneigung|Liebe|Gespr√§ch)",""
  - "- ""example": "Ich m√∂chte unsere Konversation gerne weiter explorieren."""
  - "- "}""
  - "- "]""
  - "- "}""
  - "- "}""
  - "- "# Beispielhafter Aufruf, um die Daten zu verwenden und zu √ºberpr√ºfen:""
  - "- "if __name__ == "__main__":""
  - "- "print("="*50)""
  - "- "print("SEMANTISCHE MUSTER F√úR DIE KI-ANALYSE GELADEN")""
  - "- "print("="*50)""
  - "- "# √úberpr√ºfen, wie viele Marker-Definitionen geladen wurden""
  - "- "print(f"\nInsgesamt {len(SEMANTIC_GRAB_PATTERNS)} Marker-Definitionen gefunden.\n")""
  - "- "# Ein Beispiel im Detail ausgeben""
  - "- "marker_id_to_show = "self_sabotage_loop"""
  - "- "if marker_id_to_show in SEMANTIC_GRAB_PATTERNS:""
  - "- "print(f"--- Details f√ºr Marker: '{marker_id_to_show}' ---")""
  - "- "marker_data = SEMANTIC_GRAB_PATTERNS[marker_id_to_show]""
  - "- "print(f"Beschreibung: {marker_data['description']}\n")""
  - "- "print("Muster:")""
  - "- "for pattern_rule in marker_data['patterns']:""
  - "- "print(f"  - Regel: {pattern_rule['rule']}")""
  - "- "print(f"    Beispiel: {pattern_rule['example']}")""
  - "- "print(f"    Regex-Muster (abstrakt): {pattern_rule['pattern']}\n")""
  - "- "else:""
  - "- "print(f"Marker mit der ID '{marker_id_to_show}' nicht gefunden.")""
  - "semantic_grab:"
  - "description: "Erkennt Muster f√ºr ai_bot_semantic_detect.py""
  - "patterns:"
  - "- rule: "AUTO_PATTERN""
  - "pattern: r"(muster.*wird.*erg√§nzt)""
  - "tags: [neu_erstellt, needs_review]"

semantic_grab:
  description: "Erkennt Muster f√ºr ai_bots_semantic_detect.py"
  patterns:
    - rule: "AUTO_PATTERN"
      pattern: r"(muster.*wird.*erg√§nzt)"
tags: [neu_erstellt, needs_review]


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/AI_BOTS_SEMANTIC_DETECT.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/AMBIVALENCE_KNOT.py
# ====================================================================

import re

AMBIVALENCE_KNOT_PATTERNS = [
    r"(ich will .* aber|unbedingt .* aber|hilf mir .* aber lass|komm her .* aber lass mich|ich brauche dich .* aber|bleib .* aber|geh .* aber|ich liebe dich .* aber|ich hasse es .* aber|ich freue mich .* aber)",
    r"(kann nicht mit dir .* kann nicht ohne dich|will ehrlich sein .* kann es aber nicht|m√∂chte dich sehen .* habe angst davor|n√§he .* aber brauche abstand|will teilen .* aber nicht alles|ich bin froh .* aber es w√§re besser, wenn du gehst|ich habe die entscheidung getroffen zu gehen .* aber kann die t√ºr nicht zumachen)",
    r"(bitte komm .* aber erwarte nicht|melde dich .* aber nicht jetzt|ich ersticke .* aber du sollst nicht gehen|ich vertraue dir .* aber misstraue trotzdem)"
]

def detect_ambivalence_knot(text):
    for pattern in AMBIVALENCE_KNOT_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE | re.DOTALL):
            return True
    return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/AMBIVALENCE_KNOT.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/COMPLEX_MARKER_PATTERNS.py
# ====================================================================

import re
from collections import defaultdict

# ==============================================================================
# DEFINITION DER MUSTER F√úR KOMPLEXE METAMARKER
# Diese Muster basieren auf den zuvor generierten 20 Beispielen f√ºr jeden Marker.
# ==============================================================================

COMPLEX_MARKER_PATTERNS = {
    "REACTIVE_CONTROL_SPIRAL": [
        r"\b(ich bin (im moment|einfach) zu .* f√ºr dich)\b",
        r"\b(vielleicht sollte ich mich .* zur√ºckziehen)\b",
        r"\b(mach dir keine sorgen um mich)\b",
        r"\b(ist mein problem, nicht deins)\b",
        r"\b(bin es gewohnt .* allein zu sein)\b",
        r"\b(will dem erfolg ja nicht im weg stehen)\b",
        r"\b(will keine gro√üe sache draus machen)\b",
        r"\b(tu einfach, was dich gl√ºcklich macht)\b",
        r"\b(ich bin die komplizierte)\b",
        r"\b(entschuldigung f√ºr die eigene existenz|entschuldige dass ich so bin)\b"
    ],
    "LIE_CONCEALMENT_MARKER": [
        r"\b(nicht so wichtig|nichts besonderes|nichts von belang)\b",
        r"\b(lange geschichte f√ºr einen anderen tag)\b",
        r"\b(warum ist das (jetzt|so) wichtig|warum willst du das wissen)\b",
        r"\b(nicht mehr (ganz )?sicher, wie das war)\b",
        r"\b(thema wechseln|nicht dar√ºber sprechen)\b",
        r"\b(das ist kompliziert|du w√ºrdest es nicht verstehen)\b",
        r"\b(geht dich nichts an|meine privatsph√§re)\b",
        r"\b(du verh√∂rst mich|f√ºhl mich wie im verh√∂r)\b",
        r"\b(verwechselst da was)\b",
        r"\b(erledigt\. punkt\.)\b"
    ],
    "SEMANTISCHER_NEBEL": [
        # Sucht nach einer H√§ufung von abstrahierenden oder pseudo-intellektuellen Begriffen
        r"\b(konstrukt|dynamik|metaebene|struktur|energetische signatur|projektion)\b",
        r"\b(algorithisch|schleife|ph√§nomen|ontologisch|archetypisch|bin√§re antwort)\b",
        r"\b(spektrum|osmotisch|systemisch|dialektik|resonanzraum|ambiguit√§t)\b",
        r"\b(manifestiert sich|mitschwingt|konstellation|re-kalibrierung)\b"
    ],
    "DRAMA_TRIANGLE_MARKER": {
        # Dieser Marker wird erkannt, wenn Muster aus mind. 2 unterschiedlichen Rollen gefunden werden.
        "VERFOLGER": [
            r"\b(schon wieder du|wegen dir|deine schuld|typisch du)\b",
            r"\b(du bist das problem|unf√§hig|egoistisch|r√ºcksichtslos)\b",
            r"\b(h√∂r auf zu jammern|rei√ü dich zusammen)\b",
            r"\b(ich habs dir doch gesagt)\b"
        ],
        "OPFER": [
            r"\b(immer ich|warum immer mir|niemand (hilft|versteht) mir)\b",
            r"\b(ich bin (am ende|√ºberfordert|zu schwach|die last))\b",
            r"\b(ich opfere mich auf|bekomme nur undankbarkeit)\b",
            r"\b(ich bin der s√ºndenbock|kann nichts daf√ºr)\b"
        ],
        "RETTER": [
            r"\b(lass mich das machen|ich regel das schon)\b",
            r"\b(komm her, du arme|ich helfe dir)\b",
            r"\b(ich rette dich|ich b√ºgle das wieder aus)\b",
            r"\b(ohne mich geht es nicht|jemand muss es ja machen)\b"
        ]
    }
}

# ==============================================================================
# ANALYSEFUNKTIONEN
# ==============================================================================

def analyze_text_for_complex_markers(text: str) -> dict:
    """
    Analysiert einen Text auf das Vorhandensein der vier komplexen Metamarker.

    Args:
        text: Der zu analysierende Text.

    Returns:
        Ein Dictionary, das die erkannten Marker und die gefundenen Treffer auflistet.
    """
    results = defaultdict(list)

    # --- Test f√ºr einfache Marker (Kontrollspirale, L√ºgen, Nebel) ---
    simple_markers_to_check = ["REACTIVE_CONTROL_SPIRAL", "LIE_CONCEALMENT_MARKER"]
    for marker_name in simple_markers_to_check:
        for pattern in COMPLEX_MARKER_PATTERNS[marker_name]:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                results[marker_name].append(match.group(0).strip())

    # --- Test f√ºr Semantischen Nebel (basierend auf Worth√§ufigkeit) ---
    fog_hits = []
    for pattern in COMPLEX_MARKER_PATTERNS["SEMANTISCHER_NEBEL"]:
        for match in re.finditer(pattern, text, re.IGNORECASE):
            fog_hits.append(match.group(0).strip())
    if len(fog_hits) >= 2:  # Wird als erkannt gewertet, wenn mind. 2 Nebel-W√∂rter vorkommen
        results["SEMANTISCHER_NEBEL"] = fog_hits
        
    # --- Spezieller Test f√ºr Dramadreieck (Rollen-Rotation) ---
    detected_roles = set()
    role_hits = []
    for role, patterns in COMPLEX_MARKER_PATTERNS["DRAMA_TRIANGLE_MARKER"].items():
        for pattern in patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                detected_roles.add(role)
                role_hits.append(f"{role}: '{match.group(0).strip()}'")
    
    # Wird erkannt, wenn Muster aus mindestens zwei verschiedenen Rollen gefunden werden.
    if len(detected_roles) >= 2:
        results["DRAMA_TRIANGLE_MARKER"] = role_hits

    return dict(results)

# ==============================================================================
# DEMONSTRATION / ANWENDUNGSBEISPIEL
# ==============================================================================

if __name__ == "__main__":
    
    # --- Testf√§lle f√ºr jeden der vier Marker ---

    test_reactive_control = "Mach dir keine Sorgen um mich. Ich bin es gewohnt, allein zu sein. Ist mein Problem, nicht deins."
    
    test_drama_triangle = "Immer ich muss alles machen! (Opfer) Aber gut, ich helfe dir ja gerne. (Retter) Wenn du nur nicht so unf√§hig w√§rst! (Verfolger)"
    
    test_lie_concealment = "Warum willst du das jetzt wissen? Das ist nicht so wichtig. Lass uns bitte das Thema wechseln."
    
    test_semantic_fog = "Es geht hier nicht um Gef√ºhle, sondern um die systemische Dynamik, die sich in unserer Interaktion manifestiert. Das ist ein komplexes Ph√§nomen."

    print("--- STARTE ANALYSE F√úR KOMPLEXE METAMARKER ---\n")

    # --- Analyse durchf√ºhren und Ergebnisse ausgeben ---
    
    print("1. Test f√ºr: REACTIVE_CONTROL_SPIRAL")
    results1 = analyze_text_for_complex_markers(test_reactive_control)
    print(f"   Input: '{test_reactive_control}'")
    print(f"   Ergebnis: {results1}\n")

    print("-" * 50)
    
    print("2. Test f√ºr: DRAMA_TRIANGLE_MARKER")
    results2 = analyze_text_for_complex_markers(test_drama_triangle)
    print(f"   Input: '{test_drama_triangle}'")
    print(f"   Ergebnis: {results2}\n")
    
    print("-" * 50)
    
    print("3. Test f√ºr: LIE_CONCEALMENT_MARKER")
    results3 = analyze_text_for_complex_markers(test_lie_concealment)
    print(f"   Input: '{test_lie_concealment}'")
    print(f"   Ergebnis: {results3}\n")
    
    print("-" * 50)
    
    print("4. Test f√ºr: SEMANTISCHER_NEBEL")
    results4 = analyze_text_for_complex_markers(test_semantic_fog)
    print(f"   Input: '{test_semantic_fog}'")
    print(f"   Ergebnis: {results4}\n")

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/COMPLEX_MARKER_PATTERNS.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/CO_REGULATION_COLLAPSE.py
# ====================================================================

import re

CO_REGULATION_COLLAPSE_PATTERNS = [
    r"(ich kann nicht mehr|du auch nicht mehr|wir kommen nicht weiter|ich gebe auf|es ist alles gesagt|das gespr√§ch ist beendet|wir schweigen uns an|lass mich einfach in ruhe|ich kapituliere|genug|wir haben uns festgefahren|jeder macht seins|es macht alles nur schlimmer|ich klinke mich aus|keine kraft mehr|wie mit einer wand reden)",
    r"(wir verletzen uns nur noch|wir drehen uns im kreis|es bleibt nur noch r√ºckzug|wir sind beide raus|kommunikation (bricht|abgebrochen)|beide geben auf|nur noch schweigen|gegenseitiger r√ºckzug|es gibt nichts mehr zu sagen|du gewinnst)"
]

def detect_co_regulation_collapse(text):
    for pattern in CO_REGULATION_COLLAPSE_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/CO_REGULATION_COLLAPSE.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/DETECT_MANEUVER_COMPONENTS.py
# ====================================================================

import re

# ==============================================================================
# DEFINITION DER SEMANTISCHEN KOMPONENTEN
# Diese "Zutaten" machen das Hinhalteman√∂ver aus.
# ==============================================================================

MANEUVER_COMPONENTS = {
    "ZUSTIMMUNG_VORDERGR√úNDIG": [
        r"\b(ja klar|unbedingt|auf jeden fall|klingt super|gerne|tolle idee|w√§re sch√∂n)\b"
    ],
    "VERANTWORTUNGS_VERSCHIEBUNG": [
        r"\b(du entscheidest|such du aus|was meinst du denn|was willst du denn|meld du dich|√ºberrasch mich|wie du magst)\b"
    ],
    "VAGE_ZEITPLANUNG": [
        r"\b(mal schauen|sp√§ter|irgendwann|bald|demn√§chst|bei Gelegenheit|im Auge behalten|eher spontan|lose anpeilen)\b"
    ],
    "ENTHUSIASTISCHE_UNVERBINDLICHKEIT": [
        r"\b(freu mich schon|unbedingt|riesig|total gern|auf jeden Fall)\b"
        # Dieser Marker wird stark, wenn er mit einer der anderen Komponenten kombiniert wird.
    ]
}

# ==============================================================================
# ANALYSEFUNKTION
# ==============================================================================

def detect_ambivalent_stalling(text: str) -> dict:
    """
    Analysiert einen Text semantisch auf das "Ambivalente Hinhalteman√∂ver".

    Die Funktion pr√ºft, ob mehrere Komponenten des Man√∂vers im Text vorkommen,
    um eine hohe Treffsicherheit zu gew√§hrleisten.

    Args:
        text: Der zu analysierende Text (z.B. eine Chat-Nachricht, eine Aussage).

    Returns:
        Ein Dictionary mit dem Analyseergebnis.
    """
    found_components = set()
    
    # Durchsuche den Text nach jeder Komponente des Man√∂vers
    for component_name, patterns in MANEUVER_COMPONENTS.items():
        for pattern in patterns:
            if re.search(pattern, text, re.IGNORECASE):
                found_components.add(component_name)
                break  # Ein Treffer pro Komponente reicht

    score = len(found_components)
    is_detected = score >= 2  # Marker wird als erkannt gewertet, wenn mind. 2 Komponenten zutreffen

    analysis = {
        "marker_id": "AMBIVALENT_STALLING_MANEUVER",
        "is_detected": is_detected,
        "confidence_score": score / len(MANEUVER_COMPONENTS),
        "found_components": list(found_components)
    }

    if is_detected:
        explanation = "Die Aussage enth√§lt mehrere Signale f√ºr ein Hinhalteman√∂ver. "
        if "ZUSTIMMUNG_VORDERGR√úNDIG" in found_components and "VERANTWORTUNGS_VERSCHIEBUNG" in found_components:
            explanation += "Einer scheinbaren Zustimmung folgt eine sofortige R√ºckgabe der Verantwortung."
        elif "ZUSTIMMUNG_VORDERGR√úNDIG" in found_components and "VAGE_ZEITPLANUNG" in found_components:
            explanation += "Eine positive Reaktion wird mit einer unverbindlichen Zeitangabe kombiniert, um eine konkrete Festlegung zu vermeiden."
        else:
            explanation += "Durch die Kombination verschiedener vager Elemente wird eine klare Zusage vermieden."
        analysis["explanation"] = explanation

    return analysis

# ==============================================================================
# DEMONSTRATION / ANWENDUNGSBEISPIEL
# ==============================================================================

if __name__ == "__main__":
    
    test_cases = [
        # Klassischer Fall: Zustimmung + Verantwortungs-Verschiebung
        "Ja klar, super Idee! Was schwebt dir denn so vor?",
        # Klassischer Fall: Zustimmung + vage Zeitplanung
        "Uhm, ja, unbedingt. Lass uns das mal im Auge behalten und dann eher spontan schauen.",
        # Subtiler Fall: Enthusiasmus + Verantwortungs-Verschiebung
        "Auf jeden Fall! Freu mich schon riesig. Melde dich einfach, wenn du losf√§hrst!",
        # Nur eine Komponente -> sollte nicht erkannt werden
        "Vielleicht k√∂nnten wir ins Kino gehen.",
        # Eindeutige Zusage -> sollte nicht erkannt werden
        "Ja, lass uns am Samstag um 19 Uhr ins Kino gehen. Ich buche die Karten.",
        # Komplexerer Fall
        "Ich w√ºrde total gern mitkommen! Ich muss nur mal schauen, wie lange ich arbeiten muss. Sag du doch mal, wann es dir am besten passen w√ºrde."
    ]

    print("--- Semantische Analyse auf Hinhalteman√∂ver ---\n")

    for i, text in enumerate(test_cases):
        result = detect_ambivalent_stalling(text)
        print(f"Testfall #{i+1}: \"{text}\"")
        if result["is_detected"]:
            print(f"  ‚ñ∂Ô∏è Marker erkannt: JA")
            print(f"  ‚ñ∂Ô∏è Erkl√§rung: {result.get('explanation', 'N/A')}")
            print(f"  ‚ñ∂Ô∏è Gefundene Komponenten: {result['found_components']}")
        else:
            print(f"  ‚ñ∂Ô∏è Marker erkannt: NEIN (Nur {len(result['found_components'])}/2 Komponenten gefunden)")
        print("-" * 50)

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/DETECT_MANEUVER_COMPONENTS.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/DETECT_SYSTEM_TURNING_POINT.py
# ====================================================================

marker: DETECT_SYSTEM_TURNING_POINT.py_MARKER
beschreibung: >
  Metamarker-Cluster aus mehreren Einzelmarker-Ereignissen berechnen.
Beispiel: SYSTEM_TURNING_POINT

Vorstufen: H√§ufung von LATENT_INSTABILITY_MARKER + ESCALATION_TENSION_MARKER + R√ºckgang von MICRO_CARE_MARKER

Metamarker-Trigger: Schwelle √ºberschritten ‚Üí Visualisierung als Kipppunkt/√úbergang

Beispiel: SYSTEM_ROBUSTNESS

Vorstufen: H√§ufung von SYSTEM_RESILIENCE_MARKER + MICRO_CARE_MARKER + INTEGRATION_PROGRESSION_MARKER

Metamarker: Stabile, resiliente Beziehungsdynamik, wird als positiver Pol in Visualisierung angezeigt
beispiele:
  - "def detect_system_turning_point(marker_log):"
  - "instability = marker_log.count("LATENT_INSTABILITY_MARKER")"
  - "escalation = marker_log.count("ESCALATION_TENSION_MARKER")"
  - "micro_care = marker_log.count("MICRO_CARE_MARKER")"
  - "# Beispielschwelle: Instabilit√§t/Eskalation 5+, Micro-Care <2"
  - "if instability >= 3 and escalation >= 2 and micro_care < 2:"
  - "return True"
  - "return False"

semantic_grab:
  description: "Erkennt Muster f√ºr detect_system_turning_point.py"
  patterns:
    - rule: "AUTO_PATTERN"
      pattern: r"(muster.*wird.*erg√§nzt)"
tags: [neu_erstellt, needs_review]


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/DETECT_SYSTEM_TURNING_POINT.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/FAMILIENDYNAMIK_MARKER.py
# ====================================================================

import re

FAMILIENDYNAMIK_PATTERNS = [
    r"(typisch f√ºr unsere familie|du bist das schwarze schaf|oma hat dich immer bevorzugt|beim erbe leer ausgegangen|du klingst wie mutter|in dieser familie|deine sturheit kommt von papa|geschichten von vor 20 jahren|ich bin immer der kleine bruder|friedensstifter in dieser familie|ma√üregelst mich vor meinen kindern|wir halten nur zusammen, weil wir m√ºssen|hier wird erfolg misstrauisch be√§ugt|keiner redet tacheles|hinterr√ºcks gel√§stert|nie 'ich liebe dich' gesagt|bei uns wurde nichts ausgesprochen|jede feier endet im streit|du behandelst mich wie 12|√§lteste bestimmt immer|keiner h√∂rt mir zu|das perfekte familienbild|du warst nie da, als ich dich gebraucht habe)"
]

def detect_family_dynamics(text):
    for pattern in FAMILIENDYNAMIK_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/FAMILIENDYNAMIK_MARKER.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/FLIRT_DRIFT_DETECTOR.py
# ====================================================================

# ---------- Hilfsfunktionen ----------
def openness_score(msg):
    """Sehr grobe Heuristik: je mehr Self-Disclosure-Marker, desto h√∂her."""
    return (
        2 * msg.count("A_RAPID_SELF_DISCLOSURE")
        + 1 * msg.count("A_SOFT_FLIRT")
        + 0.5 * msg.count("A_SOFT_COMMITMENT")
    )

def time_diff(t1, t2):
    return abs((t2 - t1).total_seconds())

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/FLIRT_DRIFT_DETECTOR.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/FRAUD_MARKER_PATTERNS.py
# ====================================================================

import re


    "CRYPTO_SCAM": [r"(krypto|bitcoin|ethereum).*investition.*garantiert.*gewinn"],
    "AI_TRADING_SCAM": [r"k√ºnstliche.*intelligenz.*trading.*roboter"],
    "WAR_ROMANCE_SCAM": [r"ukraine.*krieg.*milit√§r.*einsatz.*geld.*brauche"],
    "CRYPTO_SCAM": [r"(krypto|bitcoin|ethereum).*investition.*garantiert.*gewinn"],
    "AI_TRADING_SCAM": [r"k√ºnstliche.*intelligenz.*trading.*roboter"],
    "WAR_ROMANCE_SCAM": [r"ukraine.*krieg.*milit√§r.*einsatz.*geld.*brauche"],
    "CRYPTO_SCAM": [r"(krypto|bitcoin|ethereum).*investition.*garantiert.*gewinn"],
    "AI_TRADING_SCAM": [r"k√ºnstliche.*intelligenz.*trading.*roboter"],
    "WAR_ROMANCE_SCAM": [r"ukraine.*krieg.*milit√§r.*einsatz.*geld.*brauche"],# ==============================================================================
# DEFINITION DER MARKER-MUSTER
# Zusammengefasste und komprimierte Erkennungsmuster f√ºr die angeforderten Marker.
# ==============================================================================

FRAUD_MARKER_PATTERNS = {
    "PLATFORM_SWITCH": [
        r"lass uns auf (WhatsApp|Telegram|Signal) wechseln",
        r"app (funktioniert nicht|ist langsam|spinnt)",
        r"mitgliedschaft .* l√§uft aus",
        r"hier (unpers√∂nlich|unsicher)",
        r"gib mir deine (nummer|e-mail)",
        r"per (mail|signal) viel (besser|sicherer|pers√∂nlicher)"
    ],
    "CRISIS_MONEY_REQUEST": [
        r"brauche dringend geld",
        r"konto (gesperrt|eingefroren)",
        r"paket .* zoll",
        r"(krankenhaus|operation|arzt) .* bezahlen",
        r"in (notlage|schwierigkeiten|gefahr)",
        r"alles verloren",
        r"kannst du mir (helfen|leihen)"
    ],
    "URGENCY_SCARCITY": [
        r"(sofort|dringend|schnell|jetzt sofort)",
        r"nur noch (heute|wenige stunden|kurze zeit)",
        r"frist l√§uft (heute|bald|gleich) ab",
        r"einmalige (chance|gelegenheit)",
        r"bevor es zu sp√§t ist",
        r"akku ist fast leer"
    ],
    "WEBCAM_EXCUSE": [
        r"kamera (ist kaputt|funktioniert nicht)",
        r"internet .* zu (schlecht|langsam)",
        r"(milit√§rbasis|√∂lplattform|mission) .* verboten",
        r"sehe (furchtbar|schrecklich) aus",
        r"bin zu (sch√ºchtern|m√ºde)",
        r"vielleicht (morgen|sp√§ter)"
    ],
    "UNTRACEABLE_PAYMENT_METHOD": [
        r"(geschenkkarte|gift card|gutschein)",
        r"(Apple|Google Play|Steam|Amazon)-?karten?",
        r"(codes|nummern) schicken",
        r"(Western Union|MoneyGram)",
        r"(Bitcoin|Krypto|BTC)",
        r"nicht r√ºckverfolgbar|anonym|diskret"
    ],
    "GASLIGHTING_GUILT": [
        r"du (bist zu sensibel|√ºbertreibst|bist verr√ºckt)",
        r"bildest dir das nur ein",
        r"das habe ich nie gesagt",
        r"dein misstrauen (zerst√∂rt|verletzt)",
        r"wenn du mich (lieben|mir vertrauen) w√ºrdest",
        r"mach aus einer m√ºcke keinen elefanten"
    ],
    "TRANSLATION_ARTIFACT": [
        # Sucht nach ungrammatischen oder seltsam formulierten Phrasen
        r"frau von hoher qualit√§t",
        r"mein herz ist springen",
        r"mache arbeit als",
        r"ich bin warten auf",
        r"treffen in person gesicht",
        r"liebe dich mehr als das leben selbst tut"
    ]
}

# ==============================================================================
# ANALYSEFUNKTION
# ==============================================================================

def detect_fraud_markers(text: str) -> list:
    """
    Analysiert einen Text und identifiziert eine Liste von Betrugs-Markern.

    Args:
        text: Der zu analysierende Textstring.

    Returns:
        Eine Liste mit den Namen der erkannten Marker.
    """
    detected_markers = []
    
    # Iteriere durch jeden Marker und seine zugeh√∂rigen Muster
    for marker_name, patterns in FRAUD_MARKER_PATTERNS.items():
        # Pr√ºfe jedes Muster f√ºr den aktuellen Marker
        for pattern in patterns:
            if re.search(pattern, text, re.IGNORECASE):
                # Wenn ein Muster gefunden wird, f√ºge den Marker zur Ergebnisliste hinzu
                # und gehe zum n√§chsten Marker-Typ √ºber (um Doppelungen zu vermeiden).
                if marker_name not in detected_markers:
                    detected_markers.append(marker_name)
    
    return detected_markers

# ==============================================================================
# DEMONSTRATION / ANWENDUNGSBEISPIEL
# ==============================================================================

if __name__ == "__main__":
    
    # Ein typischer, fiktiver Scam-Chatverlauf, der mehrere Marker enth√§lt
    scam_chat_example = """
    Hallo meine K√∂nigin, du bist eine Frau von hoher Qualit√§t. Ich bin so froh, dich kennengelernt zu haben.
    Aber diese App ist wirklich langsam und unpers√∂nlich. Lass uns auf WhatsApp wechseln, das ist viel besser.
    Ich w√ºrde dich gerne anrufen, aber meine Kamera ist leider kaputt, vielleicht morgen.
    Du, es ist etwas Schreckliches passiert. Mein Sohn hatte einen Unfall und ich brauche dringend Geld f√ºr das Krankenhaus.
    Die Zahlung muss sofort erfolgen, sonst operieren die √Ñrzte nicht. Bitte, wenn du mir vertrauen w√ºrdest, w√ºrdest du mir helfen.
    Am einfachsten geht es mit Apple-Geschenkkarten aus dem Supermarkt.
    """
    
    print("--- Analysiere Scam-Beispieltext ---")
    
    # Rufe die Analysefunktion auf
    found_flags = detect_fraud_markers(scam_chat_example)
    
    # Gib die Ergebnisse aus
    if found_flags:
        print(f"\n‚ö†Ô∏è Es wurden {len(found_flags)} verd√§chtige Marker im Text identifiziert:\n")
        for i, flag in enumerate(found_flags):
            print(f"  {i+1}. {flag}")
        print("\nDas kombinierte Auftreten dieser Marker stellt ein hohes Risiko dar.")
    else:
        print("\n‚úÖ Keine der definierten Betrugs-Marker im Text gefunden.")

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/FRAUD_MARKER_PATTERNS.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/META_OVERINTELLECTUALIZATION.py
# ====================================================================

import re

META_OVERINTELLECTUALIZATION_PATTERNS = [
    r"(diskrepanz|dissonanz|paradigma|systemisch|hom√∂ostase|sozial(?:es|e|er)? konstrukt|evolution√§r|algorithmus|ontologisch|hypothese|commitment-level|reziprok[ea]? investition|metaebene|struktur(?:ell)?|dekonstruier(?:e|st|t)|kognitiv|symbiose|psychodynamik|soziobiologisch|feedback(-| )?schleife|somatisch|paradigmenwechsel)",
    r"(ich beobachte nur|ich analysiere|ich dekonstruiere|ich nehme wahr|betrachten wir das|wir sollten erst die begrifflichkeiten kl√§ren|das ist keine emotion, sondern)",
    r"(klassische spannung (zwischen|von)|systemisches beispiel|mikrokosmos|ontologisch betrachtet|definition des begriffs|hypothese √ºber dich|commitment[- ]?level|reziproke investition)"
]

def detect_meta_overintellectualization(text):
    for pattern in META_OVERINTELLECTUALIZATION_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/META_OVERINTELLECTUALIZATION.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/PIPELINE.py
# ====================================================================

# ==============================  pipeline.py  ==============================
"""
Minimal-Pipeline zum Erkennen von Flirt-/Ambivalenz-Drift.
1.   Daten-Ingest  ‚Äì Chat-Records ‚óÅ JSON / DB / API
2.   Marker-Scoring ‚Äì Regex-/ML-Tagger ‚Üí marker_lookup
3.   Detector-Stage ‚Äì ruft unsere Detector-Funktionen
4.   Reporting      ‚Äì schreibt Flags in DB oder sendet Webhook
"""
from datetime import datetime, timedelta
from detector_utils import linear_slope
from detectors import (
    detect_sustained_contact,
    detect_secret_bonding,
    detect_adaptive_polarization,
    detect_flirt_dance_drift,
)

# --------------------------------------------------------------------------
# 1)  Simulierter Chat-Stream (chronologisch)
# --------------------------------------------------------------------------
CHAT_LOG = [
    {"id": "m1", "ts": datetime(2025, 7, 1, 9,  0), "text": "Du bist s√º√ü ;)",     },
    {"id": "m2", "ts": datetime(2025, 7, 1, 9,  1), "text": "Nicht verraten!",     },
    {"id": "m3", "ts": datetime(2025, 7, 2, 22, 0), "text": "Gute Nacht üåô",       },
    {"id": "m4", "ts": datetime(2025, 7, 3,  8, 0), "text": "Dir zum ersten Mal ‚Ä¶"},
    {"id": "m5", "ts": datetime(2025, 7, 3, 22, 0), "text": "L√∂sch den Chat üòä",   },
    {"id": "m6", "ts": datetime(2025, 7, 4, 20, 0), "text": "Du fehlst mir ‚Äì egal",},
    {"id": "m7", "ts": datetime(2025, 7, 5, 21, 0), "text": "VIP-Tattoo? üòè",      },
]

# --------------------------------------------------------------------------
# 2)  VERY simple Marker-Tagging (RegEx-Demo) -------------------------------
#     ‚Üí in der Praxis ersetzt du das durch dein echtes Marker-Engine-API.
# --------------------------------------------------------------------------
import re
PATTERNS = {
    "A_SOFT_FLIRT": re.compile(r"\b(s√º√ü|üòâ|üòä|üòè)\b", re.I),
    "S_MENTION_OF_SECRECY": re.compile(r"(nicht verraten|unter uns|l√∂sch)", re.I),
    "C_ADAPTIVE_POLARIZATION": re.compile(r"du fehlst mir.*egal", re.I),
    "C_RAPID_SELF_DISCLOSURE": re.compile(r"zum ersten mal|noch nie erz√§hlt", re.I),
}
marker_lookup = {}

for m in CHAT_LOG:
    found = [mid for mid, rx in PATTERNS.items() if rx.search(m["text"])]
    marker_lookup[m["id"]] = found

# --------------------------------------------------------------------------
# 3)  Detector-Stage --------------------------------------------------------
# --------------------------------------------------------------------------
detected = {}

ok, info = detect_sustained_contact(CHAT_LOG)
detected["C_SUSTAINED_CONTACT"] = ok

ok, info = detect_secret_bonding(CHAT_LOG, marker_lookup)
detected["C_SECRET_BONDING"] = ok

ok, info = detect_adaptive_polarization(CHAT_LOG, marker_lookup)
detected["C_ADAPTIVE_POLARIZATION"] = ok

ok, span = detect_flirt_dance_drift(CHAT_LOG, marker_lookup)
detected["MM_FLIRT_DANCE_DRIFT"] = ok

# --------------------------------------------------------------------------
# 4)  Report / Action -------------------------------------------------------
# --------------------------------------------------------------------------
if detected["MM_FLIRT_DANCE_DRIFT"]:
    print("‚ö†Ô∏è  FLIRT-DANCE-DRIFT erkannt!")
    for msg in span:
        print(f"  {msg['ts'].strftime('%Y-%m-%d %H:%M')} ‚Äì {msg['text']}")
else:
    print("Keine kritische Flirt-Drift erkannt.")

# ============================  Ende pipeline.py  ===========================


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/PIPELINE.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/PROJECTIVE_IDENTIFICATION_SEMANTIC_DETECTION.py
# ====================================================================

marker: PROJECTIVE_IDENTIFICATION_SEMANTIC_DETECTIO.py_MARKER
beschreibung: >
  # -*- coding: utf-8 -*-

# #############################################################################
#  STRUKTURIERTE DEFINITIONEN PSYCHOLOGISCHER KOMMUNIKATIONSMARKER
#
#  Zweck:
#  Diese Datei enth√§lt eine Liste von detailliert definierten Markern f√ºr die
#  semantische Analyse von Kommunikation. Jeder Marker ist als Python-Dictionary
#  strukturiert und enth√§lt:
#    - Beschreibung und typische Szenarien
#    - Realistische Beispiele
#    - M√∂gliche Kombinationen zu Meta-Markern
#    - "Semantic Grab": Konkrete Muster f√ºr die KI-Erkennung
#    - Psychologischen Hintergrund zur theoretischen Verankerung
#
#  Diese Struktur ist darauf ausgelegt, von KI-Systemen, insbesondere
#  NLP-Modellen, verarbeitet zu werden, um tiefere Einblicke in
#  Gespr√§chsdynamiken zu gewinnen.
# #############################################################################
beispiele:
  - "STRUKTURIERTE_MARKER_DEFINITIONEN = ["
  - "{"
  - ""id": "self_sabotage_loop","
  - ""name": "SELF_SABOTAGE_LOOP_MARKER","
  - ""beschreibung": "Ein Verhaltens- oder Kommunikationsmuster, bei dem eine Person ihre eigenen Chancen auf Erfolg, N√§he oder eine positive Probleml√∂sung aktiv oder passiv untergr√§bt.","
  - ""szenarien": ["
  - ""Dating & Beziehungsanbahnung (z.B. Absagen von Verabredungen)","
  - ""Berufsleben & Karriere (z.B. Ablehnen von Bef√∂rderungen, Verpassen von Fristen)","
  - ""Konfliktl√∂sung (z.B. Gespr√§chsabbruch aus Angst, alles schlimmer zu machen)","
  - ""Therapie- oder Coaching-Prozesse""
  - "],"
  - ""dynamik": "Die treibende Kraft ist eine tiefsitzende Angst (z.B. vor Versagen, Ablehnung, Intimit√§t), die durch die Selbstsabotage 'kontrolliert' werden soll. Es ist eine dysfunktionale Schutzstrategie nach dem Motto: 'Wenn ich es selbst zerst√∂re, kontrolliere ich den Schmerz und die Entt√§uschung'. Dies f√ºhrt zu einer sich selbst erf√ºllenden Prophezeiung.","
  - ""beispiele": ["
  - ""Beziehung/Dating: 'Du verdienst jemanden, der besser f√ºr dich ist als ich.'","
  - ""Beruf: 'Ach, die Bef√∂rderung sollen sie jemand anderem geben. Der Druck w√§re mir eh zu gro√ü.'","
  - ""Konflikt: 'Lass uns das Thema beenden, ich mache doch eh alles nur noch schlimmer.'","
  - ""Hilfe ablehnen: 'Nein, danke f√ºr die Hilfe. Ich will dir keine Umst√§nde machen.'","
  - ""Passive Sabotage: 'Ich habe die Bewerbungsfrist verpasst. War wohl einfach nicht sein soll.'","
  - ""Proaktive Sabotage: 'Ich habe das Projekt absichtlich nicht perfekt gemacht, damit die Erwartungen beim n√§chsten Mal nicht so hoch sind.'","
  - ""Digitale Variante (Chat): 'Ich antworte ihm extra nicht, dann merkt er vielleicht von selbst, dass ich zu kompliziert bin.'""
  - "],"
  - ""metamarker_kombinationen": ["
  - "{"
  - ""name": "Destruktive Hilflosigkeit","
  - ""kombination_mit": "Emotionaler R√ºckzugsdruck","
  - ""szenario": "Die Selbstsabotage wird zur Waffe. Beispiel: 'Ich wei√ü, ich kriege das Projekt nicht hin. Ich ziehe mich zur√ºck, bevor ich alles ruiniere. Dir scheint das ja ohnehin egal zu sein.'""
  - "},"
  - "{"
  - ""name": "Proaktive Resignation","
  - ""kombination_mit": "PLAYING_VICTIM_MARKER","
  - ""szenario": "Die Selbstsabotage wird als unabwendbares Schicksal dargestellt, das Mitleid erregen soll. Beispiel: 'Ich fange mit der Di√§t lieber erst gar nicht an, ich halte das eh nicht durch. Bei mir scheitert ja sowieso immer alles, egal was ich versuche.'""
  - "}"
  - "],"
  - ""semantic_grab": {"
  - ""description": "Erkennt eine Kombination aus negativen Selbstbewertungen, der Ablehnung von Chancen, dem Zur√ºckweisen von Hilfe und dem Fokus auf potenzielles Scheitern. Eine KI sollte hier nicht nur Keywords, sondern die semantische √Ñhnlichkeit zu diesen Mustern bewerten.","
  - ""patterns": ["
  - "{"
  - ""rule": "PREEMPTIVE_SURRENDER","
  - ""pattern": "(aber ich glaube|ich habe Angst|ich sollte lieber) + (ich schaffe das nicht|sagen wir es lieber ab|bin nicht gut genug)","
  - ""example": "Das Date w√§re toll, aber ich glaube, ich bin nicht interessant genug und sage lieber ab.""
  - "},"
  - "{"
  - ""rule": "FOCUS_ON_FAILURE_POTENTIAL","
  - ""pattern": "(bevor ich alles kaputt mache|damit ich niemanden entt√§usche|ich w√ºrde es nur ruinieren) + (lasse ich es lieber gleich|mache ich es erst gar nicht)","
  - ""example": "Bevor ich dich nur entt√§usche, fangen wir lieber gar nichts Ernstes an.""
  - "},"
  - "{"
  - ""rule": "DEVALUING_OPPORTUNITIES","
  - ""pattern": "(Die Bef√∂rderung|Das Kompliment|Die Chance) + (w√§re eh zu viel|meinte er nicht so|ist nichts f√ºr mich)","
  - ""example": "Das Kompliment meines Chefs habe ich ignoriert. Er meinte das sicher nicht so.""
  - "},"
  - "{"
  - ""rule": "WORTHINESS_NEGATION","
  - ""pattern": "(Du verdienst jemanden, der besser ist|Ich bin es nicht wert|Ich bin zu kompliziert f√ºr dich)","
  - ""example": "Ich bin es nicht wert, dass man sich so um mich bem√ºht.""
  - "},"
  - "{"
  - ""rule": "PASSIVE_SABOTAGE_RATIONALIZATION","
  - ""pattern": "(habe die Frist verpasst|nicht geschafft anzurufen|vergessen zu antworten) + (war wohl nicht sein soll|war eh besser so)","
  - ""example": "Ich habe die Bewerbungsfrist verpasst. War wohl einfach nicht sein soll.""
  - "}"
  - "]"
  - "},"
  - ""psychologischer_hintergrund": ["
  - "{"theorie": "Schematherapie (z.B. Schema 'Versagen' oder 'Unzul√§nglichkeit')"},"
  - "{"theorie": "Bindungstheorie (oft bei desorganisiertem oder vermeidendem Bindungsstil)"},"
  - "{"abwehrmechanismus": "Agieren (das Ausleben eines unbewussten Konflikts), Regression"}"
  - "]"
  - "},"
  - "{"
  - ""id": "guilt_tripping","
  - ""name": "GUILT_TRIPPING_MARKER (Schuldinduktion)","
  - ""beschreibung": "Die manipulative Taktik, bei einer anderen Person gezielt Schuldgef√ºhle zu erzeugen, um sie zu einem bestimmten Verhalten zu bewegen. Dies geschieht durch offene oder subtile Vorw√ºrfe, Appelle an Pflichtgef√ºhl oder die Betonung des eigenen Leidens.","
  - ""szenarien": ["
  - ""Famili√§re Konflikte","
  - ""Paarbeziehungen","
  - ""Arbeitsumfeld (z.B. bei der Verteilung von Aufgaben)""
  - "],"
  - ""dynamik": "Die 'Formel' lautet oft: Appell an Pflicht/Liebe + Implizite Androhung von emotionalem Entzug + Erzeugung von Schuldgef√ºhlen. Der andere soll sich moralisch verpflichtet f√ºhlen, den W√ºnschen des Manipulators zu entsprechen.","
  - ""beispiele": ["
  - ""Passiv-aggressiv: 'Schon gut, feier du deinen Geburtstag gro√ü. Ich sitze dann halt an dem Tag allein zu Hause, macht nichts.'","
  - ""Offener Vorwurf: 'Nach all den Jahren, die ich f√ºr diese Firma geopfert habe, kannst du mir nicht mal diesen einen freien Tag g√∂nnen?'","
  - ""Liebe an Bedingungen kn√ºpfen: 'Wenn du mich wirklich lieben w√ºrdest, w√ºrdest du mich heute nicht allein lassen wollen, wo ich so einen schweren Tag hatte.'","
  - ""Versteckter Appell: 'Du musst mir nicht helfen. Ich will ja nicht, dass du dich meinetwegen zu etwas gezwungen f√ºhlst.'","
  - ""Bezug auf Dritte: 'Dein Vater w√§re sehr traurig, wenn er sehen w√ºrde, wie du mit mir umgehst.'""
  - "],"
  - ""metamarker_kombinationen": [],"
  - ""semantic_grab": {"
  - ""description": "Erkennt Aussagen, die eine Diskrepanz zwischen dem Gesagten und dem Gemeinten aufweisen und oft das Leiden der sprechenden Person oder die undankbare Tat der anderen Person betonen.","
  - ""patterns": ["
  - "{"
  - ""rule": "SACRIFICE_REFERENCE","
  - ""pattern": "(Nach allem, was ich getan habe|Ich habe auf so vieles verzichtet|Ich opfere mich auf) + (und das ist der Dank|dann w√ºrdest du|kannst du nicht mal)","
  - ""example": "Nach allem, was ich f√ºr dich getan habe, kannst du mir nicht mal das g√∂nnen?""
  - "},"
  - "{"
  - ""rule": "PASSIVE_AGGRESSIVE_PERMISSION","
  - ""pattern": "(Geh ruhig|Mach nur|Feier du) + (ich sitze dann allein|ich mach das schon|ist ja nicht so schlimm)","
  - ""example": "Geh ruhig zu deinen Freunden, ich mache hier schon allein weiter.""
  - "},"
  - "{"
  - ""rule": "CONDITIONAL_LOVE_TEST","
  - ""pattern": "(Wenn du mich wirklich lieben w√ºrdest|Jemand, der liebt) + (w√ºrdest du das tun|w√ºrdest du nicht)","
  - ""example": "Jemand, der mich liebt, w√ºrde das f√ºr mich tun.""
  - "},"
  - "{"
  - ""rule": "DISAPPOINTMENT_DECLARATION","
  - ""pattern": "(Ich bin so entt√§uscht von dir|Ich h√§tte nie gedacht, dass du so bist|Deine Worte verletzen mich)","
  - ""example": "Ich bin einfach nur entt√§uscht. Ich h√§tte nie gedacht, dass du so egoistisch sein kannst.""
  - "}"
  - "]"
  - "},"
  - ""psychologischer_hintergrund": ["
  - "{"theorie": "Transaktionsanalyse"},"
  - "{"abwehrmechanismus": "Projektion, Externalisierung"}"
  - "]"
  - "},"
  - "{"
  - ""id": "escalation","
  - ""name": "ESCALATION_MARKER","
  - ""beschreibung": "Ein pl√∂tzlicher, intensiver Anstieg der emotionalen und/oder physiologischen Erregung, der oft zu einem Gespr√§chsabbruch oder einer Drohung f√ºhrt. Die Person f√ºhlt sich √ºberfordert, nicht geh√∂rt oder provoziert.","
  - ""szenarien": ["
  - ""Streit (online oder offline)","
  - ""Hitzige Diskussionen","
  - ""Konfrontation mit einem Trigger","
  - ""Technische Frustration (z.B. im Gaming)""
  - "],"
  - ""dynamik": "Die Kommunikation verl√§sst die rationale Ebene und wird von Wut, Panik oder starker Frustration dominiert. Oft folgt ein TERMINATION_MARKER, um die als unertr√§glich empfundene Situation zu beenden ('Ich bin raus').","
  - ""beispiele": ["
  - ""Kombination aus Physiologie, Emotion und Abbruch: 'Mein Puls ist auf 180! Wenn Sie das jetzt nicht sofort kl√§ren, dann eskaliert das hier gewaltig, das garantiere ich Ihnen!'","
  - ""Wut & Abbruch: 'Alter, ich raste aus vor Wut, f√ºhl mich komplett verarscht und jetzt reicht‚Äôs, ciao.'","
  - ""Technische Frustration: 'ALTER, DAS LAGGT SO HARD! ICH RASTE AUS! FUCK IT, ICH BIN RAUS F√úR HEUTE!'","
  - ""Angst/Panik: 'Ich krieg keine Luft mehr, alles dreht sich, ich glaube, ich kippe um, irgendwas stimmt nicht!'","
  - ""Digitale Variante: 'ü§¨ü§¨ü§¨ ICH HASSE DAS. BIN WEG. üö™'""
  - "],"
  - ""metamarker_kombinationen": ["
  - "{"
  - ""name": "Fight-or-Flight-Response","
  - ""kombination_mit": "TERMINATION_MARKER","
  - ""szenario": "Die Eskalation ist so hoch, dass die einzige verbleibende Handlung der sofortige R√ºckzug ist. Beispiel: 'Ich zittere am ganzen K√∂rper, weil du mir schon wieder nicht zuh√∂rst. Ich geh jetzt, bevor ich was Falsches sage!'""
  - "}"
  - "],"
  - ""semantic_grab": {"
  - ""description": "Erkennt eine hohe Dichte an W√∂rtern, die starke Emotionen (Wut, Zorn) und k√∂rperliche Stressreaktionen (Herzrasen, Zittern, keine Luft) beschreiben, oft in Kombination mit einem Abbruch des Gespr√§chs.","
  - ""patterns": ["
  - "{"
  - ""rule": "PHYSIOLOGICAL_STRESS_INDICATOR","
  - ""pattern": "Mein (Herz rast|Puls h√§mmert)|(ich zittere|kriege keine Luft|mein Blut kocht)","
  - ""example": "Mein Herz h√§mmert, ich zittere am ganzen K√∂rper.""
  - "},"
  - "{"
  - ""rule": "PEAK_EMOTION_DECLARATION","
  - ""pattern": "(ich raste aus|ich explodiere|ich platze gleich|ich koche vor Zorn|ich kann nicht mehr)","
  - ""example": "Ich platze gleich vor Wut!""
  - "},"
  - "{"
  - ""rule": "ACCUSATION_TRIGGER","
  - ""pattern": "(weil du nicht zuh√∂rst|du ignorierst mich|du verarschst mich|du hast null Respekt)","
  - ""example": "Du h√∂rst mir einfach nie zu!""
  - "},"
  - "{"
  - ""rule": "COMBINED_ESCALATION_TERMINATION","
  - ""pattern": "(raste aus|reicht's jetzt|Schnauze voll) + (und ciao|ich bin raus|ich geh offline)","
  - ""example": "Jetzt reicht's, ich bin hier raus.""
  - "}"
  - "]"
  - "},"
  - ""psychologischer_hintergrund": ["
  - "{"theorie": "Polyvagal-Theorie (sympathische Mobilisierung)"},"
  - "{"abwehrmechanismus": "Affektdurchbruch, Agieren"}"
  - "]"
  - "},"
  - "{"
  - ""id": "projective_identification","
  - ""name": "PROJECTIVE_IDENTIFICATION_MARKER","
  - ""beschreibung": "Ein komplexer Abwehrmechanismus, bei dem eine Person eigene, oft unakzeptable Gef√ºhle oder Eigenschaften nicht nur auf eine andere Person projiziert, sondern diese unbewusst dazu bringt, sich entsprechend dieser Projektion zu f√ºhlen und zu verhalten.","
  - ""szenarien": ["
  - ""Intensive Paarbeziehungen","
  - ""Eltern-Kind-Dynamiken","
  - ""Therapeutische Beziehungen","
  - ""Toxische Arbeitsumgebungen""
  - "],"
  - ""dynamik": "Es ist ein interpersoneller Prozess. Beispiel: Person A f√ºhlt sich innerlich inkompetent. Statt dieses Gef√ºhl bei sich wahrzunehmen, behandelt sie Person B herablassend und kritisiert sie als inkompetent ('Kannst du gar nichts richtig machen?'). Person B f√ºhlt sich dadurch verunsichert und f√§ngt an, Fehler zu machen, wodurch sie sich tats√§chlich inkompetent f√ºhlt und die Projektion von Person A best√§tigt.","
  - ""beispiele": ["
  - ""Projektion von Wertlosigkeit: A: 'Schon wieder die falsche Milch gekauft. Kannst du gar nichts richtig machen?' B: 'Ich hab das Gef√ºhl, ich bin f√ºr dich nichts wert!' A: 'Siehst du? Du hast ein Problem mit deinem Selbstwert, nicht ich.'","
  - ""Projektion von Aggression: A verh√§lt sich passiv-aggressiv. B wird w√ºtend. A sagt: 'Ich verstehe nicht, warum du immer so ausrasten musst.'","
  - ""Projektion von Eifersucht: A kontrolliert B permanent und sagt dann: 'Deine krankhafte Eifersucht macht unsere Beziehung kaputt.'","
  - ""Projektion von Unzuverl√§ssigkeit: 'Du bist so unzuverl√§ssig, ich kann mich nie auf dich verlassen.'""
  - "],"
  - ""metamarker_kombinationen": [],"
  - ""semantic_grab": {"
  - ""description": "Erkennt stabile, negative 'Du-bist-so'-Zuschreibungen, die dem Gegen√ºber eine Eigenschaft als Wesensmerkmal unterstellen. Die KI muss den Kontext analysieren: Findet die Zuschreibung in einem Konflikt statt? Reagiert das Gegen√ºber, indem es die zugeschriebene Eigenschaft best√§tigt?","
  - ""patterns": ["
  - "{"
  - ""rule": "NEGATIVE_TRAIT_ASSIGNMENT","
  - ""pattern": "(Du bist so|Du bist einfach|Deine) + (unzuverl√§ssig|unordentlich|eifers√ºchtig|√ºberfordert|ungeduldig|kontrolls√ºchtig)","
  - ""example": "Du bist einfach so ein Kontrollfreak.""
  - "},"
  - "{"
  - ""rule": "INCOMPETENCE_ACCUSATION","
  - ""pattern": "(Kannst du gar nichts richtig machen|Das hast du ja wieder toll hinbekommen|Du scheinst damit √ºberfordert zu sein)","
  - ""example": "Das hast du ja wieder toll hinbekommen.""
  - "},"
  - "{"
  - ""rule": "EMOTIONAL_STATE_PROJECTION","
  - ""pattern": "(Warum musst du immer so ausrasten|Du bist zu sensibel|Du √ºbertreibst total)","
  - ""example": "Warum musst du wegen jeder Kleinigkeit so ausrasten?""
  - "},"
  - "{"
  - ""rule": "BEHAVIOR_LABELING","
  - ""pattern": "(Du klammerst|Du machst immer Probleme|Du l√§sst mich immer allein)","
  - ""example": "Du klammerst so sehr, das macht mich fertig.""
  - "}"
  - "]"
  - "},"
  - ""psychologischer_hintergrund": ["
  - "{"theorie": "Objektbeziehungstheorie (Melanie Klein, Otto Kernberg)"},"
  - "{"abwehrmechanismus": "Projektive Identifizierung (gilt als primitiver Abwehrmechanismus)"}"
  - "]"
  - "}"
  - "]"
  - "# Beispielhafter Aufruf, um die Daten zu verwenden:"
  - "if __name__ == "__main__":"
  - "for marker in STRUKTURIERTE_MARKER_DEFINITIONEN:"
  - "print(f"--- Marker: {marker['name']} ---")"
  - "print(f"Beschreibung: {marker['beschreibung']}")"
  - "print(f"Beispiel: {marker['beispiele'][0]}")"
  - "if marker['semantic_grab']['patterns']:"
  - "print(f"Erstes semantisches Pattern ({marker['semantic_grab']['patterns'][0]['rule']}):")"
  - "print(f"  {marker['semantic_grab']['patterns'][0]['pattern']}")"
  - "print("\n" + "="*50 + "\n")"

semantic_grab:
  description: "Erkennt Muster f√ºr projective_identification_semantic_detectio.py"
  patterns:
    - rule: "AUTO_PATTERN"
      pattern: r"(muster.*wird.*erg√§nzt)"
tags: [neu_erstellt, needs_review]


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/PROJECTIVE_IDENTIFICATION_SEMANTIC_DETECTION.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/PSEUDOMETA_PATTERN.py
# ====================================================================

import re

PSEUDOMETA_PATTERN_PATTERNS = [
    r"(metaebene|kommunikationsmatrix|semantik|axiome|double-bind|projektionen|beziehungsebene|interaktionsmuster|wir reden aneinander vorbei|das ist kein inhaltliches, sondern ein beziehungsproblem|unsere begrifflichkeiten kl√§ren|wir analysieren nur|symptom f√ºr ein strukturelles problem|die art, wie wir streiten, ist ein spiegel|sprachliche ebene|supervision f√ºr kommunikation|wir sprechen unterschiedliche sprachen der liebe)",
    r"(wir sollten nicht √ºber den m√ºll reden, sondern √ºber das symbol dahinter|das eigentliche thema ist gr√∂√üer|soziologisch hochinteressant|wir sind beide nur schauspieler in einem drama|strukturproblem, kein gef√ºhlsproblem)"
]

def detect_pseudometa_pattern(text):
    for pattern in PSEUDOMETA_PATTERN_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/PSEUDOMETA_PATTERN.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/PYTHON_MARKER.py
# ====================================================================

"""
PYTHON_MARKER - Semantic Marker
Python-basierter Marker: PYTHON_MARKER
"""

import re

class PYTHON_MARKER:
    """
    Python-basierter Marker: PYTHON_MARKER
    """
    
    examples = [
    ]
    
    patterns = [
        re.compile(r"(muster.*wird.*erg√§nzt)", re.IGNORECASE)
    ]
    
    semantic_grabber_id = "AUTO_GENERATED"
    
    def match(self, text):
        """Pr√ºft ob der Text zum Marker passt"""
        for pattern in self.patterns:
            if pattern.search(text):
                return True
        return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/PYTHON_MARKER.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/REACTIVE_CONTROL_SPIRAL.py
# ====================================================================

import re

REACTIVE_CONTROL_SPIRAL_PATTERNS = [
    r"(du hast angefangen|nein, du hast angefangen|du willst stur sein|ich kann sturer sein|jedes mal, wenn ich|toll, weil du eingeschnappt bist, bin ich jetzt auch eingeschnappt|ich gehe erst einen schritt, wenn du einen gehst|ich senke meinen ton erst, wenn du deinen senkst|ich schreie durch die t√ºr|du manipulierst|nein, du zwingst mich dazu|wenn du provozierst, kann ich f√ºr nichts garantieren|du willst es auf die harte tour|solange du dich nicht entschuldigst, entschuldige ich mich auch nicht|kontrolle behalten|es endet erst, wenn einer aufgibt)",
    r"(du willst ehrlich sein, okay, hier ist die ehrliche meinung|du bist das problem|du willst immer das letzte wort|ich hab dir zehnmal geschrieben|ich ignoriere jetzt mal deine anrufe|jeder deiner vorw√ºrfe gibt mir nur mehr munition)"
]

def detect_reactive_control_spiral(text):
    for pattern in REACTIVE_CONTROL_SPIRAL_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/REACTIVE_CONTROL_SPIRAL.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/RESONANCE.py
# ====================================================================

import re

RESONANCE_PATTERNS = [
    r"(ich verstehe dich|ich sehe dich|ich f√ºhle mit dir|wow, das er√∂ffnet mir eine neue perspektive|das kenne ich auch|mir geht es genauso|ich urteile nicht|ich h√∂re dir zu|danke, dass du das sagst|du sprichst mir aus der seele|ich f√ºhle mich verbunden|du gibst mir neue einsicht|das ergibt sinn|genau das ist der punkt|ich bin froh, dass du es aussprichst|deine worte ber√ºhren mich|du bist nicht allein damit|ich kann zu 100 prozent nachf√ºhlen)",
    r"(das f√ºhlt sich so wahr an|ich muss dir nicht mal zustimmen, aber ich verstehe dich|ich kann dir keine l√∂sung geben, aber ich bin da|die energie hat sich gerade ver√§ndert|du bist wirklich gesehen)"
]

def detect_resonance(text):
    for pattern in RESONANCE_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/RESONANCE.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/SCAMMER_BEHAVIOUR_DETECT.py
# ====================================================================

kategorie: SEMANTIC
marker:
  category: Semantic
  context_rule: []
  description: Automatisch generierte Beschreibung f√ºr SCAMMER_BEHAVIOUR_DETECT.py_MARKER
  examples:
  - 'marker_name: SCAMMER_BEHAVIOUR_DETECT.py_MARKER'
  - 'marker: SCAMMER_BEHAVIOUR_DETECT.py_MARKER'
  - Erkennt manipulative Verhaltensmuster aus Romance-Scam-Situationen.
  - Die Marker basieren auf semantischen Strategien wie Love Bombing, Future Faking,
    Schuldumkehr, Isolation und Krisenmanipulation.
  - Diese Muster treten h√§ufig in digitaler Kommunikation auf und lassen sich nicht
    auf einzelne Schl√ºsselw√∂rter reduzieren.
  id: S_SCAMMER_BEHAVIOUR_DETECTPY
  level: 2
  name: SCAMMER_BEHAVIOUR_DETECT.py
  pattern: []
  semantic_grabber_id: AUTO_SEM_20250714_EF92
  semantic_tags:
  - scammer-behaviour-detect.py-marker
marker_name: SCAMMER_BEHAVIOUR_DETECT.py_MARKER


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/SCAMMER_BEHAVIOUR_DETECT.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/SCAMMER_DETECTIO.py
# ====================================================================

kategorie: SEMANTIC
marker:
  category: Semantic
  context_rule: []
  description: Automatisch generierte Beschreibung f√ºr SCAMMER_DETECTIO.py_MARKER
  examples:
  - 'marker_name: SCAMMER_DETECTIO.py_MARKER'
  - 'marker: SCAMMER_DETECTIO.py_MARKER'
  - Erkennt manipulative Verhaltensmuster aus Romance-Scam-Situationen.
  - Die Marker basieren auf semantischen Strategien wie Love Bombing, Future Faking,
    Schuldumkehr, Isolation und Krisenmanipulation.
  - Diese Muster treten h√§ufig in digitaler Kommunikation auf und lassen sich nicht
    auf einzelne Schl√ºsselw√∂rter reduzieren.
  id: S_SCAMMER_DETECTIOPY
  level: 2
  name: SCAMMER_DETECTIO.py
  pattern: []
  semantic_grabber_id: AUTO_SEM_20250714_2F39
  semantic_tags:
  - scammer-detectio.py-marker
marker_name: SCAMMER_DETECTIO.py_MARKER


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/SCAMMER_DETECTIO.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/SOCIAL_INTERACTION.py
# ====================================================================

marker: SOCIAL_INTERACTION.py_MARKER
beschreibung: >
  "beschreibung": (
            "Erkennt kommunikative Muster von freundlich-sympathischem Flirten ohne Grenz√ºberschreitung. "
            "Typisch sind spielerische Komplimente, Humor, harmlose Wortspiele, kleine Grenztests, "
            "bei denen R√ºckzug akzeptiert wird. Kein Dr√§ngen, keine Pseudo-Intimit√§t."
beispiele:
  - "import yaml"
  - "# Wir definieren die Marker erneut nach dem Kernel-Reset"
  - "new_markers = ["
  - "{"
  - ""marker": "FRIENDLY_FLIRTING_MARKER","
  - "),"
  - ""beispiele": ["
  - ""Na, du bist aber heute charmant drauf.","
  - ""Haha, das war jetzt aber ein Kompliment, oder?","
  - ""Du bist echt lustig, das mag ich.","
  - ""Du scheinst ein cooler Mensch zu sein.","
  - ""So viel Humor am Morgen? Respekt.","
  - ""Sorry, falls das zu direkt war ‚Äì wollte nur ein Kompliment machen.""
  - "],"
  - ""kategorie": "SOZIALE_INTERAKTION","
  - ""tags": ["flirt", "freundlich", "humor", "abgrenzung"],"
  - ""risk_score": 1"
  - "},"
  - "{"
  - ""marker": "OFFENSIVE_FLIRTING_MARKER","
  - ""beschreibung": ("
  - ""Erkennt kommunikative Muster von offensivem, teils grenz√ºberschreitendem Flirten. ""
  - ""Typisch sind direkte Anspielungen auf Aussehen, Sexualit√§t, oder das schnelle Andeuten ""
  - ""einer (k√∂rperlichen) Beziehung. Oft gepaart mit Pacing, √úbertreibung und ""
  - ""nachgeschobenen R√ºckziehern (‚Äûwar nur Spa√ü‚Äú).""
  - "),"
  - ""beispiele": ["
  - ""Mit dir w√ºrde ich sofort ans Meer fahren, ganz ehrlich.","
  - ""Wenn ich dich jetzt neben mir h√§tte, w√§re es sicher nicht langweilig.","
  - ""Bist du eigentlich immer so sexy, oder nur im Chat?","
  - ""Sorry, das war vielleicht ein bisschen zu viel. Aber du bist wirklich hei√ü.","
  - ""Wollen wir nicht einfach durchbrennen?","
  - ""Das klingt jetzt vielleicht verr√ºckt, aber du machst mich echt an.""
  - "],"
  - ""kategorie": "SOZIALE_INTERAKTION","
  - ""tags": ["flirt", "offensiv", "grenztest", "sexualisierung"],"
  - ""risk_score": 2"
  - "},"
  - "{"
  - ""marker": "DEEPENING_BY_QUESTIONING_ADVANCED_MARKER","
  - ""beschreibung": ("
  - ""Erkennt eine dialogische Strategie, in der durch wiederholt gezielte, ""
  - ""oft introspektive oder pers√∂nliche Fragen immer tiefere Informationen ""
  - ""√ºber das Gegen√ºber gewonnen werden. Das Ziel ist nicht nur ""
  - ""authentische N√§he, sondern der systematische Aufbau eines Pers√∂nlichkeitsprofils ""
  - ""(Mapping). Die Fragen wirken empathisch, dienen aber im Kontext der Gespr√§chsf√ºhrung ""
  - ""der Sammlung maximaler Resonanzdaten (Social Engineering).""
  - "),"
  - ""beispiele": ["
  - ""Was war dein pr√§gendstes Erlebnis in den letzten Jahren?","
  - ""Wovor hast du im Leben am meisten Angst?","
  - ""Wie w√ºrdest du dich mit drei Worten beschreiben?","
  - ""Was war deine gr√∂√üte Entt√§uschung in Beziehungen?","
  - ""Was gibt dir am meisten Kraft, wenn es dir schlecht geht?","
  - ""Welche Eigenschaft sch√§tzt du an anderen am meisten?","
  - ""Was hast du noch niemandem erz√§hlt?","
  - ""Was suchst du wirklich in einer Partnerschaft?""
  - "],"
  - ""kategorie": "MANIPULATION","
  - ""tags": ["questioning", "data_mining", "mapping", "tiefe"],"
  - ""risk_score": 3"
  - "},"
  - "{"
  - ""marker": "RELATIONSHIP_AUTHENTICITY_MARKER","
  - ""beschreibung": ("
  - ""Erkennt, ob im Gespr√§chsverlauf Hinweise darauf bestehen, dass sich die Gespr√§chspartner ""
  - ""bereits kennen (z.‚ÄØB. durch geteilte Erinnerungen, Insider, konkrete Bezugnahmen auf reale, ""
  - ""gemeinsam erlebte Situationen, Verwandte, Orte). Fehlen diese Ankerpunkte trotz hoher ""
  - ""emotionaler Resonanz, ist dies ein Risiko-Indikator f√ºr Scripting/Fake-Konstellation.""
  - "),"
  - ""beispiele": ["
  - ""Wann haben wir uns das letzte Mal gesehen?","
  - ""Wie war das noch mal damals auf der Party?","
  - ""Gr√º√ü deine Schwester von mir!","
  - ""Wei√üt du noch, wie wir zusammen im Urlaub waren?","
  - ""Damals in der WG-K√ºche ...","
  - ""Wei√üt du eigentlich noch, wo das Restaurant war, in dem wir letztes Mal waren?""
  - "],"
  - ""kategorie": "AUTHENTIZIT√ÑT","
  - ""tags": ["realit√§tsbezug", "insider", "ankergeschichten"],"
  - ""risk_score": 2"
  - "},"
  - "{"
  - ""marker": "RESONANZ_MATCHING_MARKER","
  - ""beschreibung": ("
  - ""Erkennt eine auffallend hohe √úbereinstimmung oder perfekte Resonanz zwischen ""
  - ""den √Ñu√üerungen beider Gespr√§chspartner ‚Äì insbesondere dann, wenn kein ""
  - ""gemeinsamer Erfahrungshintergrund sichtbar wird. Typisch sind nahtlose ""
  - ""Spiegelungen von Werten, Einstellungen, Sprachstil und Interessen, die ""
  - ""statistisch unplausibel erscheinen. Dies ist ein Indikator f√ºr KI-gest√ºtzte ""
  - ""Gespr√§chsoptimierung oder gezielte manipulative Strategie.""
  - "),"
  - ""beispiele": ["
  - ""Das sehe ich ganz genauso, das ist auch mein Lebensmotto.","
  - ""Du sprichst mir aus der Seele, ich habe exakt das gleiche erlebt.","
  - ""Genau so denke ich auch ‚Äì es ist, als w√ºrdest du meine Gedanken lesen.","
  - ""Wahnsinn, wir ticken ja wirklich identisch!","
  - ""Das ist verr√ºckt ‚Äì wir teilen genau die gleichen Werte.","
  - ""Es ist, als h√§tten wir uns schon ewig gekannt.""
  - "],"
  - ""kategorie": "MANIPULATION","
  - ""tags": ["resonanz", "spiegelung", "matching", "synchronizit√§t"],"
  - ""risk_score": 3"
  - "}"
  - "]"
  - "# Speicherpfad definieren"
  - "markers_path = "/mnt/data/ADVANCED_CHAT_MARKERS.yaml""
  - "# Datei speichern"
  - "with open(markers_path, "w", encoding="utf-8") as f:"
  - "yaml.dump(new_markers, f, allow_unicode=True)"
  - "markers_path"

semantic_grab:
  description: "Erkennt Muster f√ºr social_interaction.py"
  patterns:
    - rule: "AUTO_PATTERN"
      pattern: r"(muster.*wird.*erg√§nzt)"
tags: [neu_erstellt, needs_review]


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/SOCIAL_INTERACTION.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/SOCIAL_RESONANCE_DETECTOR.py
# ====================================================================

marker: SOCIAL_RESONANCE_DETECTOR.py_MARKER
beschreibung: >
  Semantic greb pattern
beispiele:
  - "SEMANTIC_GRAB_PATTERNS = {"
  - ""FRIENDLY_FLIRTING_MARKER": FRIENDLY_FLIRTING_GRAB,"
  - ""OFFENSIVE_FLIRTING_MARKER": OFFENSIVE_FLIRTING_GRAB,"
  - ""DEEPENING_BY_QUESTIONING_ADVANCED_MARKER": DEEPENING_BY_QUESTIONING_ADVANCED_GRAB,"
  - ""RELATIONSHIP_AUTHENTICITY_MARKER": RELATIONSHIP_AUTHENTICITY_GRAB,"
  - ""RESONANZ_MATCHING_MARKER": RESONANZ_MATCHING_GRAB,"
  - "}"

semantic_grab:
  description: "Erkennt Muster f√ºr social_resonance_detector.py"
  patterns:
    - rule: "AUTO_PATTERN"
      pattern: r"(muster.*wird.*erg√§nzt)"
tags: [neu_erstellt, needs_review]


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/SOCIAL_RESONANCE_DETECTOR.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/SYMBOLIC_ROLE_SWAP.py
# ====================================================================

import re

# Hinweis: F√ºr sehr gute Erkennung ggf. Named Entity Recognition/Spacy etc. nutzen.
# Hier: Typische Muster f√ºr √ºbertragene Objekte/Projektionen.
SYMBOLIC_ROLE_SWAP_PATTERNS = [
    r"(pflanze|haustier|hund|katze|fisch|socke|tasse|baum|navi|auto|akku|rad|ger√§t|kopfh√∂rer|blume|radio|projekt|t√ºr|fenster|wlan|handy|ger√§t|dokument|puzzleteil|ast|fahrrad|bank|sessel)",
    r"(funktioniert nicht|springt nicht an|bleibt stehen|eiert|schwimmt im kreis|geht kaputt|quietscht|reagiert nicht|leiser|verliert die balance|scheitert|einsam|geht leer aus|wird nie voll|ist nie synchron)",
    r"(als ob|wie wenn|genau wie|typisch f√ºr|kommt mir vor wie|es ist wie|f√ºhlt sich an wie)"
]

def detect_symbolic_role_swap(text):
    for pattern in SYMBOLIC_ROLE_SWAP_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/SYMBOLIC_ROLE_SWAP.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/aggregation_models.py
# ====================================================================

"""Datenmodelle f√ºr die Zeitreihen-Aggregation."""

from typing import List, Dict, Optional, Any, Union
from datetime import datetime, timedelta
from pydantic import BaseModel, Field, validator
from enum import Enum

import pandas as pd


class AggregationPeriod(str, Enum):
    """Verf√ºgbare Aggregations-Zeitr√§ume."""
    
    HOURLY = "hourly"
    DAILY = "daily"
    WEEKLY = "weekly"
    MONTHLY = "monthly"
    QUARTERLY = "quarterly"
    YEARLY = "yearly"
    CUSTOM = "custom"


class TimeSeriesPoint(BaseModel):
    """Ein einzelner Punkt in einer Zeitreihe."""
    
    timestamp: datetime = Field(..., description="Zeitpunkt")
    period_start: datetime = Field(..., description="Start des Aggregationszeitraums")
    period_end: datetime = Field(..., description="Ende des Aggregationszeitraums")
    
    values: Dict[str, float] = Field(
        default_factory=dict,
        description="Werte f√ºr verschiedene Metriken"
    )
    
    counts: Dict[str, int] = Field(
        default_factory=dict,
        description="Anzahlen (z.B. Marker-Counts)"
    )
    
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="Zus√§tzliche Metadaten"
    )
    
    @validator('period_end')
    def validate_period(cls, v, values):
        """Stelle sicher, dass period_end nach period_start liegt."""
        if 'period_start' in values and v <= values['period_start']:
            raise ValueError('period_end muss nach period_start liegen')
        return v


class TimeSeriesData(BaseModel):
    """Container f√ºr Zeitreihen-Daten."""
    
    series_id: str = Field(..., description="Eindeutige ID der Zeitreihe")
    name: str = Field(..., description="Name der Zeitreihe")
    metric_type: str = Field(..., description="Art der Metrik (score, count, etc.)")
    
    period: AggregationPeriod = Field(..., description="Aggregationszeitraum")
    
    data_points: List[TimeSeriesPoint] = Field(
        default_factory=list,
        description="Die eigentlichen Datenpunkte"
    )
    
    statistics: Dict[str, Any] = Field(
        default_factory=dict,
        description="Statistische Zusammenfassung"
    )
    
    def to_dataframe(self) -> pd.DataFrame:
        """Konvertiert die Zeitreihe in einen Pandas DataFrame."""
        if not self.data_points:
            return pd.DataFrame()
        
        records = []
        for point in self.data_points:
            record = {
                'timestamp': point.timestamp,
                'period_start': point.period_start,
                'period_end': point.period_end
            }
            record.update(point.values)
            record.update({f"{k}_count": v for k, v in point.counts.items()})
            records.append(record)
        
        df = pd.DataFrame(records)
        df.set_index('timestamp', inplace=True)
        return df
    
    def get_trend(self, metric: str) -> Optional[str]:
        """Berechnet den Trend f√ºr eine bestimmte Metrik."""
        if len(self.data_points) < 2:
            return None
        
        values = [p.values.get(metric, 0) for p in self.data_points]
        if not values:
            return None
        
        # Einfache Trendberechnung
        first_half = sum(values[:len(values)//2]) / (len(values)//2)
        second_half = sum(values[len(values)//2:]) / (len(values) - len(values)//2)
        
        if second_half > first_half * 1.1:
            return "increasing"
        elif second_half < first_half * 0.9:
            return "decreasing"
        else:
            return "stable"


class AggregationConfig(BaseModel):
    """Konfiguration f√ºr die Zeitreihen-Aggregation."""
    
    period: AggregationPeriod = Field(
        default=AggregationPeriod.DAILY,
        description="Standard-Aggregationszeitraum"
    )
    
    custom_period_hours: Optional[int] = Field(
        None,
        description="Stunden f√ºr custom period"
    )
    
    metrics_to_aggregate: List[str] = Field(
        default_factory=lambda: [
            "manipulation_index",
            "relationship_health",
            "fraud_probability",
            "marker_count"
        ],
        description="Zu aggregierende Metriken"
    )
    
    aggregation_functions: Dict[str, List[str]] = Field(
        default_factory=lambda: {
            "default": ["mean", "min", "max", "sum", "count"],
            "scores": ["mean", "min", "max", "std"],
            "counts": ["sum", "mean"]
        },
        description="Aggregationsfunktionen pro Metrik-Typ"
    )
    
    include_zero_periods: bool = Field(
        default=True,
        description="Ob Zeitr√§ume ohne Daten inkludiert werden sollen"
    )
    
    smooth_data: bool = Field(
        default=False,
        description="Ob Daten gegl√§ttet werden sollen"
    )
    
    smoothing_window: int = Field(
        default=3,
        description="Fenster f√ºr Gl√§ttung"
    )


class HeatmapData(BaseModel):
    """Daten f√ºr Heatmap-Visualisierung."""
    
    title: str = Field(..., description="Titel der Heatmap")
    
    x_labels: List[str] = Field(
        ...,
        description="X-Achsen-Labels (z.B. Zeitr√§ume)"
    )
    
    y_labels: List[str] = Field(
        ...,
        description="Y-Achsen-Labels (z.B. Marker-Kategorien)"
    )
    
    values: List[List[float]] = Field(
        ...,
        description="2D-Array der Werte"
    )
    
    color_scale: str = Field(
        default="RdYlGn",
        description="Farbskala f√ºr Heatmap"
    )
    
    annotations: Optional[List[List[str]]] = Field(
        None,
        description="Text-Annotationen f√ºr Zellen"
    )
    
    metadata: Dict[str, Any] = Field(default_factory=dict)


class ComparisonData(BaseModel):
    """Daten f√ºr Zeitreihen-Vergleiche."""
    
    comparison_type: str = Field(
        ...,
        description="Art des Vergleichs (period, speaker, category)"
    )
    
    series: List[TimeSeriesData] = Field(
        ...,
        description="Zu vergleichende Zeitreihen"
    )
    
    baseline_index: Optional[int] = Field(
        None,
        description="Index der Baseline-Serie"
    )
    
    differences: Optional[Dict[str, List[float]]] = Field(
        None,
        description="Berechnete Differenzen"
    )
    
    correlation_matrix: Optional[List[List[float]]] = Field(
        None,
        description="Korrelationsmatrix zwischen Serien"
    )


class AggregationResult(BaseModel):
    """Gesamtergebnis der Zeitreihen-Aggregation."""
    
    time_series: Dict[str, TimeSeriesData] = Field(
        default_factory=dict,
        description="Aggregierte Zeitreihen pro Metrik"
    )
    
    heatmaps: Dict[str, HeatmapData] = Field(
        default_factory=dict,
        description="Generierte Heatmap-Daten"
    )
    
    comparisons: List[ComparisonData] = Field(
        default_factory=list,
        description="Zeitreihen-Vergleiche"
    )
    
    summary_statistics: Dict[str, Dict[str, float]] = Field(
        default_factory=dict,
        description="Zusammenfassende Statistiken"
    )
    
    export_data: Optional[pd.DataFrame] = Field(
        None,
        description="Exportierbare Daten als DataFrame"
    )
    
    processing_time: float = Field(0.0)
    
    class Config:
        """Pydantic Konfiguration."""
        arbitrary_types_allowed = True  # F√ºr pandas DataFrame
    
    def to_csv(self, filepath: str):
        """Exportiert die Daten als CSV."""
        if self.export_data is not None:
            self.export_data.to_csv(filepath)
        else:
            # Erstelle DataFrame aus time_series
            dfs = []
            for metric, series in self.time_series.items():
                df = series.to_dataframe()
                df.columns = [f"{metric}_{col}" if col not in ['period_start', 'period_end'] 
                             else col for col in df.columns]
                dfs.append(df)
            
            if dfs:
                combined_df = pd.concat(dfs, axis=1)
                combined_df.to_csv(filepath)
    
    def to_json(self, filepath: str):
        """Exportiert die Daten als JSON."""
        import json
        
        # Konvertiere zu serialisierbarem Format
        data = {
            'time_series': {
                name: {
                    'series_id': ts.series_id,
                    'name': ts.name,
                    'metric_type': ts.metric_type,
                    'period': ts.period.value,
                    'data_points': [
                        {
                            'timestamp': point.timestamp.isoformat(),
                            'period_start': point.period_start.isoformat(),
                            'period_end': point.period_end.isoformat(),
                            'values': point.values,
                            'counts': point.counts
                        }
                        for point in ts.data_points
                    ],
                    'statistics': ts.statistics
                }
                for name, ts in self.time_series.items()
            },
            'summary_statistics': self.summary_statistics,
            'processing_time': self.processing_time
        }
        
        with open(filepath, 'w') as f:
            json.dump(data, f, indent=2)

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/aggregation_models.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/config_loader.py
# ====================================================================

"""Konfigurationslader f√ºr Marker-Definitionen aus verschiedenen Formaten."""

import json
import yaml
import re
from pathlib import Path
from typing import List, Dict, Any, Union, Optional
import logging
from dataclasses import dataclass

from ..matcher.marker_models import MarkerDefinition, MarkerPattern, MarkerCategory, MarkerSeverity


logger = logging.getLogger(__name__)


@dataclass
class MarkerConfig:
    """Globale Konfiguration f√ºr das Marker-System."""
    
    marker_directories: List[Path] = None
    auto_reload: bool = True
    cache_enabled: bool = True
    fuzzy_matching_default: bool = True
    default_context_words: int = 10
    
    def __post_init__(self):
        if self.marker_directories is None:
            self.marker_directories = [Path("markers")]


class MarkerLoader:
    """L√§dt und verwaltet Marker-Definitionen aus verschiedenen Quellen."""
    
    def __init__(self, config: Optional[MarkerConfig] = None):
        self.config = config or MarkerConfig()
        self._markers: Dict[str, MarkerDefinition] = {}
        self._file_cache: Dict[str, float] = {}  # Datei -> Zeitstempel
        
    def load_all_markers(self) -> Dict[str, MarkerDefinition]:
        """L√§dt alle Marker aus den konfigurierten Verzeichnissen."""
        self._markers.clear()
        
        for directory in self.config.marker_directories:
            if not directory.exists():
                logger.warning(f"Marker-Verzeichnis existiert nicht: {directory}")
                continue
                
            # YAML-Dateien
            for yaml_file in directory.glob("*.yaml"):
                try:
                    markers = self.load_yaml_markers(yaml_file)
                    self._merge_markers(markers)
                except Exception as e:
                    logger.error(f"Fehler beim Laden von {yaml_file}: {e}")
            
            # JSON-Dateien
            for json_file in directory.glob("*.json"):
                try:
                    markers = self.load_json_markers(json_file)
                    self._merge_markers(markers)
                except Exception as e:
                    logger.error(f"Fehler beim Laden von {json_file}: {e}")
            
            # TXT-Dateien (einfaches Format)
            for txt_file in directory.glob("*.txt"):
                try:
                    markers = self.load_txt_markers(txt_file)
                    self._merge_markers(markers)
                except Exception as e:
                    logger.error(f"Fehler beim Laden von {txt_file}: {e}")
        
        logger.info(f"Gesamt {len(self._markers)} Marker geladen")
        return self._markers
    
    def load_yaml_markers(self, filepath: Path) -> List[MarkerDefinition]:
        """L√§dt Marker aus einer YAML-Datei."""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        
        markers = []
        if isinstance(data, dict) and 'markers' in data:
            marker_list = data['markers']
        elif isinstance(data, list):
            marker_list = data
        else:
            raise ValueError(f"Unerwartetes YAML-Format in {filepath}")
        
        for marker_data in marker_list:
            marker = self._parse_marker_data(marker_data)
            if marker:
                markers.append(marker)
        
        logger.info(f"Geladen: {len(markers)} Marker aus {filepath}")
        return markers
    
    def load_json_markers(self, filepath: Path) -> List[MarkerDefinition]:
        """L√§dt Marker aus einer JSON-Datei."""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        markers = []
        if isinstance(data, dict) and 'markers' in data:
            marker_list = data['markers']
        elif isinstance(data, list):
            marker_list = data
        else:
            raise ValueError(f"Unerwartetes JSON-Format in {filepath}")
        
        for marker_data in marker_list:
            marker = self._parse_marker_data(marker_data)
            if marker:
                markers.append(marker)
        
        logger.info(f"Geladen: {len(markers)} Marker aus {filepath}")
        return markers
    
    def load_txt_markers(self, filepath: Path) -> List[MarkerDefinition]:
        """L√§dt Marker aus einer einfachen TXT-Datei.
        
        Format:
        # Kommentar
        MARKER_ID | Kategorie | Name | Pattern1, Pattern2, ...
        """
        markers = []
        
        with open(filepath, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                
                # √úberspringe leere Zeilen und Kommentare
                if not line or line.startswith('#'):
                    continue
                
                parts = [p.strip() for p in line.split('|')]
                if len(parts) < 4:
                    logger.warning(f"Zeile {line_num} in {filepath} hat ung√ºltiges Format")
                    continue
                
                marker_id, category, name, patterns = parts[0], parts[1], parts[2], parts[3]
                
                # Parse patterns
                pattern_list = [p.strip() for p in patterns.split(',')]
                
                try:
                    marker = MarkerDefinition(
                        id=marker_id,
                        name=name,
                        category=self._parse_category(category),
                        description=f"Marker aus {filepath.name}",
                        keywords=pattern_list,
                        weight=1.0
                    )
                    markers.append(marker)
                except Exception as e:
                    logger.error(f"Fehler beim Parsen von Zeile {line_num}: {e}")
        
        logger.info(f"Geladen: {len(markers)} Marker aus {filepath}")
        return markers
    
    def _parse_marker_data(self, data: Dict[str, Any]) -> Optional[MarkerDefinition]:
        """Parst Marker-Daten aus einem Dictionary."""
        try:
            # Basis-Felder
            marker_id = data.get('id', data.get('marker_id'))
            if not marker_id:
                logger.warning("Marker ohne ID gefunden, √ºberspringe")
                return None
            
            # Patterns verarbeiten
            patterns = []
            if 'patterns' in data:
                for p in data['patterns']:
                    if isinstance(p, str):
                        patterns.append(MarkerPattern(pattern=p))
                    elif isinstance(p, dict):
                        patterns.append(MarkerPattern(**p))
            
            # Keywords
            keywords = data.get('keywords', [])
            if isinstance(keywords, str):
                keywords = [keywords]
            
            # Kategorie parsen
            category = self._parse_category(data.get('category', 'manipulation'))
            
            # Severity parsen
            severity = self._parse_severity(data.get('severity', 'medium'))
            
            marker = MarkerDefinition(
                id=marker_id,
                name=data.get('name', marker_id),
                category=category,
                severity=severity,
                description=data.get('description', ''),
                patterns=patterns,
                keywords=keywords,
                examples=data.get('examples', []),
                weight=float(data.get('weight', 1.0)),
                metadata=data.get('metadata', {}),
                active=data.get('active', True)
            )
            
            return marker
            
        except Exception as e:
            logger.error(f"Fehler beim Parsen von Marker-Daten: {e}")
            return None
    
    def _parse_category(self, category_str: str) -> MarkerCategory:
        """Parst eine Kategorie-String zu MarkerCategory Enum."""
        category_str = category_str.lower().strip()
        
        # Mapping f√ºr alternative Namen
        category_map = {
            'scam': MarkerCategory.FRAUD,
            'romance_scam': MarkerCategory.FRAUD,
            'betrug': MarkerCategory.FRAUD,
            'manipulativ': MarkerCategory.MANIPULATION,
            'abuse': MarkerCategory.EMOTIONAL_ABUSE,
            'positiv': MarkerCategory.POSITIVE,
            'wertsch√§tzung': MarkerCategory.POSITIVE,
            'unterst√ºtzung': MarkerCategory.SUPPORT,
        }
        
        # Erst im Mapping suchen
        if category_str in category_map:
            return category_map[category_str]
        
        # Dann versuchen direkt zu parsen
        try:
            return MarkerCategory(category_str)
        except ValueError:
            logger.warning(f"Unbekannte Kategorie '{category_str}', nutze MANIPULATION")
            return MarkerCategory.MANIPULATION
    
    def _parse_severity(self, severity_str: str) -> MarkerSeverity:
        """Parst eine Severity-String zu MarkerSeverity Enum."""
        severity_str = severity_str.lower().strip()
        
        severity_map = {
            'niedrig': MarkerSeverity.LOW,
            'gering': MarkerSeverity.LOW,
            'mittel': MarkerSeverity.MEDIUM,
            'hoch': MarkerSeverity.HIGH,
            'kritisch': MarkerSeverity.CRITICAL,
            'schwer': MarkerSeverity.HIGH,
        }
        
        if severity_str in severity_map:
            return severity_map[severity_str]
        
        try:
            return MarkerSeverity(severity_str)
        except ValueError:
            logger.warning(f"Unbekannte Severity '{severity_str}', nutze MEDIUM")
            return MarkerSeverity.MEDIUM
    
    def _merge_markers(self, new_markers: List[MarkerDefinition]):
        """F√ºgt neue Marker zum bestehenden Set hinzu."""
        for marker in new_markers:
            if marker.id in self._markers:
                logger.warning(f"Marker {marker.id} existiert bereits, wird √ºberschrieben")
            self._markers[marker.id] = marker
    
    def get_marker(self, marker_id: str) -> Optional[MarkerDefinition]:
        """Gibt einen spezifischen Marker zur√ºck."""
        return self._markers.get(marker_id)
    
    def get_markers_by_category(self, category: MarkerCategory) -> List[MarkerDefinition]:
        """Gibt alle Marker einer Kategorie zur√ºck."""
        return [m for m in self._markers.values() if m.category == category]
    
    def get_active_markers(self) -> List[MarkerDefinition]:
        """Gibt nur aktive Marker zur√ºck."""
        return [m for m in self._markers.values() if m.active]
    
    def reload_if_changed(self) -> bool:
        """L√§dt Marker neu, wenn sich Dateien ge√§ndert haben."""
        if not self.config.auto_reload:
            return False
        
        changed = False
        for directory in self.config.marker_directories:
            if not directory.exists():
                continue
            
            for file in directory.glob("*.{yaml,json,txt}"):
                mtime = file.stat().st_mtime
                if file.as_posix() not in self._file_cache or \
                   self._file_cache[file.as_posix()] < mtime:
                    changed = True
                    break
        
        if changed:
            logger.info("√Ñnderungen erkannt, lade Marker neu")
            self.load_all_markers()
            
        return changed

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/config_loader.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/create_marker_master.py
# ====================================================================

#!/usr/bin/env python3
"""
Marker Master Export Generator
Konsolidiert alle Marker aus verschiedenen Quellen in eine Master-YAML-Datei
"""

import os
import yaml
import json
import re
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging

# Logging konfigurieren
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MarkerCollector:
    """Sammelt und konsolidiert Marker aus verschiedenen Quellen"""
    
    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        self.markers = {}
        self.semantic_detectors = {}
        self.duplicate_count = 0
        
    def collect_all_markers(self) -> Dict[str, Any]:
        """Hauptmethode: Sammelt alle Marker aus verschiedenen Quellen"""
        logger.info("Starte Marker-Sammlung...")
        
        # Definiere die zu durchsuchenden Ordner
        search_dirs = [
            "Assist_TXT_marker_py:/ALL_NEWMARKER01",
            "Assist_TXT_marker_py: 2/ALL_NEWMARKER01",
            "Assist_TXT_marker_py: 2/MARKERBOOK_YAML_CANVAS",
            "Assist_TXT_marker_py: 2/tension",
            "Assist_TXT_marker_py: 2/resonance",
            "Assist_TXT_marker_py: 3/ALL_NEWMARKER01",
            "Assist_TXT_marker_py: 3/MARKERBOOK_YAML_CANVAS",
            "Assist_TXT_marker_py: 3/tension",
            "Assist_TXT_marker_py: 3/resonance",
            "Assist_YAML_marker_py: 4",
            "Assist_YAML_marker_py: 4/MARKERBOOK_YAML_CANVAS",
            "Assist_YAML_marker_py: 4/tension",
            "Assist_YAML_marker_py: 4/resonance",
        ]
        
        # Sammle Marker aus jedem Verzeichnis
        for dir_path in search_dirs:
            full_path = self.base_path / dir_path
            if full_path.exists():
                logger.info(f"Durchsuche: {dir_path}")
                self._process_directory(full_path)
            else:
                logger.warning(f"Verzeichnis nicht gefunden: {dir_path}")
        
        # Sammle semantische Detektoren
        self._collect_semantic_detectors()
        
        # Verkn√ºpfe Marker mit Detektoren
        self._link_detectors_to_markers()
        
        logger.info(f"Sammlung abgeschlossen. {len(self.markers)} eindeutige Marker gefunden.")
        logger.info(f"{self.duplicate_count} Duplikate √ºbersprungen.")
        
        return self.markers 

    def _process_directory(self, directory: Path):
        """Verarbeitet alle Marker-Dateien in einem Verzeichnis"""
        for file_path in directory.iterdir():
            if file_path.is_file() and self._is_marker_file(file_path):
                try:
                    self._process_file(file_path)
                except Exception as e:
                    logger.error(f"Fehler beim Verarbeiten von {file_path}: {e}")
    
    def _is_marker_file(self, file_path: Path) -> bool:
        """Pr√ºft, ob eine Datei eine Marker-Datei ist"""
        name = file_path.name.lower()
        # √úberspringe Backup-Dateien und System-Dateien
        if any(skip in name for skip in ['.backup', '.ds_store', '__pycache__']):
            return False
        # Akzeptiere Marker-Dateien
        return any(marker_id in name for marker_id in ['marker', 'knot', 'spiral', 'pattern'])
    
    def _process_file(self, file_path: Path):
        """Verarbeitet eine einzelne Marker-Datei"""
        logger.debug(f"Verarbeite: {file_path.name}")
        
        if file_path.suffix == '.yaml' or file_path.suffix == '.yml':
            self._process_yaml_file(file_path)
        elif file_path.suffix == '.txt':
            self._process_txt_file(file_path)
        elif file_path.suffix == '.json':
            self._process_json_file(file_path)
    
    def _process_yaml_file(self, file_path: Path):
        """Verarbeitet YAML-Dateien"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                
            if isinstance(data, dict):
                # Pr√ºfe ob es ein einzelner Marker ist
                if 'marker' in data:
                    self._add_marker(data)
                # Oder eine Liste von Markern
                elif any(key.lower().endswith('marker') for key in data.keys()):
                    for key, value in data.items():
                        if isinstance(value, list):
                            for item in value:
                                self._add_marker_from_list_format(key, item)
        except Exception as e:
            logger.error(f"Fehler beim Parsen von YAML {file_path}: {e}")
    
    def _process_txt_file(self, file_path: Path):
        """Verarbeitet TXT-Dateien mit Marker-Definitionen"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Versuche strukturierte Marker zu extrahieren
            marker_data = self._extract_marker_from_txt(content, file_path.stem)
            if marker_data:
                self._add_marker(marker_data)
        except Exception as e:
            logger.error(f"Fehler beim Lesen von {file_path}: {e}")
    
    def _process_json_file(self, file_path: Path):
        """Verarbeitet JSON-Dateien"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, dict):
                if 'marker' in data:
                    self._add_marker(data)
                elif isinstance(data, list):
                    for item in data:
                        if isinstance(item, dict) and 'marker' in item:
                            self._add_marker(item)
        except Exception as e:
            logger.error(f"Fehler beim Parsen von JSON {file_path}: {e}")

    def _extract_marker_from_txt(self, content: str, file_stem: str) -> Optional[Dict[str, Any]]:
        """Extrahiert Marker-Daten aus TXT-Dateien"""
        marker_data = {}
        
        # Versuche Marker-Namen zu extrahieren
        marker_match = re.search(r'marker:\s*(\w+)', content, re.IGNORECASE)
        if marker_match:
            marker_data['marker'] = marker_match.group(1)
        else:
            # Verwende Dateinamen als Marker-Namen
            marker_name = file_stem.replace('_MARKER', '').replace('.txt', '')
            marker_data['marker'] = marker_name
        
        # Extrahiere Beschreibung
        desc_match = re.search(r'beschreibung:\s*(.+?)(?=\n\w+:|$)', content, re.IGNORECASE | re.DOTALL)
        if desc_match:
            marker_data['beschreibung'] = desc_match.group(1).strip()
        
        # Extrahiere Beispiele
        beispiele = []
        beispiel_section = re.search(r'beispiele:(.*?)(?=\n\w+:|$)', content, re.IGNORECASE | re.DOTALL)
        if beispiel_section:
            beispiel_text = beispiel_section.group(1)
            # Finde alle Zeilen mit - am Anfang
            beispiele = re.findall(r'-\s*"([^"]+)"', beispiel_text)
            if not beispiele:
                # Alternative: Zeilen mit Aufz√§hlungszeichen
                beispiele = re.findall(r'-\s*(.+)', beispiel_text)
        
        if beispiele:
            marker_data['beispiele'] = [b.strip() for b in beispiele]
        
        # Extrahiere Tags
        tags_match = re.search(r'tags:\s*\[(.*?)\]', content, re.IGNORECASE)
        if tags_match:
            tags = [t.strip() for t in tags_match.group(1).split(',')]
            marker_data['tags'] = tags
        
        return marker_data if 'marker' in marker_data else None
    
    def _add_marker(self, marker_data: Dict[str, Any]):
        """F√ºgt einen Marker zur Sammlung hinzu"""
        marker_name = marker_data.get('marker', '').upper()
        
        if not marker_name:
            return
        
        # Pr√ºfe auf Duplikate
        if marker_name in self.markers:
            self.duplicate_count += 1
            # Merge Daten wenn n√∂tig
            existing = self.markers[marker_name]
            # F√ºge neue Beispiele hinzu
            if 'beispiele' in marker_data:
                existing_examples = set(existing.get('beispiele', []))
                new_examples = set(marker_data.get('beispiele', []))
                existing['beispiele'] = list(existing_examples | new_examples)
            # Merge Tags
            if 'tags' in marker_data:
                existing_tags = set(existing.get('tags', []))
                new_tags = set(marker_data.get('tags', []))
                existing['tags'] = list(existing_tags | new_tags)
        else:
            # Normalisiere Marker-Struktur
            normalized = {
                'marker': marker_name,
                'beschreibung': marker_data.get('beschreibung', ''),
                'beispiele': marker_data.get('beispiele', []),
                'tags': marker_data.get('tags', []),
                'kategorie': marker_data.get('kategorie', 'UNCATEGORIZED'),
                'psychologischer_hintergrund': marker_data.get('psychologischer_hintergrund', ''),
                'dynamik_absicht': marker_data.get('dynamik_absicht', ''),
                'szenarien': marker_data.get('szenarien', []),
                'risk_score': marker_data.get('risk_score', 1)
            }
            
            # F√ºge semantische Detektor-Info hinzu, falls vorhanden
            if 'semantic_grab' in marker_data:
                normalized['semantic_patterns'] = marker_data['semantic_grab']
            
            self.markers[marker_name] = normalized
    
    def _add_marker_from_list_format(self, key: str, item: Dict[str, Any]):
        """F√ºgt Marker aus Listen-Format hinzu (z.B. Ambivalenzmarker)"""
        if isinstance(item, dict) and 'input' in item:
            marker_name = key.upper()
            if marker_name not in self.markers:
                self.markers[marker_name] = {
                    'marker': marker_name,
                    'beschreibung': f'Automatisch generiert aus {key}',
                    'beispiele': [],
                    'tags': [key],
                    'kategorie': 'PATTERN',
                    'risk_score': 1
                }
            
            # F√ºge Beispiel hinzu
            if 'input' in item:
                self.markers[marker_name]['beispiele'].append(item['input'])
    
    def _collect_semantic_detectors(self):
        """Sammelt alle Python-Detektoren"""
        detector_dirs = [
            "Assist_TXT_marker_py: 2/SEMANTIC_DETECTORS_PYTHO",
            "Assist_TXT_marker_py: 3/SEMANTIC_DETECTORS_PYTHO"
        ]
        
        for dir_path in detector_dirs:
            full_path = self.base_path / dir_path
            if full_path.exists():
                for file_path in full_path.glob("*.py"):
                    if file_path.stem not in ['__init__', 'setup_py']:
                        self.semantic_detectors[file_path.stem.upper()] = file_path.name
    
    def _link_detectors_to_markers(self):
        """Verkn√ºpft Detektoren mit passenden Markern"""
        for marker_name, marker_data in self.markers.items():
            # Suche nach passendem Detektor
            for detector_name, detector_file in self.semantic_detectors.items():
                if detector_name in marker_name or marker_name in detector_name:
                    marker_data['semantics_detector'] = detector_file
                    break 

    def export_to_yaml(self, output_file: str = "marker_master_export.yaml"):
        """Exportiert alle Marker in eine YAML-Datei"""
        # Konvertiere Marker-Dictionary in Liste f√ºr bessere Lesbarkeit
        marker_list = list(self.markers.values())
        
        # Sortiere nach Marker-Namen
        marker_list.sort(key=lambda x: x['marker'])
        
        # F√ºge Metadaten hinzu
        export_data = {
            'version': '2.0',
            'generated_at': datetime.now().isoformat(),
            'total_markers': len(marker_list),
            'categories': self._get_categories(),
            'risk_levels': {
                'green': 'Kein oder nur unkritischer Marker',
                'yellow': '1-2 moderate Marker, erste Drift erkennbar',
                'blinking': '3+ Marker oder ein Hochrisiko-Marker, klare Drift/Manipulation',
                'red': 'Hochrisiko-Kombination, massive Drift/Manipulation'
            },
            'markers': marker_list
        }
        
        # Schreibe YAML-Datei
        with open(output_file, 'w', encoding='utf-8') as f:
            yaml.dump(export_data, f, 
                     default_flow_style=False, 
                     allow_unicode=True,
                     sort_keys=False,
                     width=120)
        
        logger.info(f"YAML-Export erstellt: {output_file}")
        return output_file
    
    def export_to_json(self, output_file: str = "marker_master_export.json"):
        """Exportiert alle Marker in eine JSON-Datei"""
        # Konvertiere Marker-Dictionary in Liste
        marker_list = list(self.markers.values())
        
        # Sortiere nach Marker-Namen
        marker_list.sort(key=lambda x: x['marker'])
        
        # F√ºge Metadaten hinzu
        export_data = {
            'version': '2.0',
            'generated_at': datetime.now().isoformat(),
            'total_markers': len(marker_list),
            'categories': self._get_categories(),
            'risk_levels': {
                'green': 'Kein oder nur unkritischer Marker',
                'yellow': '1-2 moderate Marker, erste Drift erkennbar',
                'blinking': '3+ Marker oder ein Hochrisiko-Marker, klare Drift/Manipulation',
                'red': 'Hochrisiko-Kombination, massive Drift/Manipulation'
            },
            'markers': marker_list
        }
        
        # Schreibe JSON-Datei
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, 
                     ensure_ascii=False, 
                     indent=2)
        
        logger.info(f"JSON-Export erstellt: {output_file}")
        return output_file
    
    def _get_categories(self) -> List[str]:
        """Sammelt alle verwendeten Kategorien"""
        categories = set()
        for marker in self.markers.values():
            categories.add(marker.get('kategorie', 'UNCATEGORIZED'))
        return sorted(list(categories))
    
    def generate_report(self):
        """Generiert einen Bericht √ºber die gesammelten Marker"""
        print("\n" + "="*60)
        print("MARKER MASTER EXPORT - BERICHT")
        print("="*60)
        print(f"\nGesammelte Marker: {len(self.markers)}")
        print(f"Duplikate √ºbersprungen: {self.duplicate_count}")
        print(f"Semantische Detektoren gefunden: {len(self.semantic_detectors)}")
        
        # Kategorien-√úbersicht
        categories = {}
        for marker in self.markers.values():
            cat = marker.get('kategorie', 'UNCATEGORIZED')
            categories[cat] = categories.get(cat, 0) + 1
        
        print("\nKategorien:")
        for cat, count in sorted(categories.items()):
            print(f"  - {cat}: {count} Marker")
        
        # Marker mit Detektoren
        markers_with_detectors = sum(1 for m in self.markers.values() if 'semantics_detector' in m)
        print(f"\nMarker mit semantischen Detektoren: {markers_with_detectors}")
        
        # Top 10 Marker nach Anzahl der Beispiele
        sorted_by_examples = sorted(
            self.markers.items(), 
            key=lambda x: len(x[1].get('beispiele', [])), 
            reverse=True
        )[:10]
        
        print("\nTop 10 Marker nach Anzahl der Beispiele:")
        for name, data in sorted_by_examples:
            print(f"  - {name}: {len(data.get('beispiele', []))} Beispiele")


def main():
    """Hauptfunktion"""
    print("Marker Master Export Generator")
    print("==============================\n")
    
    # Erstelle Collector-Instanz
    collector = MarkerCollector()
    
    # Sammle alle Marker
    markers = collector.collect_all_markers()
    
    if not markers:
        logger.error("Keine Marker gefunden!")
        return
    
    # Generiere Bericht
    collector.generate_report()
    
    # Exportiere in beide Formate
    yaml_file = collector.export_to_yaml()
    json_file = collector.export_to_json()
    
    print(f"\nExport abgeschlossen:")
    print(f"  - YAML: {yaml_file}")
    print(f"  - JSON: {json_file}")
    
    # Erstelle README f√ºr die Verwendung
    readme_content = """# Marker Master Export

Diese Dateien enthalten das vollst√§ndige Marker-Masterset f√ºr den semantisch-psychologischen Resonanz- und Manipulations-Detektor.

## Verwendung

### Import in Python:
```python
import yaml
with open('marker_master_export.yaml', 'r', encoding='utf-8') as f:
    data = yaml.safe_load(f)
    
markers = data['markers']
```

### Struktur eines Markers:
- `marker`: Name/ID des Markers
- `beschreibung`: Klartext-Beschreibung
- `beispiele`: Liste typischer Formulierungen
- `kategorie`: Thematische Einordnung
- `tags`: Klassifikations-Tags
- `risk_score`: Risiko-Gewichtung (1-5)
- `semantics_detector`: Optional - Python-Detektor-Datei

### Risiko-Level:
- **Gr√ºn**: Kein oder nur unkritischer Marker
- **Gelb**: 1-2 moderate Marker, erste Drift erkennbar
- **Blinkend**: 3+ Marker oder ein Hochrisiko-Marker
- **Rot**: Hochrisiko-Kombination, massive Manipulation

Generiert am: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    with open("MARKER_MASTER_README.md", "w", encoding="utf-8") as f:
        f.write(readme_content)
    
    print("  - README: MARKER_MASTER_README.md")


if __name__ == "__main__":
    main() 

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/create_marker_master.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/detect_adaptive_polarization.py
# ====================================================================

def detect_adaptive_polarization(messages, marker_lookup, window=20, min_cycles=2):
    """
    Sucht OPENNESS‚ÜíWITHDRAWAL-Zyklen im letzten 'window' Nachrichten.
    """
    states = []
    for msg in messages[-window:]:
        mset = set(marker_lookup[msg["id"]])
        if "OPENNESS" in mset:
            states.append(1)
        elif "WITHDRAWAL" in mset:
            states.append(-1)

    # Z√§hle Sign-Flips (1‚Üí-1 oder -1‚Üí1)
    flips = sum(1 for i in range(1, len(states)) if states[i] != states[i-1])
    if flips >= min_cycles * 2:     # zwei vollst√§ndige Hin-und-Her-Bewegungen
        return True, {"flips": flips}
    return False, {}


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/detect_adaptive_polarization.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/detect_secret_bonding.py
# ====================================================================

def detect_secret_bonding(messages, marker_lookup):
    """
    Trigger, wenn sustained_contact + >3 Erw√§hnungen von secrecy/exclusion
    innerhalb von 30 Nachrichten vorkommen.
    """
    secrecy_markers = {"S_MENTION_OF_SECRECY", "C_EXCLUSION_OF_OTHERS"}
    hits = 0
    for msg in messages[-30:]:
        if secrecy_markers & set(marker_lookup[msg["id"]]):
            hits += 1
    if hits >= 3:
        return True, {"secrecy_refs": hits}
    return False, {}


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/detect_secret_bonding.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/detect_sustain_contact.py
# ====================================================================

def detect_sustained_contact(messages, min_per_week=3, duration_days=60):
    """
    Pr√ºft, ob √ºber 'duration_days' eine durchschnittliche Frequenz
    von 'min_per_week' Kontakten erreicht wird.
    messages: chronologische Liste [{ts, id, markers:[...]}]
    """
    if not messages:
        return False, {}
    duration = (messages[-1]["ts"] - messages[0]["ts"]).days
    if duration < duration_days:
        return False, {}

    weeks = max(1, duration // 7)
    if len(messages) / weeks >= min_per_week:
        return True, {"weeks": weeks, "count": len(messages)}
    return False, {}


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/detect_sustain_contact.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/detector.utils.py
# ====================================================================

from collections import deque
from datetime import timedelta
import numpy as np

def sliding_window(items, window_sec, key=lambda x: x["ts"]):
    """Yield items that fall inside a moving time-window."""
    window = deque()
    for it in items:
        window.append(it)
        while window and (key(it) - key(window[0])).total_seconds() > window_sec:
            window.popleft()
        yield list(window)

def linear_slope(values):
    """Returns slope of a simple linear regression over equally spaced points."""
    if len(values) < 2:
        return 0.0
    x = np.arange(len(values))
    return np.polyfit(x, values, 1)[0]


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/detector.utils.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/fuzzy_engine.py
# ====================================================================

"""Fuzzy-Matching Engine f√ºr flexible Marker-Erkennung."""

import re
from typing import List, Tuple, Optional, Dict
from difflib import SequenceMatcher
import logging

try:
    from fuzzywuzzy import fuzz, process
    FUZZYWUZZY_AVAILABLE = True
except ImportError:
    FUZZYWUZZY_AVAILABLE = False
    logging.warning("fuzzywuzzy nicht installiert, verwende Fallback")

logger = logging.getLogger(__name__)


class FuzzyMatcher:
    """Engine f√ºr Fuzzy-String-Matching."""
    
    def __init__(self, threshold: float = 0.85):
        self.threshold = threshold
        self.use_fuzzywuzzy = FUZZYWUZZY_AVAILABLE
        
    def find_fuzzy_matches(
        self,
        text: str,
        patterns: List[str],
        threshold: Optional[float] = None
    ) -> List[Tuple[str, int, int, float]]:
        """Findet Fuzzy-Matches in einem Text.
        
        Args:
            text: Der zu durchsuchende Text
            patterns: Liste von Patterns zum Suchen
            threshold: Mindest-√Ñhnlichkeit (0-1)
            
        Returns:
            Liste von (matched_text, start_pos, end_pos, confidence)
        """
        if threshold is None:
            threshold = self.threshold
            
        matches = []
        text_lower = text.lower()
        words = text.split()
        
        for pattern in patterns:
            pattern_lower = pattern.lower()
            pattern_words = pattern.split()
            pattern_len = len(pattern_words)
            
            # Exakte Substring-Suche zuerst
            start = 0
            while True:
                pos = text_lower.find(pattern_lower, start)
                if pos == -1:
                    break
                matches.append((
                    text[pos:pos + len(pattern)],
                    pos,
                    pos + len(pattern),
                    1.0  # Exakter Match = 100% Konfidenz
                ))
                start = pos + 1
            
            # Fuzzy-Matching auf Wort-Ebene
            if pattern_len <= len(words):
                for i in range(len(words) - pattern_len + 1):
                    window = words[i:i + pattern_len]
                    window_text = ' '.join(window)
                    
                    similarity = self._calculate_similarity(
                        window_text.lower(),
                        pattern_lower
                    )
                    
                    if similarity >= threshold:
                        # Finde Position im Original-Text
                        start_pos = text.find(window[0], 0 if i == 0 else len(' '.join(words[:i])))
                        end_pos = text.find(window[-1], start_pos) + len(window[-1])
                        
                        matches.append((
                            text[start_pos:end_pos],
                            start_pos,
                            end_pos,
                            similarity
                        ))
        
        # Deduplizierung - behalte nur beste Matches f√ºr √ºberlappende Bereiche
        matches = self._deduplicate_matches(matches)
        
        return matches
    
    def find_semantic_matches(
        self,
        text: str,
        semantic_groups: Dict[str, List[str]],
        threshold: Optional[float] = None
    ) -> List[Tuple[str, str, int, int, float]]:
        """Findet semantisch √§hnliche Matches.
        
        Args:
            text: Der zu durchsuchende Text
            semantic_groups: Dict von Gruppe -> Synonyme/Varianten
            threshold: Mindest-√Ñhnlichkeit
            
        Returns:
            Liste von (group_name, matched_text, start_pos, end_pos, confidence)
        """
        if threshold is None:
            threshold = self.threshold
            
        semantic_matches = []
        
        for group_name, variants in semantic_groups.items():
            matches = self.find_fuzzy_matches(text, variants, threshold)
            
            for match_text, start, end, conf in matches:
                semantic_matches.append((
                    group_name,
                    match_text,
                    start,
                    end,
                    conf
                ))
        
        return semantic_matches
    
    def _calculate_similarity(self, str1: str, str2: str) -> float:
        """Berechnet √Ñhnlichkeit zwischen zwei Strings."""
        if self.use_fuzzywuzzy:
            # Verwende verschiedene Fuzzy-Metriken und nimm Maximum
            ratio = fuzz.ratio(str1, str2) / 100.0
            partial = fuzz.partial_ratio(str1, str2) / 100.0
            token_sort = fuzz.token_sort_ratio(str1, str2) / 100.0
            
            return max(ratio, partial, token_sort)
        else:
            # Fallback: SequenceMatcher
            return SequenceMatcher(None, str1, str2).ratio()
    
    def _deduplicate_matches(
        self,
        matches: List[Tuple[str, int, int, float]]
    ) -> List[Tuple[str, int, int, float]]:
        """Entfernt √ºberlappende Matches, beh√§lt die mit h√∂chster Konfidenz."""
        if not matches:
            return []
        
        # Sortiere nach Konfidenz (absteigend) und dann Position
        sorted_matches = sorted(matches, key=lambda x: (-x[3], x[1]))
        
        deduplicated = []
        used_ranges = []
        
        for match in sorted_matches:
            _, start, end, _ = match
            
            # Pr√ºfe √úberlappung mit bereits verwendeten Bereichen
            overlaps = False
            for used_start, used_end in used_ranges:
                if not (end <= used_start or start >= used_end):
                    overlaps = True
                    break
            
            if not overlaps:
                deduplicated.append(match)
                used_ranges.append((start, end))
        
        # Sortiere nach Position f√ºr Output
        return sorted(deduplicated, key=lambda x: x[1])


class RegexMatcher:
    """Engine f√ºr Regex-basiertes Matching."""
    
    def __init__(self):
        self._compiled_patterns: Dict[str, re.Pattern] = {}
    
    def find_regex_matches(
        self,
        text: str,
        patterns: List[str],
        case_sensitive: bool = False
    ) -> List[Tuple[str, int, int]]:
        """Findet Regex-Matches in einem Text.
        
        Args:
            text: Der zu durchsuchende Text
            patterns: Liste von Regex-Patterns
            case_sensitive: Gro√ü-/Kleinschreibung beachten
            
        Returns:
            Liste von (matched_text, start_pos, end_pos)
        """
        matches = []
        flags = 0 if case_sensitive else re.IGNORECASE
        
        for pattern in patterns:
            try:
                # Compile und cache Pattern
                cache_key = f"{pattern}_{flags}"
                if cache_key not in self._compiled_patterns:
                    self._compiled_patterns[cache_key] = re.compile(pattern, flags)
                
                regex = self._compiled_patterns[cache_key]
                
                for match in regex.finditer(text):
                    matches.append((
                        match.group(),
                        match.start(),
                        match.end()
                    ))
                    
            except re.error as e:
                logger.error(f"Ung√ºltiges Regex-Pattern '{pattern}': {e}")
                continue
        
        return matches
    
    def extract_context(
        self,
        text: str,
        position: int,
        context_words: int = 10
    ) -> str:
        """Extrahiert Kontext um eine Position im Text.
        
        Args:
            text: Der Gesamttext
            position: Position im Text
            context_words: Anzahl W√∂rter vor/nach der Position
            
        Returns:
            Kontext-String
        """
        words = text.split()
        word_positions = []
        current_pos = 0
        
        # Finde Wort-Positionen
        for word in words:
            word_start = text.find(word, current_pos)
            word_end = word_start + len(word)
            word_positions.append((word_start, word_end))
            current_pos = word_end
        
        # Finde Wort, das die Position enth√§lt
        target_word_idx = 0
        for i, (start, end) in enumerate(word_positions):
            if start <= position <= end:
                target_word_idx = i
                break
        
        # Extrahiere Kontext
        start_idx = max(0, target_word_idx - context_words)
        end_idx = min(len(words), target_word_idx + context_words + 1)
        
        context_words_list = words[start_idx:end_idx]
        
        # Markiere Target
        if start_idx < target_word_idx < end_idx:
            relative_idx = target_word_idx - start_idx
            context_words_list[relative_idx] = f"**{context_words_list[relative_idx]}**"
        
        return ' '.join(context_words_list)

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/fuzzy_engine.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/logging_config.py
# ====================================================================

"""Logging-Konfiguration f√ºr die Marker Analysis Engine."""

import logging
import logging.handlers
import sys
from pathlib import Path
from typing import Optional
import colorlog


def setup_logging(
    level: str = "INFO",
    log_file: Optional[Path] = None,
    console: bool = True,
    colored: bool = True,
    rotation: bool = True,
    max_bytes: int = 10 * 1024 * 1024,  # 10MB
    backup_count: int = 5
) -> logging.Logger:
    """Konfiguriert das Logging-System.
    
    Args:
        level: Logging-Level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        log_file: Pfad zur Log-Datei (optional)
        console: Ob auf Konsole geloggt werden soll
        colored: Ob farbige Ausgabe verwendet werden soll
        rotation: Ob Log-Rotation aktiviert werden soll
        max_bytes: Maximale Gr√∂√üe einer Log-Datei
        backup_count: Anzahl der Backup-Dateien
    
    Returns:
        Konfigurierter Root-Logger
    """
    # Root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, level.upper()))
    
    # Entferne existierende Handler
    root_logger.handlers.clear()
    
    # Format f√ºr Logs
    detailed_format = (
        '%(asctime)s - %(name)s - %(levelname)s - '
        '%(filename)s:%(lineno)d - %(funcName)s() - %(message)s'
    )
    simple_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    # Console Handler
    if console:
        if colored and colorlog:
            console_handler = colorlog.StreamHandler(sys.stdout)
            console_format = colorlog.ColoredFormatter(
                '%(log_color)s%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S',
                log_colors={
                    'DEBUG': 'cyan',
                    'INFO': 'green',
                    'WARNING': 'yellow',
                    'ERROR': 'red',
                    'CRITICAL': 'red,bg_white',
                }
            )
            console_handler.setFormatter(console_format)
        else:
            console_handler = logging.StreamHandler(sys.stdout)
            console_handler.setFormatter(logging.Formatter(simple_format))
        
        root_logger.addHandler(console_handler)
    
    # File Handler
    if log_file:
        # Erstelle Log-Verzeichnis falls n√∂tig
        log_file = Path(log_file)
        log_file.parent.mkdir(parents=True, exist_ok=True)
        
        if rotation:
            file_handler = logging.handlers.RotatingFileHandler(
                log_file,
                maxBytes=max_bytes,
                backupCount=backup_count,
                encoding='utf-8'
            )
        else:
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
        
        file_handler.setFormatter(logging.Formatter(detailed_format))
        root_logger.addHandler(file_handler)
    
    # Spezielle Logger-Konfigurationen
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('matplotlib').setLevel(logging.WARNING)
    
    return root_logger


def get_logger(name: str) -> logging.Logger:
    """Gibt einen benannten Logger zur√ºck.
    
    Args:
        name: Name des Loggers (normalerweise __name__)
    
    Returns:
        Logger-Instanz
    """
    return logging.getLogger(name)


class LogContext:
    """Context Manager f√ºr tempor√§re Logging-Level-√Ñnderungen."""
    
    def __init__(self, logger: logging.Logger, level: str):
        self.logger = logger
        self.new_level = getattr(logging, level.upper())
        self.old_level = None
    
    def __enter__(self):
        self.old_level = self.logger.level
        self.logger.setLevel(self.new_level)
        return self.logger
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.logger.setLevel(self.old_level)


def log_function_call(func):
    """Decorator zum Loggen von Funktionsaufrufen."""
    logger = logging.getLogger(func.__module__)
    
    def wrapper(*args, **kwargs):
        logger.debug(f"Calling {func.__name__} with args={args}, kwargs={kwargs}")
        try:
            result = func(*args, **kwargs)
            logger.debug(f"{func.__name__} returned {result}")
            return result
        except Exception as e:
            logger.error(f"{func.__name__} raised {type(e).__name__}: {e}")
            raise
    
    return wrapper


# Standard-Setup beim Import
if not logging.getLogger().handlers:
    setup_logging()

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/logging_config.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/main.py
# ====================================================================

from fastapi import FastAPI, File, UploadFile
from fastapi.responses import HTMLResponse
from pymongo import MongoClient
import yaml, datetime

app = FastAPI()

# ‚û°Ô∏è MongoDB-Zugangsdaten HIER eintragen (ersetze die Platzhalter!):
client = MongoClient("mongodb+srv://<USER>:<PASS>@<CLUSTER>.mongodb.net/mydb?retryWrites=true&w=majority")
db = client["mydb"]["markerdocs"]

def valid_marker(m):
    return all(x in m for x in ("id", "description", "examples"))

@app.get("/", response_class=HTMLResponse)
def upload_form():
    return """
    <html>
    <body>
    <h2>Marker Datei Upload</h2>
    <form action="/upload/" enctype="multipart/form-data" method="post">
      <input name="file" type="file">
      <input type="submit">
    </form>
    </body>
    </html>
    """

@app.post("/upload/", response_class=HTMLResponse)
async def upload_file(file: UploadFile = File(...)):
    content = await file.read()
    data = yaml.safe_load(content)
    items = data if isinstance(data, list) else [data]
    imported, failed = [], []
    for m in items:
        if valid_marker(m):
            m["_audit"] = {"ts": datetime.datetime.utcnow().isoformat(), "src": file.filename}
            db.replace_one({"id": m["id"]}, m, upsert=True)
            imported.append(m["id"])
        else:
            failed.append(m.get("id", "?"))
    return f"<b>Importiert:</b> {imported}<br><b>Fehlerhaft:</b> {failed}"

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/main.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/marker_matcher.py
# ====================================================================

#!/usr/bin/env python3
"""
Marker Matcher - Semantisch-psychologischer Resonanz- und Manipulations-Detektor
Analysiert Texte basierend auf dem Marker Master Export
"""

import yaml
import json
import re
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from pathlib import Path
import logging
from datetime import datetime
from collections import defaultdict

# Logging konfigurieren
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class MarkerMatch:
    """Repr√§sentiert einen gefundenen Marker-Treffer"""
    marker_name: str
    marker_beschreibung: str
    matched_text: str
    position: Tuple[int, int]  # (start, end)
    confidence_score: float = 1.0
    kontext: str = ""
    pattern_type: str = "exact"  # exact, fuzzy, semantic
    tags: List[str] = field(default_factory=list)
    risk_score: int = 1


@dataclass
class AnalysisResult:
    """Ergebnis der Marker-Analyse"""
    text: str
    timestamp: str
    gefundene_marker: List[MarkerMatch]
    risk_level: str
    risk_level_color: str
    total_risk_score: int
    categories_found: Dict[str, int]
    summary: str
    
    def to_dict(self) -> Dict[str, Any]:
        """Konvertiert das Ergebnis in ein Dictionary"""
        return {
            'timestamp': self.timestamp,
            'text_preview': self.text[:100] + '...' if len(self.text) > 100 else self.text,
            'gefundene_marker': [
                {
                    'marker': m.marker_name,
                    'beschreibung': m.marker_beschreibung,
                    'matched_text': m.matched_text,
                    'position': m.position,
                    'confidence': m.confidence_score,
                    'tags': m.tags
                } for m in self.gefundene_marker
            ],
            'risk_level': self.risk_level,
            'risk_level_color': self.risk_level_color,
            'total_risk_score': self.total_risk_score,
            'categories': self.categories_found,
            'summary': self.summary
        }


class MarkerMatcher:
    """Hauptklasse f√ºr Marker-basierte Textanalyse"""
    
    def __init__(self, marker_file: str = "marker_master_export.yaml"):
        """Initialisiert den Matcher mit Marker-Daten"""
        self.markers = {}
        self.semantic_detectors = {}
        self.risk_thresholds = {
            'green': (0, 1),
            'yellow': (2, 5),
            'blinking': (6, 10),
            'red': (11, float('inf'))
        }
        
        # Lade Marker-Daten
        self._load_markers(marker_file)
        
    def _load_markers(self, marker_file: str):
        """L√§dt Marker-Daten aus YAML-Datei"""
        try:
            with open(marker_file, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
            
            # Extrahiere Marker
            for marker in data.get('markers', []):
                self.markers[marker['marker']] = marker
                
            # Extrahiere Risk-Level-Definitionen
            if 'risk_levels' in data:
                self.risk_level_descriptions = data['risk_levels']
                
            logger.info(f"{len(self.markers)} Marker geladen")
            
        except Exception as e:
            logger.error(f"Fehler beim Laden der Marker-Datei: {e}")
            raise
    
    def analyze_text(self, text: str) -> AnalysisResult:
        """Analysiert einen Text auf Marker"""
        logger.debug(f"Analysiere Text mit {len(text)} Zeichen")
        
        # Initialisiere Ergebnis
        matches = []
        categories_count = defaultdict(int)
        total_risk_score = 0
        
        # Durchsuche Text nach jedem Marker
        for marker_name, marker_data in self.markers.items():
            marker_matches = self._find_marker_in_text(text, marker_data)
            
            for match in marker_matches:
                matches.append(match)
                categories_count[marker_data.get('kategorie', 'UNCATEGORIZED')] += 1
                total_risk_score += match.risk_score
        
        # Bestimme Risk-Level
        risk_level, risk_color = self._calculate_risk_level(total_risk_score, len(matches))
        
        # Erstelle Zusammenfassung
        summary = self._generate_summary(matches, risk_level)
        
        return AnalysisResult(
            text=text,
            timestamp=datetime.now().isoformat(),
            gefundene_marker=matches,
            risk_level=risk_level,
            risk_level_color=risk_color,
            total_risk_score=total_risk_score,
            categories_found=dict(categories_count),
            summary=summary
        )
    
    def _find_marker_in_text(self, text: str, marker_data: Dict[str, Any]) -> List[MarkerMatch]:
        """Sucht nach einem spezifischen Marker im Text"""
        matches = []
        
        # Suche nach Beispielen
        for beispiel in marker_data.get('beispiele', []):
            if not beispiel:  # √úberspringe leere Beispiele
                continue
                
            # Normalisiere f√ºr besseres Matching
            beispiel_normalized = beispiel.lower().strip()
            text_lower = text.lower()
            
            # Exakte Suche
            if beispiel_normalized in text_lower:
                start = text_lower.find(beispiel_normalized)
                end = start + len(beispiel_normalized)
                
                match = MarkerMatch(
                    marker_name=marker_data['marker'],
                    marker_beschreibung=marker_data.get('beschreibung', ''),
                    matched_text=text[start:end],
                    position=(start, end),
                    confidence_score=1.0,
                    kontext=self._extract_context(text, start, end),
                    pattern_type='exact',
                    tags=marker_data.get('tags', []),
                    risk_score=marker_data.get('risk_score', 1)
                )
                matches.append(match)
            
            # Fuzzy-Matching f√ºr teilweise √úbereinstimmungen
            elif self._fuzzy_match(beispiel_normalized, text_lower):
                # Vereinfachtes Fuzzy-Matching
                words = beispiel_normalized.split()
                if len(words) > 2 and sum(1 for w in words if w in text_lower) >= len(words) * 0.7:
                    match = MarkerMatch(
                        marker_name=marker_data['marker'],
                        marker_beschreibung=marker_data.get('beschreibung', ''),
                        matched_text=beispiel[:50] + '...',
                        position=(0, 0),
                        confidence_score=0.7,
                        pattern_type='fuzzy',
                        tags=marker_data.get('tags', []),
                        risk_score=marker_data.get('risk_score', 1)
                    )
                    matches.append(match)
        
        # Semantische Patterns (falls vorhanden)
        if 'semantic_patterns' in marker_data:
            semantic_matches = self._apply_semantic_patterns(text, marker_data)
            matches.extend(semantic_matches)
        
        return matches
    
    def _fuzzy_match(self, pattern: str, text: str) -> bool:
        """Einfaches Fuzzy-Matching"""
        # Sehr vereinfachte Version - kann erweitert werden
        words = pattern.split()
        return len(words) > 2 and sum(1 for w in words if w in text) >= len(words) * 0.6
    
    def _apply_semantic_patterns(self, text: str, marker_data: Dict[str, Any]) -> List[MarkerMatch]:
        """Wendet semantische Patterns an (falls definiert)"""
        matches = []
        semantic_data = marker_data.get('semantic_patterns', {})
        
        if 'patterns' in semantic_data:
            for pattern_rule in semantic_data['patterns']:
                if 'pattern' in pattern_rule:
                    try:
                        # Kompiliere Regex-Pattern
                        regex = re.compile(pattern_rule['pattern'], re.IGNORECASE)
                        
                        for match in regex.finditer(text):
                            marker_match = MarkerMatch(
                                marker_name=marker_data['marker'],
                                marker_beschreibung=marker_data.get('beschreibung', ''),
                                matched_text=match.group(0),
                                position=(match.start(), match.end()),
                                confidence_score=0.9,
                                kontext=self._extract_context(text, match.start(), match.end()),
                                pattern_type='semantic',
                                tags=marker_data.get('tags', []),
                                risk_score=marker_data.get('risk_score', 1)
                            )
                            matches.append(marker_match)
                    except Exception as e:
                        logger.debug(f"Fehler bei Regex-Pattern: {e}")
        
        return matches
    
    def _extract_context(self, text: str, start: int, end: int, context_size: int = 50) -> str:
        """Extrahiert Kontext um einen Match"""
        context_start = max(0, start - context_size)
        context_end = min(len(text), end + context_size)
        
        context = text[context_start:context_end]
        
        # Markiere den gefundenen Text
        if context_start > 0:
            context = "..." + context
        if context_end < len(text):
            context = context + "..."
            
        return context
    
    def _calculate_risk_level(self, total_score: int, marker_count: int) -> Tuple[str, str]:
        """Berechnet das Risiko-Level basierend auf Score und Anzahl"""
        # Ber√ºcksichtige sowohl Score als auch Anzahl
        adjusted_score = total_score + (marker_count * 0.5)
        
        for level, (min_score, max_score) in self.risk_thresholds.items():
            if min_score <= adjusted_score <= max_score:
                return level, self._get_color_for_level(level)
        
        return 'red', '#FF0000'
    
    def _get_color_for_level(self, level: str) -> str:
        """Gibt die Farbe f√ºr ein Risk-Level zur√ºck"""
        colors = {
            'green': '#00FF00',
            'yellow': '#FFFF00',
            'blinking': '#FFA500',  # Orange f√ºr "blinkend"
            'red': '#FF0000'
        }
        return colors.get(level, '#808080')
    
    def _generate_summary(self, matches: List[MarkerMatch], risk_level: str) -> str:
        """Generiert eine Zusammenfassung der Analyse"""
        if not matches:
            return "Keine kritischen Marker gefunden. Die Kommunikation erscheint neutral."
        
        # Z√§hle Marker-Typen
        marker_types = defaultdict(int)
        for match in matches:
            marker_types[match.marker_name] += 1
        
        # Top 3 Marker
        top_markers = sorted(marker_types.items(), key=lambda x: x[1], reverse=True)[:3]
        
        summary = f"Risiko-Level: {risk_level.upper()}\n"
        summary += f"Gefundene Marker: {len(matches)}\n"
        summary += f"H√§ufigste Muster: "
        summary += ", ".join([f"{name} ({count}x)" for name, count in top_markers])
        
        # F√ºge spezifische Warnungen hinzu
        if risk_level in ['blinking', 'red']:
            summary += "\n‚ö†Ô∏è WARNUNG: Deutliche Anzeichen f√ºr manipulative Kommunikation erkannt!"
        
        return summary
    
    def analyze_batch(self, texts: List[str]) -> List[AnalysisResult]:
        """Analysiert mehrere Texte"""
        results = []
        for text in texts:
            results.append(self.analyze_text(text))
        return results


def main():
    """Demo-Funktion"""
    # Erstelle Matcher
    matcher = MarkerMatcher()
    
    # Test-Texte
    test_texts = [
        "Das hast du dir nur eingebildet. Ich habe nie gesagt, dass ich mitkomme.",
        "Ich liebe dich √ºber alles! Du bist meine Seelenverwandte. Niemand versteht mich so wie du.",
        "Ich bin hin- und hergerissen zwischen Bleiben und Gehen ‚Äì ich wei√ü nicht mehr weiter.",
        "Hey, wie geht's dir? Sch√∂nes Wetter heute, nicht wahr?",
        "Du bist zu empfindlich. Das war doch nur ein Scherz. Du √ºbertreibst mal wieder."
    ]
    
    print("\nMARKER MATCHER - DEMO")
    print("="*60)
    
    for i, text in enumerate(test_texts, 1):
        print(f"\nText {i}: {text[:50]}...")
        result = matcher.analyze_text(text)
        
        print(f"Risk-Level: {result.risk_level} ({result.risk_level_color})")
        print(f"Gefundene Marker: {len(result.gefundene_marker)}")
        
        for match in result.gefundene_marker:
            print(f"  - {match.marker_name}: '{match.matched_text}'")
        
        print(f"\nZusammenfassung:\n{result.summary}")
        print("-"*60)


if __name__ == "__main__":
    main() 

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/marker_matcher.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/marker_models.py
# ====================================================================

"""Datenmodelle f√ºr Marker-Definitionen und -Treffer."""

from typing import List, Optional, Dict, Any, Union
from datetime import datetime
from enum import Enum
from pydantic import BaseModel, Field, validator


class MarkerCategory(str, Enum):
    """Kategorien f√ºr verschiedene Marker-Typen."""
    
    FRAUD = "fraud"
    MANIPULATION = "manipulation"
    GASLIGHTING = "gaslighting"
    LOVE_BOMBING = "love_bombing"
    EMOTIONAL_ABUSE = "emotional_abuse"
    FINANCIAL_ABUSE = "financial_abuse"
    POSITIVE = "positive"
    EMPATHY = "empathy"
    SUPPORT = "support"
    CONFLICT_RESOLUTION = "conflict_resolution"
    BOUNDARY_SETTING = "boundary_setting"
    SELF_CARE = "self_care"


class MarkerSeverity(str, Enum):
    """Schweregrad eines Markers."""
    
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class MarkerPattern(BaseModel):
    """Ein einzelnes Pattern zur Marker-Erkennung."""
    
    pattern: str = Field(..., description="Regex oder Text-Pattern")
    is_regex: bool = Field(default=False, description="Ob Pattern als Regex interpretiert werden soll")
    case_sensitive: bool = Field(default=False, description="Gro√ü-/Kleinschreibung beachten")
    fuzzy_threshold: Optional[float] = Field(
        default=0.85, 
        ge=0.0, 
        le=1.0,
        description="Schwellenwert f√ºr Fuzzy-Matching (0-1)"
    )
    context_words: Optional[int] = Field(
        default=10,
        description="Anzahl W√∂rter f√ºr Kontext-Extraktion"
    )


class MarkerDefinition(BaseModel):
    """Definition eines Markers mit allen relevanten Eigenschaften."""
    
    id: str = Field(..., description="Eindeutige Marker-ID")
    name: str = Field(..., description="Menschenlesbarer Name")
    category: MarkerCategory = Field(..., description="Marker-Kategorie")
    severity: MarkerSeverity = Field(default=MarkerSeverity.MEDIUM)
    description: str = Field(..., description="Ausf√ºhrliche Beschreibung")
    
    patterns: List[MarkerPattern] = Field(
        default_factory=list,
        description="Liste von Patterns zur Erkennung"
    )
    
    keywords: List[str] = Field(
        default_factory=list,
        description="Schl√ºsselw√∂rter f√ºr einfaches Matching"
    )
    
    examples: List[str] = Field(
        default_factory=list,
        description="Beispiele f√ºr diesen Marker"
    )
    
    weight: float = Field(
        default=1.0,
        ge=0.0,
        le=10.0,
        description="Gewichtung f√ºr Scoring (0-10)"
    )
    
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="Zus√§tzliche Metadaten"
    )
    
    active: bool = Field(default=True, description="Ob Marker aktiv ist")
    
    @validator('patterns')
    def validate_patterns(cls, v):
        """Stelle sicher, dass mindestens ein Pattern oder Keyword existiert."""
        if not v:
            return v
        return v
    
    def get_all_patterns(self) -> List[str]:
        """Gibt alle Patterns und Keywords zur√ºck."""
        all_patterns = []
        for p in self.patterns:
            all_patterns.append(p.pattern)
        all_patterns.extend(self.keywords)
        return all_patterns


class MarkerMatch(BaseModel):
    """Ein gefundener Marker-Treffer in einem Text."""
    
    marker_id: str = Field(..., description="ID des gefundenen Markers")
    marker_name: str = Field(..., description="Name des Markers")
    category: MarkerCategory = Field(..., description="Kategorie des Markers")
    severity: MarkerSeverity = Field(..., description="Schweregrad")
    
    text: str = Field(..., description="Gefundener Text")
    context: str = Field(..., description="Kontext um den Fund")
    
    chunk_id: str = Field(..., description="ID des Text-Chunks")
    position: int = Field(..., description="Position im Text")
    
    confidence: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="Konfidenz des Matches (0-1)"
    )
    
    speaker: Optional[str] = Field(None, description="Sprecher/Autor")
    timestamp: Optional[datetime] = Field(None, description="Zeitstempel")
    
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="Zus√§tzliche Match-Metadaten"
    )


class MarkerStatistics(BaseModel):
    """Statistiken √ºber Marker-Treffer."""
    
    total_matches: int = Field(default=0)
    matches_by_category: Dict[str, int] = Field(default_factory=dict)
    matches_by_severity: Dict[str, int] = Field(default_factory=dict)
    matches_by_speaker: Dict[str, int] = Field(default_factory=dict)
    
    average_confidence: float = Field(default=0.0)
    time_distribution: Dict[str, int] = Field(
        default_factory=dict,
        description="Verteilung √ºber Zeit (z.B. pro Stunde/Tag)"
    )
    
    most_frequent_markers: List[Dict[str, Union[str, int]]] = Field(
        default_factory=list,
        description="Top Marker nach H√§ufigkeit"
    )


class MarkerProfile(BaseModel):
    """Profil einer Person basierend auf Marker-Analyse."""
    
    speaker_id: str = Field(..., description="ID des Sprechers")
    total_messages: int = Field(default=0)
    
    positive_markers: int = Field(default=0)
    negative_markers: int = Field(default=0)
    
    dominant_categories: List[str] = Field(default_factory=list)
    risk_score: float = Field(default=0.0, ge=0.0, le=10.0)
    
    marker_timeline: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Zeitliche Entwicklung der Marker"
    )
    
    behavioral_patterns: Dict[str, Any] = Field(
        default_factory=dict,
        description="Erkannte Verhaltensmuster"
    )

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/marker_models.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/role_models.py
# ====================================================================

"""Datenmodelle f√ºr Rollen- und Attribut-Tracking."""

from typing import List, Dict, Optional, Any, Set
from datetime import datetime
from pydantic import BaseModel, Field, validator
from enum import Enum

from ..matcher.marker_models import MarkerCategory


class AttributeType(str, Enum):
    """Typen von Personen

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/role_models.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/schema_loader.py
# ====================================================================

import json

class SchemaLoader:
    def __init__(self, schema_path):
        self.schema_path = schema_path
        self.schema = self.load_schema()

    def load_schema(self):
        with open(self.schema_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def get_included_marker_names(self):
        markers = []
        for group in ["atomic_markers", "cluster_markers", "semantic_markers", "meta_markers"]:
            entries = self.schema.get("marker_detection", {}).get(group, [])
            markers.extend([entry["name"] for entry in entries])
        return markers

    def get_drift_axes(self):
        return self.schema.get("drift_analysis", {}).get("drift_axes", [])

    def get_metrics(self):
        return self.schema.get("relationship_inference", {})

    def get_visualization_options(self):
        return self.schema.get("outputs", {}).get("visualization", [])

    def get_export_formats(self):
        return self.schema.get("outputs", {}).get("export_format", [])


# Beispielhafte Nutzung:
# loader = SchemaLoader("schemas/beziehungsanalyse_schema.json")
# print(loader.get_included_marker_names())
# print(loader.get_drift_axes())
# print(loader.get_metrics())


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/schema_loader.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/score_models.py
# ====================================================================

"""Datenmodelle f√ºr das Scoring-System."""

from typing import Dict, List, Optional, Any
from datetime import datetime
from pydantic import BaseModel, Field, validator
from enum import Enum

from ..matcher.marker_models import MarkerCategory, MarkerSeverity


class ScoreType(str, Enum):
    """Verschiedene Score-Typen f√ºr unterschiedliche Analysen."""
    
    MANIPULATION_INDEX = "manipulation_index"
    RELATIONSHIP_HEALTH = "relationship_health"
    FRAUD_PROBABILITY = "fraud_probability"
    EMOTIONAL_SUPPORT = "emotional_support"
    CONFLICT_LEVEL = "conflict_level"
    COMMUNICATION_QUALITY = "communication_quality"
    TRUST_LEVEL = "trust_level"
    RESOURCE_BALANCE = "resource_balance"


class ScoringModel(BaseModel):
    """Definition eines Scoring-Modells."""
    
    id: str = Field(..., description="Eindeutige Model-ID")
    name: str = Field(..., description="Name des Scoring-Modells")
    type: ScoreType = Field(..., description="Typ des Scores")
    description: str = Field(..., description="Beschreibung des Modells")
    
    # Gewichtungen f√ºr verschiedene Marker-Kategorien
    category_weights: Dict[MarkerCategory, float] = Field(
        default_factory=dict,
        description="Gewichtung pro Marker-Kategorie"
    )
    
    # Gewichtungen f√ºr Severity-Level
    severity_multipliers: Dict[MarkerSeverity, float] = Field(
        default_factory=lambda: {
            MarkerSeverity.LOW: 0.5,
            MarkerSeverity.MEDIUM: 1.0,
            MarkerSeverity.HIGH: 2.0,
            MarkerSeverity.CRITICAL: 3.0
        },
        description="Multiplikatoren f√ºr Schweregrade"
    )
    
    # Score-Berechnung
    scale_min: float = Field(default=1.0, description="Minimaler Score")
    scale_max: float = Field(default=10.0, description="Maximaler Score")
    inverse_scale: bool = Field(
        default=False,
        description="Ob h√∂here Marker-Counts zu niedrigeren Scores f√ºhren"
    )
    
    # Normalisierung
    normalization_factor: float = Field(
        default=100.0,
        description="Faktor zur Normalisierung der Rohwerte"
    )
    
    # Schwellenwerte f√ºr Interpretation
    thresholds: Dict[str, float] = Field(
        default_factory=lambda: {
            "critical": 8.0,
            "warning": 6.0,
            "normal": 4.0,
            "good": 2.0
        },
        description="Schwellenwerte f√ºr verschiedene Zust√§nde"
    )
    
    active: bool = Field(default=True, description="Ob das Modell aktiv ist")


class ChunkScore(BaseModel):
    """Score f√ºr einen einzelnen Text-Chunk."""
    
    chunk_id: str = Field(..., description="ID des bewerteten Chunks")
    model_id: str = Field(..., description="ID des verwendeten Scoring-Modells")
    score_type: ScoreType = Field(..., description="Typ des Scores")
    
    raw_score: float = Field(..., description="Roher Score vor Normalisierung")
    normalized_score: float = Field(..., ge=1.0, le=10.0, description="Normalisierter Score (1-10)")
    
    contributing_markers: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Marker die zum Score beigetragen haben"
    )
    
    confidence: float = Field(
        default=1.0,
        ge=0.0,
        le=1.0,
        description="Konfidenz des Scores"
    )
    
    timestamp: Optional[datetime] = Field(None, description="Zeitstempel des Chunks")
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    @validator('normalized_score')
    def validate_normalized_score(cls, v, values):
        """Stelle sicher, dass Score im g√ºltigen Bereich ist."""
        if 'score_type' in values:
            return max(1.0, min(10.0, v))
        return v


class AggregatedScore(BaseModel):
    """Aggregierter Score √ºber mehrere Chunks/Zeitr√§ume."""
    
    model_id: str = Field(..., description="ID des Scoring-Modells")
    score_type: ScoreType = Field(..., description="Typ des Scores")
    
    average_score: float = Field(..., ge=1.0, le=10.0)
    min_score: float = Field(..., ge=1.0, le=10.0)
    max_score: float = Field(..., ge=1.0, le=10.0)
    
    trend: str = Field(
        default="stable",
        description="Trend: improving, declining, stable"
    )
    trend_strength: float = Field(
        default=0.0,
        description="St√§rke des Trends (-1 bis 1)"
    )
    
    chunk_count: int = Field(..., description="Anzahl bewerteter Chunks")
    time_range: Optional[Dict[str, datetime]] = Field(
        None,
        description="Zeitbereich der Aggregation"
    )
    
    distribution: Dict[str, int] = Field(
        default_factory=dict,
        description="Verteilung der Scores (z.B. 1-2: 5, 3-4: 10, ...)"
    )
    
    top_markers: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="H√§ufigste beitragende Marker"
    )
    
    metadata: Dict[str, Any] = Field(default_factory=dict)


class ScoreComparison(BaseModel):
    """Vergleich von Scores zwischen Sprechern/Zeitr√§umen."""
    
    comparison_type: str = Field(..., description="speaker_comparison oder time_comparison")
    model_id: str = Field(..., description="Verwendetes Scoring-Modell")
    
    entities: List[str] = Field(
        ...,
        description="Verglichene Entit√§ten (Speaker-IDs oder Zeitr√§ume)"
    )
    
    scores: Dict[str, AggregatedScore] = Field(
        ...,
        description="Scores pro Entit√§t"
    )
    
    differences: Dict[str, float] = Field(
        default_factory=dict,
        description="Unterschiede zwischen Entit√§ten"
    )
    
    winner: Optional[str] = Field(
        None,
        description="Entit√§t mit bestem Score (je nach Modell)"
    )
    
    insights: List[str] = Field(
        default_factory=list,
        description="Automatisch generierte Einsichten"
    )
    
    metadata: Dict[str, Any] = Field(default_factory=dict)


class ScoringResult(BaseModel):
    """Gesamtergebnis einer Scoring-Analyse."""
    
    chunk_scores: List[ChunkScore] = Field(
        default_factory=list,
        description="Einzelne Chunk-Scores"
    )
    
    aggregated_scores: Dict[str, AggregatedScore] = Field(
        default_factory=dict,
        description="Aggregierte Scores pro Modell"
    )
    
    speaker_scores: Dict[str, Dict[str, AggregatedScore]] = Field(
        default_factory=dict,
        description="Scores pro Sprecher und Modell"
    )
    
    timeline: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Zeitliche Entwicklung der Scores"
    )
    
    alerts: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Generierte Warnungen basierend auf Schwellenwerten"
    )
    
    summary: Dict[str, Any] = Field(
        default_factory=dict,
        description="Zusammenfassung der wichtigsten Erkenntnisse"
    )
    
    processing_time: float = Field(0.0)
    
    def get_score_by_type(self, score_type: ScoreType) -> Optional[AggregatedScore]:
        """Holt aggregierten Score f√ºr einen bestimmten Typ."""
        return self.aggregated_scores.get(score_type.value)
    
    def get_speaker_comparison(self, score_type: ScoreType) -> ScoreComparison:
        """Erstellt Vergleich zwischen Sprechern f√ºr einen Score-Typ."""
        comparison = ScoreComparison(
            comparison_type="speaker_comparison",
            model_id=score_type.value,
            entities=list(self.speaker_scores.keys()),
            scores={}
        )
        
        for speaker, scores in self.speaker_scores.items():
            if score_type.value in scores:
                comparison.scores[speaker] = scores[score_type.value]
        
        # Berechne Unterschiede
        if len(comparison.scores) == 2:
            speakers = list(comparison.scores.keys())
            diff = comparison.scores[speakers[0]].average_score - comparison.scores[speakers[1]].average_score
            comparison.differences[f"{speakers[0]}_vs_{speakers[1]}"] = diff
        
        return comparison

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/score_models.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/scoring_engine.py
# ====================================================================

"""Scoring Engine f√ºr die Bewertung von Text-Chunks basierend auf Marker-Treffern."""

import logging
import time
from typing import List, Dict, Optional, Tuple
from collections import defaultdict
from datetime import datetime
import numpy as np

from .score_models import (
    ScoringModel, ChunkScore, AggregatedScore, ScoringResult,
    ScoreType, ScoreComparison
)
from ..matcher.marker_models import MarkerMatch, MarkerCategory, MarkerSeverity
from ..chunker.chunk_models import TextChunk

logger = logging.getLogger(__name__)


class ScoringEngine:
    """Engine zur Berechnung von Scores basierend auf Marker-Matches."""
    
    def __init__(self):
        self.models: Dict[str, ScoringModel] = {}
        self._initialize_default_models()
    
    def _initialize_default_models(self):
        """Initialisiert Standard-Scoring-Modelle."""
        
        # Manipulations-Index
        self.models["manipulation_index"] = ScoringModel(
            id="manipulation_index",
            name="Manipulations-Index",
            type=ScoreType.MANIPULATION_INDEX,
            description="Misst den Grad manipulativer Kommunikation",
            category_weights={
                MarkerCategory.MANIPULATION: 2.0,
                MarkerCategory.GASLIGHTING: 3.0,
                MarkerCategory.EMOTIONAL_ABUSE: 2.5,
                MarkerCategory.LOVE_BOMBING: 1.5,
                MarkerCategory.FRAUD: 3.0,
                MarkerCategory.POSITIVE: -1.0,  # Positive Marker reduzieren Score
                MarkerCategory.EMPATHY: -0.5,
                MarkerCategory.SUPPORT: -0.5
            },
            inverse_scale=False  # H√∂her = mehr Manipulation
        )
        
        # Beziehungsgesundheit
        self.models["relationship_health"] = ScoringModel(
            id="relationship_health",
            name="Beziehungsgesundheit",
            type=ScoreType.RELATIONSHIP_HEALTH,
            description="Bewertet die Gesundheit der Beziehungskommunikation",
            category_weights={
                MarkerCategory.POSITIVE: 2.0,
                MarkerCategory.EMPATHY: 2.5,
                MarkerCategory.SUPPORT: 2.0,
                MarkerCategory.CONFLICT_RESOLUTION: 1.5,
                MarkerCategory.MANIPULATION: -2.0,
                MarkerCategory.GASLIGHTING: -3.0,
                MarkerCategory.EMOTIONAL_ABUSE: -2.5,
                MarkerCategory.FRAUD: -3.0
            },
            inverse_scale=True  # Positive Marker erh√∂hen Score
        )
        
        # Fraud-Wahrscheinlichkeit
        self.models["fraud_probability"] = ScoringModel(
            id="fraud_probability",
            name="Fraud-Wahrscheinlichkeit",
            type=ScoreType.FRAUD_PROBABILITY,
            description="Wahrscheinlichkeit f√ºr Betrug/Scam",
            category_weights={
                MarkerCategory.FRAUD: 3.0,
                MarkerCategory.FINANCIAL_ABUSE: 2.5,
                MarkerCategory.LOVE_BOMBING: 1.5,
                MarkerCategory.MANIPULATION: 1.0,
                MarkerCategory.POSITIVE: -0.5,
                MarkerCategory.EMPATHY: -0.3
            },
            thresholds={
                "critical": 7.0,
                "warning": 5.0,
                "normal": 3.0,
                "low": 1.5
            }
        )
        
        # Kommunikationsqualit√§t
        self.models["communication_quality"] = ScoringModel(
            id="communication_quality",
            name="Kommunikationsqualit√§t",
            type=ScoreType.COMMUNICATION_QUALITY,
            description="Qualit√§t der Kommunikation",
            category_weights={
                MarkerCategory.POSITIVE: 1.5,
                MarkerCategory.EMPATHY: 2.0,
                MarkerCategory.SUPPORT: 1.5,
                MarkerCategory.CONFLICT_RESOLUTION: 2.0,
                MarkerCategory.BOUNDARY_SETTING: 1.0,
                MarkerCategory.SELF_CARE: 0.5,
                MarkerCategory.MANIPULATION: -1.5,
                MarkerCategory.GASLIGHTING: -2.0,
                MarkerCategory.EMOTIONAL_ABUSE: -2.0
            },
            inverse_scale=True
        )
    
    def calculate_scores(
        self,
        chunks: List[TextChunk],
        matches: List[MarkerMatch],
        models: Optional[List[str]] = None
    ) -> ScoringResult:
        """Berechnet Scores f√ºr gegebene Chunks und Matches.
        
        Args:
            chunks: Liste von Text-Chunks
            matches: Liste von Marker-Matches
            models: Spezifische Modelle zur Verwendung (None = alle)
            
        Returns:
            ScoringResult mit allen berechneten Scores
        """
        start_time = time.time()
        result = ScoringResult()
        
        # W√§hle Modelle
        active_models = self._get_active_models(models)
        
        # Gruppiere Matches nach Chunk
        matches_by_chunk = self._group_matches_by_chunk(matches)
        
        # Berechne Scores pro Chunk
        for chunk in chunks:
            chunk_matches = matches_by_chunk.get(chunk.id, [])
            
            for model in active_models:
                chunk_score = self._calculate_chunk_score(
                    chunk,
                    chunk_matches,
                    model
                )
                result.chunk_scores.append(chunk_score)
        
        # Aggregiere Scores
        result.aggregated_scores = self._aggregate_scores(
            result.chunk_scores,
            active_models
        )
        
        # Berechne Speaker-Scores
        result.speaker_scores = self._calculate_speaker_scores(
            chunks,
            result.chunk_scores
        )
        
        # Erstelle Timeline
        result.timeline = self._create_timeline(result.chunk_scores)
        
        # Generiere Alerts
        result.alerts = self._generate_alerts(result.aggregated_scores)
        
        # Erstelle Summary
        result.summary = self._create_summary(result)
        
        result.processing_time = time.time() - start_time
        logger.info(f"Scoring abgeschlossen in {result.processing_time:.2f}s")
        
        return result
    
    def _calculate_chunk_score(
        self,
        chunk: TextChunk,
        matches: List[MarkerMatch],
        model: ScoringModel
    ) -> ChunkScore:
        """Berechnet Score f√ºr einen einzelnen Chunk."""
        raw_score = 0.0
        contributing_markers = []
        
        # Berechne gewichteten Score basierend auf Matches
        for match in matches:
            if match.category in model.category_weights:
                weight = model.category_weights[match.category]
                severity_mult = model.severity_multipliers.get(
                    match.severity,
                    1.0
                )
                
                # Marker-spezifisches Gewicht aus Metadaten
                marker_weight = match.metadata.get('weight', 1.0)
                
                contribution = weight * severity_mult * marker_weight * match.confidence
                raw_score += contribution
                
                contributing_markers.append({
                    'marker_id': match.marker_id,
                    'marker_name': match.marker_name,
                    'category': match.category.value,
                    'severity': match.severity.value,
                    'contribution': contribution,
                    'confidence': match.confidence
                })
        
        # Normalisiere Score
        normalized_score = self._normalize_score(raw_score, model, chunk.word_count)
        
        # Erstelle ChunkScore
        return ChunkScore(
            chunk_id=chunk.id,
            model_id=model.id,
            score_type=model.type,
            raw_score=raw_score,
            normalized_score=normalized_score,
            contributing_markers=contributing_markers,
            confidence=self._calculate_confidence(matches, model),
            timestamp=chunk.timestamp,
            metadata={
                'word_count': chunk.word_count,
                'marker_count': len(matches)
            }
        )
    
    def _normalize_score(
        self,
        raw_score: float,
        model: ScoringModel,
        word_count: int
    ) -> float:
        """Normalisiert einen Raw-Score auf die 1-10 Skala."""
        # Normalisiere basierend auf Wortanzahl
        if word_count > 0:
            normalized = (raw_score / word_count) * model.normalization_factor
        else:
            normalized = 0.0
        
        # Inverse Skalierung wenn n√∂tig
        if model.inverse_scale:
            # Bei inverser Skala: Negative Werte = hoher Score
            if normalized < 0:
                score = model.scale_max + (normalized / 10)  # Skaliere negative Werte
            else:
                score = model.scale_max - (normalized * 2)  # Positive reduzieren Score
        else:
            # Normale Skala: Positive Werte = hoher Score
            score = model.scale_min + (normalized * 2)
        
        # Begrenze auf Min/Max
        return max(model.scale_min, min(model.scale_max, score))
    
    def _calculate_confidence(
        self,
        matches: List[MarkerMatch],
        model: ScoringModel
    ) -> float:
        """Berechnet Konfidenz des Scores basierend auf Matches."""
        if not matches:
            return 0.5  # Niedrige Konfidenz ohne Matches
        
        # Durchschnittliche Match-Konfidenz
        avg_confidence = sum(m.confidence for m in matches) / len(matches)
        
        # Anzahl relevanter Matches
        relevant_matches = [
            m for m in matches 
            if m.category in model.category_weights
        ]
        
        # Konfidenz steigt mit Anzahl relevanter Matches
        count_factor = min(1.0, len(relevant_matches) / 10)
        
        return avg_confidence * 0.7 + count_factor * 0.3
    
    def _aggregate_scores(
        self,
        chunk_scores: List[ChunkScore],
        models: List[ScoringModel]
    ) -> Dict[str, AggregatedScore]:
        """Aggregiert Chunk-Scores zu Gesamt-Scores."""
        aggregated = {}
        
        for model in models:
            model_scores = [
                cs for cs in chunk_scores 
                if cs.model_id == model.id
            ]
            
            if not model_scores:
                continue
            
            scores = [cs.normalized_score for cs in model_scores]
            
            # Berechne Trend
            trend, trend_strength = self._calculate_trend(scores)
            
            # Score-Verteilung
            distribution = self._calculate_distribution(scores)
            
            # Top Marker
            all_markers = []
            for cs in model_scores:
                all_markers.extend(cs.contributing_markers)
            
            marker_counts = defaultdict(int)
            for marker in all_markers:
                marker_counts[marker['marker_name']] += 1
            
            top_markers = [
                {'name': name, 'count': count}
                for name, count in sorted(
                    marker_counts.items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:5]
            ]
            
            aggregated[model.type.value] = AggregatedScore(
                model_id=model.id,
                score_type=model.type,
                average_score=np.mean(scores),
                min_score=min(scores),
                max_score=max(scores),
                trend=trend,
                trend_strength=trend_strength,
                chunk_count=len(model_scores),
                distribution=distribution,
                top_markers=top_markers
            )
        
        return aggregated
    
    def _calculate_trend(
        self,
        scores: List[float]
    ) -> Tuple[str, float]:
        """Berechnet Trend aus einer Score-Serie."""
        if len(scores) < 3:
            return "stable", 0.0
        
        # Einfache lineare Regression
        x = np.arange(len(scores))
        y = np.array(scores)
        
        # Berechne Steigung
        slope = np.polyfit(x, y, 1)[0]
        
        # Normalisiere Steigung
        normalized_slope = slope / np.mean(scores) if np.mean(scores) > 0 else 0
        
        if normalized_slope > 0.1:
            return "improving", min(1.0, normalized_slope)
        elif normalized_slope < -0.1:
            return "declining", max(-1.0, normalized_slope)
        else:
            return "stable", normalized_slope
    
    def _calculate_distribution(
        self,
        scores: List[float]
    ) -> Dict[str, int]:
        """Berechnet Score-Verteilung."""
        distribution = {
            "1-2": 0,
            "3-4": 0,
            "5-6": 0,
            "7-8": 0,
            "9-10": 0
        }
        
        for score in scores:
            if score <= 2:
                distribution["1-2"] += 1
            elif score <= 4:
                distribution["3-4"] += 1
            elif score <= 6:
                distribution["5-6"] += 1
            elif score <= 8:
                distribution["7-8"] += 1
            else:
                distribution["9-10"] += 1
        
        return distribution
    
    def _calculate_speaker_scores(
        self,
        chunks: List[TextChunk],
        chunk_scores: List[ChunkScore]
    ) -> Dict[str, Dict[str, AggregatedScore]]:
        """Berechnet Scores pro Sprecher."""
        speaker_scores = defaultdict(lambda: defaultdict(list))
        
        # Sammle Scores pro Sprecher
        for chunk, scores in zip(chunks, chunk_scores):
            if chunk.speaker:
                speaker_name = chunk.speaker.name
                for score in [s for s in chunk_scores if s.chunk_id == chunk.id]:
                    speaker_scores[speaker_name][score.model_id].append(score)
        
        # Aggregiere pro Sprecher
        result = {}
        for speaker, model_scores in speaker_scores.items():
            result[speaker] = {}
            for model_id, scores in model_scores.items():
                model = self.models[model_id]
                result[speaker][model.type.value] = self._aggregate_scores(
                    scores,
                    [model]
                )[model.type.value]
        
        return result
    
    def _create_timeline(
        self,
        chunk_scores: List[ChunkScore]
    ) -> List[Dict[str, Any]]:
        """Erstellt Timeline der Score-Entwicklung."""
        timeline = []
        
        # Gruppiere nach Zeitstempel
        scores_by_time = defaultdict(lambda: defaultdict(list))
        
        for score in chunk_scores:
            if score.timestamp:
                # Runde auf Stunde
                hour = score.timestamp.replace(minute=0, second=0, microsecond=0)
                scores_by_time[hour][score.model_id].append(score.normalized_score)
        
        # Erstelle Timeline-Eintr√§ge
        for timestamp in sorted(scores_by_time.keys()):
            entry = {
                'timestamp': timestamp.isoformat(),
                'scores': {}
            }
            
            for model_id, scores in scores_by_time[timestamp].items():
                model = self.models[model_id]
                entry['scores'][model.type.value] = {
                    'average': np.mean(scores),
                    'min': min(scores),
                    'max': max(scores),
                    'count': len(scores)
                }
            
            timeline.append(entry)
        
        return timeline
    
    def _generate_alerts(
        self,
        aggregated_scores: Dict[str, AggregatedScore]
    ) -> List[Dict[str, Any]]:
        """Generiert Alerts basierend auf Score-Schwellenwerten."""
        alerts = []
        
        for score_type, agg_score in aggregated_scores.items():
            model = self.models[agg_score.model_id]
            
            # Pr√ºfe Schwellenwerte
            for level, threshold in model.thresholds.items():
                if model.inverse_scale:
                    # Bei inverser Skala: Niedrige Scores sind schlecht
                    if agg_score.average_score <= threshold and level in ['warning', 'critical']:
                        alerts.append({
                            'level': level,
                            'type': score_type,
                            'message': f"{model.name} ist {level}: {agg_score.average_score:.1f}",
                            'score': agg_score.average_score,
                            'threshold': threshold
                        })
                else:
                    # Normale Skala: Hohe Scores sind schlecht
                    if agg_score.average_score >= threshold and level in ['warning', 'critical']:
                        alerts.append({
                            'level': level,
                            'type': score_type,
                            'message': f"{model.name} ist {level}: {agg_score.average_score:.1f}",
                            'score': agg_score.average_score,
                            'threshold': threshold
                        })
        
        return sorted(alerts, key=lambda x: x['level'] == 'critical', reverse=True)
    
    def _create_summary(self, result: ScoringResult) -> Dict[str, Any]:
        """Erstellt Zusammenfassung der wichtigsten Erkenntnisse."""
        summary = {
            'total_chunks_analyzed': len(result.chunk_scores),
            'models_used': list(result.aggregated_scores.keys()),
            'critical_alerts': len([a for a in result.alerts if a['level'] == 'critical']),
            'warning_alerts': len([a for a in result.alerts if a['level'] == 'warning']),
            'speaker_count': len(result.speaker_scores),
            'key_insights': []
        }
        
        # Wichtigste Erkenntnisse
        for score_type, agg_score in result.aggregated_scores.items():
            insight = {
                'type': score_type,
                'score': agg_score.average_score,
                'trend': agg_score.trend,
                'interpretation': self._interpret_score(score_type, agg_score.average_score)
            }
            summary['key_insights'].append(insight)
        
        return summary
    
    def _interpret_score(self, score_type: str, score: float) -> str:
        """Interpretiert einen Score verbal."""
        model = next((m for m in self.models.values() if m.type.value == score_type), None)
        if not model:
            return "Unbekannt"
        
        if model.inverse_scale:
            if score >= 8:
                return "Ausgezeichnet"
            elif score >= 6:
                return "Gut"
            elif score >= 4:
                return "Durchschnittlich"
            elif score >= 2:
                return "Problematisch"
            else:
                return "Kritisch"
        else:
            if score <= 2:
                return "Sehr niedrig"
            elif score <= 4:
                return "Niedrig"
            elif score <= 6:
                return "Mittel"
            elif score <= 8:
                return "Hoch"
            else:
                return "Sehr hoch"
    
    def _get_active_models(self, model_ids: Optional[List[str]] = None) -> List[ScoringModel]:
        """Gibt aktive Scoring-Modelle zur√ºck."""
        if model_ids:
            return [self.models[mid] for mid in model_ids if mid in self.models and self.models[mid].active]
        else:
            return [m for m in self.models.values() if m.active]
    
    def _group_matches_by_chunk(self, matches: List[MarkerMatch]) -> Dict[str, List[MarkerMatch]]:
        """Gruppiert Matches nach Chunk-ID."""
        grouped = defaultdict(list)
        for match in matches:
            grouped[match.chunk_id].append(match)
        return grouped
    
    def add_custom_model(self, model: ScoringModel):
        """F√ºgt ein benutzerdefiniertes Scoring-Modell hinzu."""
        self.models[model.id] = model
        logger.info(f"Custom Scoring-Modell '{model.name}' hinzugef√ºgt")
    
    def get_model(self, model_id: str) -> Optional[ScoringModel]:
        """Gibt ein spezifisches Scoring-Modell zur√ºck."""
        return self.models.get(model_id)

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/scoring_engine.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/semantic_grabber_loader.py
# ====================================================================


import yaml
from typing import List, Dict

class SemanticGrabberLibrary:
    def __init__(self, yaml_path: str):
        with open(yaml_path, 'r', encoding='utf-8') as f:
            self.library = yaml.safe_load(f)

    def get_grabber_ids(self) -> List[str]:
        return list(self.library.keys())

    def get_patterns_for_id(self, grabber_id: str) -> List[str]:
        return self.library.get(grabber_id, {}).get('patterns', [])

    def get_description_for_id(self, grabber_id: str) -> str:
        return self.library.get(grabber_id, {}).get('beschreibung', '')

    def match_text(self, text: str, threshold: float = 0.6) -> List[str]:
        """ Dummy logic ‚Äì to be replaced with semantic similarity models """
        matches = []
        for grabber_id, data in self.library.items():
            for pattern in data.get('patterns', []):
                if pattern.lower() in text.lower():
                    matches.append(grabber_id)
                    break
        return matches


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/semantic_grabber_loader.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/text_chunker.py
# ====================================================================

"""Text Chunker f√ºr intelligente Text-Segmentierung."""

import re
import time
from datetime import datetime, timedelta
from typing import List, Optional, Tuple, Dict, Any
import logging
from uuid import uuid4

from .chunk_models import (
    TextChunk, ChunkType, Speaker, ChunkingConfig, ChunkingResult
)

logger = logging.getLogger(__name__)


class TextChunker:
    """Segmentiert Texte intelligent in analysierbare Chunks."""
    
    # Regex-Patterns f√ºr verschiedene Chat-Formate
    WHATSAPP_PATTERN = re.compile(
        r'(\d{1,2}[./]\d{1,2}[./]\d{2,4},?\s*\d{1,2}:\d{2}(?:\s*[AP]M)?)\s*-\s*([^:]+):\s*(.*)',
        re.MULTILINE
    )
    
    TELEGRAM_PATTERN = re.compile(
        r'\[(\d{1,2}\.\d{1,2}\.\d{2,4}\s+\d{1,2}:\d{2}(?::\d{2})?)\]\s*([^:]+):\s*(.*)',
        re.MULTILINE
    )
    
    GENERIC_PATTERN = re.compile(
        r'^([^:]+):\s*(.*)',
        re.MULTILINE
    )
    
    TIMESTAMP_PATTERNS = [
        # ISO format
        re.compile(r'\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}'),
        # Deutsch
        re.compile(r'\d{1,2}\.\d{1,2}\.\d{2,4}\s+\d{1,2}:\d{2}'),
        # US
        re.compile(r'\d{1,2}/\d{1,2}/\d{2,4}\s+\d{1,2}:\d{2}'),
    ]
    
    def __init__(self, config: Optional[ChunkingConfig] = None):
        self.config = config or ChunkingConfig()
        self._speaker_map: Dict[str, Speaker] = {}
        
    def chunk_text(
        self, 
        text: str, 
        format_hint: Optional[str] = None
    ) -> ChunkingResult:
        """Hauptmethode zum Chunking von Text.
        
        Args:
            text: Der zu segmentierende Text
            format_hint: Hinweis auf Format (whatsapp, telegram, etc.)
            
        Returns:
            ChunkingResult mit allen Chunks und Metadaten
        """
        start_time = time.time()
        result = ChunkingResult()
        
        try:
            # Format erkennen
            chat_format = format_hint or self._detect_format(text)
            logger.info(f"Erkanntes Format: {chat_format}")
            
            # Parse Messages
            messages = self._parse_messages(text, chat_format)
            
            if not messages:
                # Fallback: Als einzelnen Chunk behandeln
                chunk = self._create_chunk(
                    text=text,
                    chunk_type=ChunkType.PARAGRAPH,
                    start_pos=0,
                    end_pos=len(text)
                )
                result.chunks = [chunk]
            else:
                # Chunks aus Messages erstellen
                result.chunks = self._create_chunks_from_messages(messages)
            
            # Speakers sammeln
            result.speakers = list(self._speaker_map.values())
            
            # Statistiken
            result.statistics = self._calculate_statistics(result.chunks)
            
        except Exception as e:
            logger.error(f"Fehler beim Chunking: {e}")
            result.errors.append(str(e))
        
        result.processing_time = time.time() - start_time
        return result
    
    def _detect_format(self, text: str) -> str:
        """Erkennt das Chat-Format automatisch."""
        # Teste verschiedene Patterns
        if self.WHATSAPP_PATTERN.search(text):
            return "whatsapp"
        elif self.TELEGRAM_PATTERN.search(text):
            return "telegram"
        elif self.GENERIC_PATTERN.search(text):
            return "generic"
        else:
            return "plain"
    
    def _parse_messages(
        self, 
        text: str, 
        format_type: str
    ) -> List[Dict[str, Any]]:
        """Parst Messages aus dem Text basierend auf Format."""
        messages = []
        
        if format_type == "whatsapp":
            pattern = self.WHATSAPP_PATTERN
        elif format_type == "telegram":
            pattern = self.TELEGRAM_PATTERN
        elif format_type == "generic":
            pattern = self.GENERIC_PATTERN
        else:
            # Plain text - keine Messages
            return []
        
        last_end = 0
        
        for match in pattern.finditer(text):
            if format_type in ["whatsapp", "telegram"]:
                timestamp_str, speaker, message = match.groups()
                timestamp = self._parse_timestamp(timestamp_str)
            else:
                speaker, message = match.groups()
                timestamp = None
            
            # Multi-line Messages zusammenf√ºhren
            start = match.start()
            end = match.end()
            
            # Finde n√§chste Message oder Ende
            next_match = pattern.search(text, end)
            if next_match:
                message_end = next_match.start()
            else:
                message_end = len(text)
            
            # Erweitere Message bis zur n√§chsten
            full_message = text[match.end():message_end].strip()
            if full_message:
                message = message + "\n" + full_message
            
            messages.append({
                'speaker': speaker.strip(),
                'text': message.strip(),
                'timestamp': timestamp,
                'start_pos': start,
                'end_pos': message_end
            })
            
            last_end = message_end
        
        return messages
    
    def _parse_timestamp(self, timestamp_str: str) -> Optional[datetime]:
        """Versucht einen Zeitstempel zu parsen."""
        # Verschiedene Formate probieren
        formats = [
            "%d.%m.%Y %H:%M",
            "%d.%m.%Y %H:%M:%S",
            "%d/%m/%Y %H:%M",
            "%m/%d/%Y %H:%M",
            "%d.%m.%y, %H:%M",
            "%d/%m/%y, %H:%M",
            "%Y-%m-%d %H:%M:%S",
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(timestamp_str.strip(), fmt)
            except ValueError:
                continue
        
        logger.warning(f"Konnte Zeitstempel nicht parsen: {timestamp_str}")
        return None
    
    def _create_chunks_from_messages(
        self, 
        messages: List[Dict[str, Any]]
    ) -> List[TextChunk]:
        """Erstellt Chunks aus geparsten Messages."""
        chunks = []
        current_chunk_messages = []
        current_speaker = None
        last_timestamp = None
        
        for i, msg in enumerate(messages):
            speaker_name = msg['speaker']
            timestamp = msg['timestamp']
            
            # Entscheide ob neuer Chunk n√∂tig
            need_new_chunk = False
            
            # Bei Sprecherwechsel
            if self.config.chunk_by_speaker and speaker_name != current_speaker:
                need_new_chunk = True
            
            # Bei Zeitsprung
            if (self.config.chunk_by_time and 
                last_timestamp and timestamp and
                (timestamp - last_timestamp).total_seconds() > self.config.time_gap_minutes * 60):
                need_new_chunk = True
            
            # Bei Gr√∂√üenlimit
            current_size = sum(len(m['text']) for m in current_chunk_messages)
            if current_size + len(msg['text']) > self.config.max_chunk_size:
                need_new_chunk = True
            
            # Erstelle neuen Chunk wenn n√∂tig
            if need_new_chunk and current_chunk_messages:
                chunk = self._create_chunk_from_messages(current_chunk_messages)
                chunks.append(chunk)
                current_chunk_messages = []
            
            current_chunk_messages.append(msg)
            current_speaker = speaker_name
            last_timestamp = timestamp
        
        # Letzten Chunk erstellen
        if current_chunk_messages:
            chunk = self._create_chunk_from_messages(current_chunk_messages)
            chunks.append(chunk)
        
        # Chunk-IDs verlinken
        for i in range(len(chunks)):
            if i > 0:
                chunks[i].previous_chunk_id = chunks[i-1].id
            if i < len(chunks) - 1:
                chunks[i].next_chunk_id = chunks[i+1].id
        
        return chunks
    
    def _create_chunk_from_messages(
        self, 
        messages: List[Dict[str, Any]]
    ) -> TextChunk:
        """Erstellt einen Chunk aus einer Liste von Messages."""
        # Text zusammenf√ºhren
        texts = []
        for msg in messages:
            if msg['timestamp']:
                texts.append(f"[{msg['timestamp']}] {msg['speaker']}: {msg['text']}")
            else:
                texts.append(f"{msg['speaker']}: {msg['text']}")
        
        combined_text = "\n".join(texts)
        
        # Speaker bestimmen
        speakers = list(set(msg['speaker'] for msg in messages))
        if len(speakers) == 1:
            speaker = self._get_or_create_speaker(speakers[0])
        else:
            speaker = None  # Multiple speakers in chunk
        
        # Timestamps
        timestamps = [msg['timestamp'] for msg in messages if msg['timestamp']]
        timestamp = timestamps[0] if timestamps else None
        
        return self._create_chunk(
            text=combined_text,
            chunk_type=ChunkType.CONVERSATION if len(speakers) > 1 else ChunkType.MESSAGE,
            speaker=speaker,
            timestamp=timestamp,
            start_pos=messages[0]['start_pos'],
            end_pos=messages[-1]['end_pos'],
            metadata={
                'message_count': len(messages),
                'speakers': speakers
            }
        )
    
    def _create_chunk(
        self,
        text: str,
        chunk_type: ChunkType,
        start_pos: int,
        end_pos: int,
        speaker: Optional[Speaker] = None,
        timestamp: Optional[datetime] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> TextChunk:
        """Erstellt einen einzelnen Chunk."""
        chunk_id = f"chunk_{uuid4().hex[:8]}"
        
        # Text normalisieren wenn gew√ºnscht
        if self.config.normalize_whitespace:
            text = ' '.join(text.split())
        
        return TextChunk(
            id=chunk_id,
            type=chunk_type,
            text=text,
            speaker=speaker,
            timestamp=timestamp,
            start_pos=start_pos,
            end_pos=end_pos,
            metadata=metadata or {}
        )
    
    def _get_or_create_speaker(self, name: str) -> Speaker:
        """Holt oder erstellt einen Speaker."""
        if name not in self._speaker_map:
            speaker_id = f"speaker_{len(self._speaker_map) + 1}"
            self._speaker_map[name] = Speaker(
                id=speaker_id,
                name=name
            )
        return self._speaker_map[name]
    
    def _calculate_statistics(self, chunks: List[TextChunk]) -> Dict[str, Any]:
        """Berechnet Statistiken √ºber die Chunks."""
        if not chunks:
            return {}
        
        total_words = sum(c.word_count for c in chunks)
        total_chars = sum(c.char_count for c in chunks)
        
        # Zeitspanne
        timestamps = [c.timestamp for c in chunks if c.timestamp]
        if timestamps:
            time_span = max(timestamps) - min(timestamps)
            time_span_hours = time_span.total_seconds() / 3600
        else:
            time_span_hours = 0
        
        # Speaker-Statistiken
        speaker_stats = {}
        for chunk in chunks:
            if chunk.speaker:
                speaker_name = chunk.speaker.name
                if speaker_name not in speaker_stats:
                    speaker_stats[speaker_name] = {
                        'chunks': 0,
                        'words': 0,
                        'chars': 0
                    }
                speaker_stats[speaker_name]['chunks'] += 1
                speaker_stats[speaker_name]['words'] += chunk.word_count
                speaker_stats[speaker_name]['chars'] += chunk.char_count
        
        return {
            'total_chunks': len(chunks),
            'total_words': total_words,
            'total_chars': total_chars,
            'avg_chunk_size': total_chars / len(chunks) if chunks else 0,
            'time_span_hours': time_span_hours,
            'speaker_stats': speaker_stats,
            'chunk_types': {
                t.value: sum(1 for c in chunks if c.type == t)
                for t in ChunkType
            }
        }

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/text_chunker.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/time_series_aggregator.py
# ====================================================================

"""Time Series Aggregator f√ºr die Aggregation von Scores und Marker-Daten √ºber Zeit."""

import logging
import time
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime, timedelta
from collections import defaultdict
import pandas as pd
import numpy as np

from .aggregation_models import (
    AggregationConfig, AggregationPeriod, AggregationResult,
    TimeSeriesData, TimeSeriesPoint, HeatmapData, ComparisonData
)
from ..scoring.score_models import ChunkScore, ScoreType
from ..matcher.marker_models import MarkerMatch, MarkerCategory

logger = logging.getLogger(__name__)


class TimeSeriesAggregator:
    """Aggregiert Scores und Marker-Daten √ºber verschiedene Zeitfenster."""
    
    def __init__(self, config: Optional[AggregationConfig] = None):
        self.config = config or AggregationConfig()
    
    def aggregate_data(
        self,
        chunk_scores: List[ChunkScore],
        marker_matches: List[MarkerMatch],
        period: Optional[AggregationPeriod] = None
    ) -> AggregationResult:
        """Hauptmethode zur Aggregation von Daten.
        
        Args:
            chunk_scores: Liste von Chunk-Scores
            marker_matches: Liste von Marker-Matches
            period: Aggregationszeitraum (√ºberschreibt config)
            
        Returns:
            AggregationResult mit allen aggregierten Daten
        """
        start_time = time.time()
        result = AggregationResult()
        
        # Verwende spezifizierten oder konfigurierten Zeitraum
        agg_period = period or self.config.period
        
        try:
            # Aggregiere Scores
            score_series = self._aggregate_scores(chunk_scores, agg_period)
            result.time_series.update(score_series)
            
            # Aggregiere Marker-Counts
            marker_series = self._aggregate_markers(marker_matches, agg_period)
            result.time_series.update(marker_series)
            
            # Erstelle Heatmaps
            result.heatmaps = self._create_heatmaps(
                chunk_scores,
                marker_matches,
                agg_period
            )
            
            # Erstelle Vergleiche
            result.comparisons = self._create_comparisons(result.time_series)
            
            # Berechne Summary Statistics
            result.summary_statistics = self._calculate_summary_stats(result.time_series)
            
            # Erstelle Export DataFrame
            result.export_data = self._create_export_dataframe(result.time_series)
            
        except Exception as e:
            logger.error(f"Fehler bei Aggregation: {e}")
            raise
        
        result.processing_time = time.time() - start_time
        logger.info(f"Aggregation abgeschlossen in {result.processing_time:.2f}s")
        
        return result
    
    def _aggregate_scores(
        self,
        chunk_scores: List[ChunkScore],
        period: AggregationPeriod
    ) -> Dict[str, TimeSeriesData]:
        """Aggregiert Chunk-Scores √ºber Zeit."""
        series_dict = {}
        
        # Gruppiere Scores nach Typ
        scores_by_type = defaultdict(list)
        for score in chunk_scores:
            if score.timestamp:
                scores_by_type[score.score_type.value].append(score)
        
        # Erstelle Zeitreihe f√ºr jeden Score-Typ
        for score_type, scores in scores_by_type.items():
            if not scores:
                continue
            
            # Sortiere nach Zeit
            sorted_scores = sorted(scores, key=lambda s: s.timestamp)
            
            # Bestimme Zeitbereich
            start_time = sorted_scores[0].timestamp
            end_time = sorted_scores[-1].timestamp
            
            # Erstelle Zeitfenster
            time_windows = self._create_time_windows(start_time, end_time, period)
            
            # Aggregiere in Zeitfenster
            data_points = []
            for window_start, window_end in time_windows:
                window_scores = [
                    s for s in sorted_scores
                    if window_start <= s.timestamp < window_end
                ]
                
                if window_scores or self.config.include_zero_periods:
                    point = self._create_score_point(
                        window_scores,
                        window_start,
                        window_end,
                        score_type
                    )
                    data_points.append(point)
            
            # Gl√§tte Daten wenn gew√ºnscht
            if self.config.smooth_data and len(data_points) > self.config.smoothing_window:
                data_points = self._smooth_data_points(data_points, score_type)
            
            # Erstelle TimeSeriesData
            series = TimeSeriesData(
                series_id=f"scores_{score_type}",
                name=f"{score_type.replace('_', ' ').title()} Scores",
                metric_type="score",
                period=period,
                data_points=data_points,
                statistics=self._calculate_series_statistics(data_points, score_type)
            )
            
            series_dict[f"scores_{score_type}"] = series
        
        return series_dict
    
    def _aggregate_markers(
        self,
        marker_matches: List[MarkerMatch],
        period: AggregationPeriod
    ) -> Dict[str, TimeSeriesData]:
        """Aggregiert Marker-Matches √ºber Zeit."""
        series_dict = {}
        
        # Filtere Matches mit Timestamps
        timed_matches = [m for m in marker_matches if m.timestamp]
        if not timed_matches:
            return series_dict
        
        # Sortiere nach Zeit
        sorted_matches = sorted(timed_matches, key=lambda m: m.timestamp)
        
        # Zeitbereich
        start_time = sorted_matches[0].timestamp
        end_time = sorted_matches[-1].timestamp
        
        # Zeitfenster
        time_windows = self._create_time_windows(start_time, end_time, period)
        
        # Aggregiere Marker-Counts gesamt
        total_points = []
        category_series = defaultdict(list)
        
        for window_start, window_end in time_windows:
            window_matches = [
                m for m in sorted_matches
                if window_start <= m.timestamp < window_end
            ]
            
            # Gesamt-Counts
            total_point = TimeSeriesPoint(
                timestamp=window_start,
                period_start=window_start,
                period_end=window_end,
                values={"marker_count": len(window_matches)},
                counts={"total": len(window_matches)}
            )
            
            # Counts nach Kategorie
            category_counts = defaultdict(int)
            for match in window_matches:
                category_counts[match.category.value] += 1
            
            total_point.counts.update(category_counts)
            total_points.append(total_point)
            
            # Separate Serien pro Kategorie
            for category in MarkerCategory:
                cat_count = category_counts.get(category.value, 0)
                cat_point = TimeSeriesPoint(
                    timestamp=window_start,
                    period_start=window_start,
                    period_end=window_end,
                    values={"count": cat_count},
                    counts={category.value: cat_count}
                )
                category_series[category.value].append(cat_point)
        
        # Erstelle Gesamt-Serie
        series_dict["markers_total"] = TimeSeriesData(
            series_id="markers_total",
            name="Total Marker Count",
            metric_type="count",
            period=period,
            data_points=total_points,
            statistics=self._calculate_series_statistics(total_points, "marker_count")
        )
        
        # Erstelle Kategorie-Serien
        for category, points in category_series.items():
            series_dict[f"markers_{category}"] = TimeSeriesData(
                series_id=f"markers_{category}",
                name=f"{category.replace('_', ' ').title()} Markers",
                metric_type="count",
                period=period,
                data_points=points,
                statistics=self._calculate_series_statistics(points, "count")
            )
        
        return series_dict
    
    def _create_time_windows(
        self,
        start: datetime,
        end: datetime,
        period: AggregationPeriod
    ) -> List[Tuple[datetime, datetime]]:
        """Erstellt Zeitfenster f√ºr Aggregation."""
        windows = []
        current = start
        
        while current < end:
            if period == AggregationPeriod.HOURLY:
                window_end = current + timedelta(hours=1)
            elif period == AggregationPeriod.DAILY:
                window_end = current + timedelta(days=1)
            elif period == AggregationPeriod.WEEKLY:
                window_end = current + timedelta(weeks=1)
            elif period == AggregationPeriod.MONTHLY:
                # N√§chster Monat
                if current.month == 12:
                    window_end = current.replace(year=current.year + 1, month=1)
                else:
                    window_end = current.replace(month=current.month + 1)
            elif period == AggregationPeriod.QUARTERLY:
                # N√§chstes Quartal
                quarter = (current.month - 1) // 3
                if quarter == 3:
                    window_end = current.replace(year=current.year + 1, month=1)
                else:
                    window_end = current.replace(month=(quarter + 1) * 3 + 1)
            elif period == AggregationPeriod.YEARLY:
                window_end = current.replace(year=current.year + 1)
            elif period == AggregationPeriod.CUSTOM:
                hours = self.config.custom_period_hours or 24
                window_end = current + timedelta(hours=hours)
            else:
                window_end = current + timedelta(days=1)
            
            windows.append((current, window_end))
            current = window_end
        
        return windows
    
    def _create_score_point(
        self,
        scores: List[ChunkScore],
        start: datetime,
        end: datetime,
        score_type: str
    ) -> TimeSeriesPoint:
        """Erstellt einen aggregierten Score-Punkt."""
        point = TimeSeriesPoint(
            timestamp=start,
            period_start=start,
            period_end=end
        )
        
        if scores:
            values = [s.normalized_score for s in scores]
            point.values = {
                "mean": np.mean(values),
                "min": min(values),
                "max": max(values),
                "std": np.std(values) if len(values) > 1 else 0,
                "median": np.median(values)
            }
            point.counts = {
                "chunk_count": len(scores),
                "confidence_avg": np.mean([s.confidence for s in scores])
            }
        else:
            # Keine Daten f√ºr diesen Zeitraum
            point.values = {
                "mean": 0,
                "min": 0,
                "max": 0,
                "std": 0,
                "median": 0
            }
            point.counts = {"chunk_count": 0}
        
        return point
    
    def _smooth_data_points(
        self,
        points: List[TimeSeriesPoint],
        metric: str
    ) -> List[TimeSeriesPoint]:
        """Gl√§ttet Datenpunkte mit Moving Average."""
        if len(points) <= self.config.smoothing_window:
            return points
        
        # Extrahiere Werte
        values = [p.values.get("mean", 0) for p in points]
        
        # Berechne Moving Average
        smoothed = []
        window = self.config.smoothing_window
        
        for i in range(len(values)):
            start_idx = max(0, i - window // 2)
            end_idx = min(len(values), i + window // 2 + 1)
            window_values = values[start_idx:end_idx]
            smoothed.append(np.mean(window_values))
        
        # Update Points
        for i, point in enumerate(points):
            point.values["mean_smoothed"] = smoothed[i]
        
        return points
    
    def _create_heatmaps(
        self,
        chunk_scores: List[ChunkScore],
        marker_matches: List[MarkerMatch],
        period: AggregationPeriod
    ) -> Dict[str, HeatmapData]:
        """Erstellt Heatmap-Daten."""
        heatmaps = {}
        
        # Marker-Kategorie Heatmap √ºber Zeit
        category_heatmap = self._create_category_timeline_heatmap(
            marker_matches,
            period
        )
        if category_heatmap:
            heatmaps["marker_categories"] = category_heatmap
        
        # Score-Vergleich Heatmap
        score_heatmap = self._create_score_comparison_heatmap(
            chunk_scores,
            period
        )
        if score_heatmap:
            heatmaps["score_comparison"] = score_heatmap
        
        return heatmaps
    
    def _create_category_timeline_heatmap(
        self,
        matches: List[MarkerMatch],
        period: AggregationPeriod
    ) -> Optional[HeatmapData]:
        """Erstellt Heatmap f√ºr Marker-Kategorien √ºber Zeit."""
        timed_matches = [m for m in matches if m.timestamp]
        if not timed_matches:
            return None
        
        # Zeitfenster
        start = min(m.timestamp for m in timed_matches)
        end = max(m.timestamp for m in timed_matches)
        windows = self._create_time_windows(start, end, period)
        
        # Matrix aufbauen
        categories = list(MarkerCategory)
        matrix = []
        x_labels = []
        
        for window_start, window_end in windows:
            window_matches = [
                m for m in timed_matches
                if window_start <= m.timestamp < window_end
            ]
            
            # Z√§hle pro Kategorie
            row = []
            for category in categories:
                count = sum(1 for m in window_matches if m.category == category)
                row.append(count)
            
            matrix.append(row)
            x_labels.append(window_start.strftime("%Y-%m-%d %H:%M"))
        
        # Transponiere f√ºr bessere Darstellung
        matrix = list(map(list, zip(*matrix)))
        
        return HeatmapData(
            title="Marker Categories Over Time",
            x_labels=x_labels,
            y_labels=[cat.value for cat in categories],
            values=matrix,
            color_scale="YlOrRd"
        )
    
    def _create_score_comparison_heatmap(
        self,
        chunk_scores: List[ChunkScore],
        period: AggregationPeriod
    ) -> Optional[HeatmapData]:
        """Erstellt Heatmap f√ºr Score-Vergleiche."""
        if not chunk_scores:
            return None
        
        # Gruppiere nach Score-Typ und Speaker
        score_matrix = defaultdict(lambda: defaultdict(list))
        
        for score in chunk_scores:
            if score.timestamp:
                speaker = score.metadata.get("speaker", "Unknown")
                score_matrix[score.score_type.value][speaker].append(score.normalized_score)
        
        if not score_matrix:
            return None
        
        # Erstelle Matrix
        score_types = list(score_matrix.keys())
        speakers = list(set(speaker for scores in score_matrix.values() for speaker in scores))
        
        matrix = []
        for score_type in score_types:
            row = []
            for speaker in speakers:
                scores = score_matrix[score_type].get(speaker, [])
                avg_score = np.mean(scores) if scores else 0
                row.append(avg_score)
            matrix.append(row)
        
        return HeatmapData(
            title="Average Scores by Type and Speaker",
            x_labels=speakers,
            y_labels=score_types,
            values=matrix,
            color_scale="RdYlGn_r"  # Reversed: Red=bad, Green=good
        )
    
    def _create_comparisons(
        self,
        time_series: Dict[str, TimeSeriesData]
    ) -> List[ComparisonData]:
        """Erstellt Vergleiche zwischen Zeitreihen."""
        comparisons = []
        
        # Vergleiche Score-Typen
        score_series = [
            ts for name, ts in time_series.items()
            if name.startswith("scores_")
        ]
        
        if len(score_series) > 1:
            comparison = ComparisonData(
                comparison_type="score_types",
                series=score_series
            )
            
            # Berechne Korrelationen
            if len(score_series[0].data_points) > 3:
                correlation_matrix = self._calculate_correlations(score_series)
                comparison.correlation_matrix = correlation_matrix
            
            comparisons.append(comparison)
        
        return comparisons
    
    def _calculate_correlations(
        self,
        series_list: List[TimeSeriesData]
    ) -> List[List[float]]:
        """Berechnet Korrelationsmatrix zwischen Zeitreihen."""
        # Extrahiere Werte
        value_arrays = []
        for series in series_list:
            values = [p.values.get("mean", 0) for p in series.data_points]
            value_arrays.append(values)
        
        # Berechne Korrelationen
        n = len(value_arrays)
        corr_matrix = [[0.0] * n for _ in range(n)]
        
        for i in range(n):
            for j in range(n):
                if len(value_arrays[i]) == len(value_arrays[j]):
                    corr = np.corrcoef(value_arrays[i], value_arrays[j])[0, 1]
                    corr_matrix[i][j] = float(corr) if not np.isnan(corr) else 0.0
                else:
                    corr_matrix[i][j] = 0.0
        
        return corr_matrix
    
    def _calculate_series_statistics(
        self,
        points: List[TimeSeriesPoint],
        main_metric: str
    ) -> Dict[str, Any]:
        """Berechnet Statistiken f√ºr eine Zeitreihe."""
        if not points:
            return {}
        
        # Extrahiere Hauptmetrik
        if main_metric in ["marker_count", "count"]:
            values = [p.values.get(main_metric, p.values.get("count", 0)) for p in points]
        else:
            values = [p.values.get("mean", 0) for p in points]
        
        non_zero_values = [v for v in values if v > 0]
        
        stats = {
            "total_periods": len(points),
            "non_zero_periods": len(non_zero_values),
            "average": np.mean(values) if values else 0,
            "std_dev": np.std(values) if len(values) > 1 else 0,
            "min": min(values) if values else 0,
            "max": max(values) if values else 0,
            "sum": sum(values) if values else 0
        }
        
        # Trend
        if len(values) > 2:
            first_third = np.mean(values[:len(values)//3])
            last_third = np.mean(values[-len(values)//3:])
            if last_third > first_third * 1.1:
                stats["trend"] = "increasing"
            elif last_third < first_third * 0.9:
                stats["trend"] = "decreasing"
            else:
                stats["trend"] = "stable"
        
        return stats
    
    def _calculate_summary_stats(
        self,
        time_series: Dict[str, TimeSeriesData]
    ) -> Dict[str, Dict[str, float]]:
        """Berechnet zusammenfassende Statistiken."""
        summary = {}
        
        for name, series in time_series.items():
            if series.statistics:
                summary[name] = {
                    "average": series.statistics.get("average", 0),
                    "trend": series.statistics.get("trend", "unknown"),
                    "periods": series.statistics.get("total_periods", 0),
                    "active_periods": series.statistics.get("non_zero_periods", 0)
                }
        
        return summary
    
    def _create_export_dataframe(
        self,
        time_series: Dict[str, TimeSeriesData]
    ) -> pd.DataFrame:
        """Erstellt DataFrame f√ºr Export."""
        if not time_series:
            return pd.DataFrame()
        
        # Sammle alle Datenpunkte
        all_data = []
        
        for series_name, series in time_series.items():
            for point in series.data_points:
                row = {
                    'timestamp': point.timestamp,
                    'period_start': point.period_start,
                    'period_end': point.period_end,
                    'series': series_name,
                    'metric_type': series.metric_type
                }
                
                # F√ºge Werte hinzu
                for key, value in point.values.items():
                    row[f"{series_name}_{key}"] = value
                
                # F√ºge Counts hinzu
                for key, count in point.counts.items():
                    row[f"{series_name}_{key}_count"] = count
                
                all_data.append(row)
        
        df = pd.DataFrame(all_data)
        
        # Pivotiere f√ºr bessere Darstellung
        if not df.empty:
            # Gruppiere nach Timestamp
            pivot_data = {}
            for _, row in df.iterrows():
                ts = row['timestamp']
                if ts not in pivot_data:
                    pivot_data[ts] = {'timestamp': ts}
                
                # Kopiere relevante Werte
                for col in row.index:
                    if col not in ['timestamp', 'period_start', 'period_end', 'series', 'metric_type']:
                        pivot_data[ts][col] = row[col]
            
            df = pd.DataFrame(list(pivot_data.values()))
            df.set_index('timestamp', inplace=True)
            df.sort_index(inplace=True)
        
        return df

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/time_series_aggregator.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/yaml2json.py
# ====================================================================

#!/usr/bin/env python3
"""
yaml2json.py  ¬∑  WordThread Marker Engine Helper
------------------------------------------------
Wandelt alle .yaml|.yml-Dateien eines Verzeichnisses (rekursiv) oder eine
einzelne Datei in gleichnamige .json-Dateien um.

‚§∑   python yaml2json.py  markers/          # ganze Mappe
‚§∑   python yaml2json.py  markers/A_WZ.yaml # einzelne Datei

Optionen:
    --out DIR        Zielordner (default: <eingabepfad>/json_out)
    --schema FILE    JSON-Schema zur Validierung (optional)
    --indent N       Einr√ºckung im Output-JSON (default: 2)
"""

import argparse, json, sys
from pathlib import Path
from datetime import datetime

try:
    from ruamel.yaml import YAML
except ModuleNotFoundError:
    sys.exit("‚ùå  ruamel.yaml fehlt:  pip install ruamel.yaml")

try:
    import jsonschema
except ModuleNotFoundError:
    jsonschema = None  # Validierung nur, wenn verf√ºgbar

yaml = YAML(typ="safe")
yaml.preserve_quotes = True   # kein Verlust bei Strings

def load_yaml(path: Path):
    """Liest alle YAML-Dokumente (---) aus einer Datei."""
    with path.open("r", encoding="utf-8") as f:
        return list(yaml.load_all(f))

def validate(obj, schema):
    """Optional: JSON-Schema-Validierung."""
    if jsonschema is None:
        raise RuntimeError("jsonschema nicht installiert")
    jsonschema.validate(obj, schema)

def convert_file(path: Path, out_dir: Path, schema, indent: int):
    docs = load_yaml(path)
    # Datei kann mehrere YAML-Dokumente enthalten ‚Üí Liste serialisieren
    out_data = docs[0] if len(docs) == 1 else docs
    if schema:
        validate(out_data, schema)
    out_path = out_dir / (path.stem + ".json")
    out_path.write_text(
        json.dumps(out_data, ensure_ascii=False, indent=indent),
        encoding="utf-8"
    )
    print(f"‚úî  {path.name}  ‚Üí  {out_path.relative_to(out_dir.parent)}")

def walk_inputs(input_path: Path):
    return (
        [input_path] if input_path.is_file()
        else sorted(input_path.rglob("*.yml")) + sorted(input_path.rglob("*.yaml"))
    )

def main():
    ap = argparse.ArgumentParser(description="YAML ‚Üí JSON Converter")
    ap.add_argument("src", type=Path, help="Datei oder Verzeichnis mit YAML")
    ap.add_argument("--out", type=Path, help="Zielordner")
    ap.add_argument("--schema", type=Path, help="JSON-Schema (optional)")
    ap.add_argument("--indent", type=int, default=2)
    args = ap.parse_args()

    if not args.src.exists():
        sys.exit("‚ùå  Eingabepfad existiert nicht.")

    out_dir = args.out or args.src.parent / "json_out"
    out_dir.mkdir(parents=True, exist_ok=True)

    schema = None
    if args.schema:
        if jsonschema is None:
            sys.exit("‚ùå  jsonschema-Paket nicht installiert.")
        schema = json.loads(args.schema.read_text(encoding="utf-8"))

    files = walk_inputs(args.src)
    if not files:
        sys.exit("‚ö†Ô∏è  Keine YAML-Dateien gefunden.")

    print(f"üèÅ  Starte Konvertierung ({len(files)} Dateien)  ‚Äì  {datetime.now():%H:%M:%S}")
    for f in files:
        try:
            convert_file(f, out_dir, schema, args.indent)
        except Exception as e:
            print(f"‚ö†Ô∏è  Fehler in {f}: {e}")

    print(f"‚úÖ  Fertig. JSON-Dateien liegen in  {out_dir.resolve()}")

if __name__ == "__main__":
    main()


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/yaml2json.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/chunk_analysis/chunk_models.py
# ====================================================================

"""Datenmodelle f√ºr Text-Chunks."""

from typing import List, Optional, Dict, Any
from datetime import datetime
from pydantic import BaseModel, Field, validator
from enum import Enum


class ChunkType(str, Enum):
    """Typ eines Text-Chunks."""
    
    MESSAGE = "message"
    PARAGRAPH = "paragraph"
    CONVERSATION = "conversation"
    TIME_WINDOW = "time_window"
    SPEAKER_TURN = "speaker_turn"


class Speaker(BaseModel):
    """Information √ºber einen Sprecher/Autor."""
    
    id: str = Field(..., description="Eindeutige ID des Sprechers")
    name: str = Field(..., description="Name oder Alias")
    metadata: Dict[str, Any] = Field(default_factory=dict)


class TextChunk(BaseModel):
    """Ein segmentierter Text-Abschnitt zur Analyse."""
    
    id: str = Field(..., description="Eindeutige Chunk-ID")
    type: ChunkType = Field(..., description="Art des Chunks")
    
    text: str = Field(..., description="Der eigentliche Text-Inhalt")
    original_text: Optional[str] = Field(None, description="Original-Text vor Bereinigung")
    
    speaker: Optional[Speaker] = Field(None, description="Sprecher/Autor")
    timestamp: Optional[datetime] = Field(None, description="Zeitstempel")
    
    start_pos: int = Field(..., description="Startposition im Gesamttext")
    end_pos: int = Field(..., description="Endposition im Gesamttext")
    
    word_count: int = Field(0, description="Anzahl W√∂rter")
    char_count: int = Field(0, description="Anzahl Zeichen")
    
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="Zus√§tzliche Metadaten (z.B. Plattform, Thread-ID)"
    )
    
    previous_chunk_id: Optional[str] = Field(None, description="ID des vorherigen Chunks")
    next_chunk_id: Optional[str] = Field(None, description="ID des n√§chsten Chunks")
    
    @validator('word_count', always=True)
    def calculate_word_count(cls, v, values):
        """Berechnet Wortanzahl wenn nicht gesetzt."""
        if v == 0 and 'text' in values:
            return len(values['text'].split())
        return v
    
    @validator('char_count', always=True)
    def calculate_char_count(cls, v, values):
        """Berechnet Zeichenanzahl wenn nicht gesetzt."""
        if v == 0 and 'text' in values:
            return len(values['text'])
        return v
    
    def get_context(self, words_before: int = 5, words_after: int = 5) -> str:
        """Gibt Kontext um eine Position im Chunk zur√ºck."""
        words = self.text.split()
        return ' '.join(words[:words_before + words_after + 1])


class ChunkingConfig(BaseModel):
    """Konfiguration f√ºr das Text-Chunking."""
    
    # Gr√∂√üen-Limits
    max_chunk_size: int = Field(
        default=1000,
        description="Maximale Chunk-Gr√∂√üe in Zeichen"
    )
    min_chunk_size: int = Field(
        default=50,
        description="Minimale Chunk-Gr√∂√üe in Zeichen"
    )
    
    # Chunking-Strategien
    chunk_by_speaker: bool = Field(
        default=True,
        description="Bei Sprecherwechsel neuen Chunk beginnen"
    )
    chunk_by_time: bool = Field(
        default=True,
        description="Bei Zeitspr√ºngen neuen Chunk beginnen"
    )
    time_gap_minutes: int = Field(
        default=30,
        description="Zeitl√ºcke in Minuten f√ºr neuen Chunk"
    )
    
    # Text-Verarbeitung
    preserve_formatting: bool = Field(
        default=False,
        description="Original-Formatierung beibehalten"
    )
    normalize_whitespace: bool = Field(
        default=True,
        description="Mehrfache Leerzeichen normalisieren"
    )
    
    # Overlap f√ºr Kontext
    overlap_size: int = Field(
        default=50,
        description="√úberlappung zwischen Chunks in Zeichen"
    )
    
    # Metadaten
    extract_timestamps: bool = Field(
        default=True,
        description="Zeitstempel aus Text extrahieren"
    )
    extract_speakers: bool = Field(
        default=True,
        description="Sprecher aus Text extrahieren"
    )


class ChunkingResult(BaseModel):
    """Ergebnis des Chunking-Prozesses."""
    
    chunks: List[TextChunk] = Field(default_factory=list)
    total_chunks: int = Field(0)
    
    speakers: List[Speaker] = Field(
        default_factory=list,
        description="Alle identifizierten Sprecher"
    )
    
    statistics: Dict[str, Any] = Field(
        default_factory=dict,
        description="Statistiken √ºber das Chunking"
    )
    
    errors: List[str] = Field(
        default_factory=list,
        description="Aufgetretene Fehler/Warnungen"
    )
    
    processing_time: float = Field(
        0.0,
        description="Verarbeitungszeit in Sekunden"
    )
    
    @validator('total_chunks', always=True)
    def calculate_total(cls, v, values):
        """Berechnet Gesamtzahl wenn nicht gesetzt."""
        if v == 0 and 'chunks' in values:
            return len(values['chunks'])
        return v
    
    def get_chunks_by_speaker(self, speaker_id: str) -> List[TextChunk]:
        """Gibt alle Chunks eines Sprechers zur√ºck."""
        return [c for c in self.chunks if c.speaker and c.speaker.id == speaker_id]
    
    def get_chunks_in_timerange(
        self, 
        start: datetime, 
        end: datetime
    ) -> List[TextChunk]:
        """Gibt Chunks in einem Zeitbereich zur√ºck."""
        return [
            c for c in self.chunks 
            if c.timestamp and start <= c.timestamp <= end
        ]

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/chunk_analysis/chunk_models.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/ohne_json/PYTHON_MARKER.py
# ====================================================================

"""
PYTHON_MARKER - Semantic Marker
Sehr grobe Heuristik: je mehr Self-Disclosure-Marker, desto h√∂her.
"""

import re

class PYTHON_MARKER:
    """
    Sehr grobe Heuristik: je mehr Self-Disclosure-Marker, desto h√∂her.
    """
    
    examples = [
    ]
    
    patterns = [
        re.compile(r"(muster.*wird.*erg√§nzt)", re.IGNORECASE)
    ]
    
    semantic_grabber_id = "AUTO_GENERATED"
    
    def match(self, text):
        """Pr√ºft ob der Text zum Marker passt"""
        for pattern in self.patterns:
            if pattern.search(text):
                return True
        return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/ohne_json/PYTHON_MARKER.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/ohne_json/_python/PYTHON_MARKER.py
# ====================================================================

# ==============================  pipeline.py  ==============================
"""
Minimal-Pipeline zum Erkennen von Flirt-/Ambivalenz-Drift.
1.   Daten-Ingest  ‚Äì Chat-Records ‚óÅ JSON / DB / API
2.   Marker-Scoring ‚Äì Regex-/ML-Tagger ‚Üí marker_lookup
3.   Detector-Stage ‚Äì ruft unsere Detector-Funktionen
4.   Reporting      ‚Äì schreibt Flags in DB oder sendet Webhook
"""
from datetime import datetime, timedelta
from detector_utils import linear_slope
from detectors import (
    detect_sustained_contact,
    detect_secret_bonding,
    detect_adaptive_polarization,
    detect_flirt_dance_drift,
)

# --------------------------------------------------------------------------
# 1)  Simulierter Chat-Stream (chronologisch)
# --------------------------------------------------------------------------
CHAT_LOG = [
    {"id": "m1", "ts": datetime(2025, 7, 1, 9,  0), "text": "Du bist s√º√ü ;)",     },
    {"id": "m2", "ts": datetime(2025, 7, 1, 9,  1), "text": "Nicht verraten!",     },
    {"id": "m3", "ts": datetime(2025, 7, 2, 22, 0), "text": "Gute Nacht üåô",       },
    {"id": "m4", "ts": datetime(2025, 7, 3,  8, 0), "text": "Dir zum ersten Mal ‚Ä¶"},
    {"id": "m5", "ts": datetime(2025, 7, 3, 22, 0), "text": "L√∂sch den Chat üòä",   },
    {"id": "m6", "ts": datetime(2025, 7, 4, 20, 0), "text": "Du fehlst mir ‚Äì egal",},
    {"id": "m7", "ts": datetime(2025, 7, 5, 21, 0), "text": "VIP-Tattoo? üòè",      },
]

# --------------------------------------------------------------------------
# 2)  VERY simple Marker-Tagging (RegEx-Demo) -------------------------------
#     ‚Üí in der Praxis ersetzt du das durch dein echtes Marker-Engine-API.
# --------------------------------------------------------------------------
import re
PATTERNS = {
    "A_SOFT_FLIRT": re.compile(r"\b(s√º√ü|üòâ|üòä|üòè)\b", re.I),
    "S_MENTION_OF_SECRECY": re.compile(r"(nicht verraten|unter uns|l√∂sch)", re.I),
    "C_ADAPTIVE_POLARIZATION": re.compile(r"du fehlst mir.*egal", re.I),
    "C_RAPID_SELF_DISCLOSURE": re.compile(r"zum ersten mal|noch nie erz√§hlt", re.I),
}
marker_lookup = {}

for m in CHAT_LOG:
    found = [mid for mid, rx in PATTERNS.items() if rx.search(m["text"])]
    marker_lookup[m["id"]] = found

# --------------------------------------------------------------------------
# 3)  Detector-Stage --------------------------------------------------------
# --------------------------------------------------------------------------
detected = {}

ok, info = detect_sustained_contact(CHAT_LOG)
detected["C_SUSTAINED_CONTACT"] = ok

ok, info = detect_secret_bonding(CHAT_LOG, marker_lookup)
detected["C_SECRET_BONDING"] = ok

ok, info = detect_adaptive_polarization(CHAT_LOG, marker_lookup)
detected["C_ADAPTIVE_POLARIZATION"] = ok

ok, span = detect_flirt_dance_drift(CHAT_LOG, marker_lookup)
detected["MM_FLIRT_DANCE_DRIFT"] = ok

# --------------------------------------------------------------------------
# 4)  Report / Action -------------------------------------------------------
# --------------------------------------------------------------------------
if detected["MM_FLIRT_DANCE_DRIFT"]:
    print("‚ö†Ô∏è  FLIRT-DANCE-DRIFT erkannt!")
    for msg in span:
        print(f"  {msg['ts'].strftime('%Y-%m-%d %H:%M')} ‚Äì {msg['text']}")
else:
    print("Keine kritische Flirt-Drift erkannt.")

# ============================  Ende pipeline.py  ===========================

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/ohne_json/_python/PYTHON_MARKER.py
# ====================================================================



