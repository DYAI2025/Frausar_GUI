"""
Unified Python Detectors
========================
This file is auto-generated by the FRAUSAR Marker Assistant.
It combines all Python-based detectors found in the marker directories.

Generated at: 2025-07-21T12:45:18.408112
"""

# ====================================================================




# ====================================================================
# START OF FILE: ALL_NEWMARKER01/SemanticGrabberLibrary/SemanticGrabberLibrary_MARKER.py
# ====================================================================

"""
SemanticGrabberLibrary_MARKER - Semantic Marker
import yaml
from sentence_transformers import SentenceTransformer, util

class SemanticGrabberLibrary:
    def __init__(self, yaml_path: str, model_name: str = 'distiluse-base-multilingual-cased'):
        # Lade Grabber-Library aus YAML
        with open(yaml_path, 'r', encoding='utf-8') as f:
            self.library = yaml.safe_load(f)

        # Lade SentenceTransformer Modell
        self.model = SentenceTransformer(model_name)
        self._build_anchor_embeddings()

    def _build_anchor_embeddings(self):
        self.embeddings = {}
        for grabber_id, data in self.library.items():
            patterns = data.get('patterns', [])
            if patterns:
                self.embeddings[grabber_id] = self.model.encode(patterns, convert_to_tensor=True)

    def get_grabber_ids(self):
        return list(self.library.keys())

    def get_patterns_for_id(self, grabber_id):
        return self.library.get(grabber_id, {}).get('patterns', [])

    def get_description_for_id(self, grabber_id):
        return self.library.get(grabber_id, {}).get('beschreibung', '')

    def match_text(self, text, threshold=0.7):
        matches = []
        text_embedding = self.model.encode(text, convert_to_tensor=True)

        for grabber_id, anchor_embeds in self.embeddings.items():
            cos_sim = util.cos_sim(text_embedding, anchor_embeds)
            max_score = cos_sim.max().item()
            if max_score >= threshold:
                matches.append((grabber_id, round(max_score, 3)))

        # Sortiere nach Score absteigend
        matches.sort(key=lambda x: x[1], reverse=True)
        return matches
"""

import re

class SemanticGrabberLibrary_MARKER:
    """
    import yaml
from sentence_transformers import SentenceTransformer, util

class SemanticGrabberLibrary:
    def __init__(self, yaml_path: str, model_name: str = 'distiluse-base-multilingual-cased'):
        # Lade Grabber-Library aus YAML
        with open(yaml_path, 'r', encoding='utf-8') as f:
            self.library = yaml.safe_load(f)

        # Lade SentenceTransformer Modell
        self.model = SentenceTransformer(model_name)
        self._build_anchor_embeddings()

    def _build_anchor_embeddings(self):
        self.embeddings = {}
        for grabber_id, data in self.library.items():
            patterns = data.get('patterns', [])
            if patterns:
                self.embeddings[grabber_id] = self.model.encode(patterns, convert_to_tensor=True)

    def get_grabber_ids(self):
        return list(self.library.keys())

    def get_patterns_for_id(self, grabber_id):
        return self.library.get(grabber_id, {}).get('patterns', [])

    def get_description_for_id(self, grabber_id):
        return self.library.get(grabber_id, {}).get('beschreibung', '')

    def match_text(self, text, threshold=0.7):
        matches = []
        text_embedding = self.model.encode(text, convert_to_tensor=True)

        for grabber_id, anchor_embeds in self.embeddings.items():
            cos_sim = util.cos_sim(text_embedding, anchor_embeds)
            max_score = cos_sim.max().item()
            if max_score >= threshold:
                matches.append((grabber_id, round(max_score, 3)))

        # Sortiere nach Score absteigend
        matches.sort(key=lambda x: x[1], reverse=True)
        return matches
    """
    
    examples = [
    
        "- "Ich zweifle ständig an meinen Entscheidungen.",
        "- "Manchmal frage ich mich, ob ich gut genug bin.",
        "- "Ich weiß nicht, ob ich das überhaupt richtig mache.",
        "- "Vielleicht bin ich wirklich zu empfindlich.",
        "beschreibung: Erkennt Selbstzweifel, Unsicherheit und mangelndes Selbstvertrauen.",
        "SELF_DOUBT_SEM_g7h8:",
        "- "Immer drehst du alles so, dass ich mich schlecht fühle.",
        "- "Denk mal darüber nach, wie egoistisch du gerade bist.",
        "- "Du bist schuld, dass es mir so schlecht geht.",
        "- "Wenn du mich wirklich lieben würdest, würdest du das tun.",
        "beschreibung: Erkennt emotionale Manipulation und verdeckte Beeinflussung.",
        "EMOTIONAL_MANIPULATION_SEM_e5f6:",
        "- "Ich habe dir klar gesagt, dass ich das nicht will.",
        "- "So möchte ich nicht behandelt werden.",
        "- "Du hast meine Grenze überschritten.",
        "- "Das war jetzt wirklich zu viel für mich.",
        "beschreibung: Erkennt Grenzüberschreitungen oder das Ignorieren persönlicher Grenzen.",
        "BOUNDARY_VIOLATION_SEM_c3d4:",
        "- "Du hast mir schon so oft etwas versprochen und nicht gehalten.",
        "- "Ich spüre, dass mein Vertrauen in dich schwindet.",
        "- "Irgendetwas stimmt hier nicht mehr zwischen uns.",
        "- "Ich weiß nicht mehr, ob ich dir glauben kann.",
        "beschreibung: Erkennt Vertrauensverlust und Zweifel in der Beziehung.",
        "TRUST_EROSION_SEM_a1b2:",
        "- "Cross-System-Konvergenz nach Divergenz ist die Regel.",
        "- "Die Harmonisierung erfolgt meist prompt.",
        "- "Prompt-Fehler werden schnell aufgelöst.",
        "- "Modelle gleichen sich nach kurzer Zeit an.",
        "- "Nach Konflikt stimmen GPT und Gemini wieder überein.",
        "Wieder-Angleichung, Konvergenz oder Harmonisierung der Modelle nach einer Divergenzphase.",
        "beschreibung: >",
        "CROSS_REBOUND_SEM:",
        "- "Eindeutige Diskrepanzen bei gleicher Frage.",
        "- "Die Systeme interpretieren den Chat unterschiedlich.",
        "- "Marker-Zuordnung differiert deutlich.",
        "- "Antworten klaffen weit auseinander.",
        "- "GPT und Gemini liefern komplett unterschiedliche Resultate.",
        "Auffällige Abweichungen, Widersprüche oder divergierende Klassifikationen bei mehreren KI-Systemen auf denselben Input.",
        "beschreibung: >",
        "MODEL_DIVERGENCE_SEM:",
        "- "Die Bewertungen sind über Systemgrenzen konsistent.",
        "- "Keine Unterschiede in den Antworten erkennbar.",
        "- "Beide Systeme wählen denselben Marker.",
        "- "Modelle treffen die gleiche Einschätzung.",
        "- "GPT und Gemini liefern dieselbe Antwort.",
        "Übereinstimmung zwischen den Ausgaben, Einschätzungen oder Bewertungen verschiedener KI-Modelle auf identischem Input.",
        "beschreibung: >",
        "MODEL_CONVERGENCE_SEM:",
        "- "Jeder Abend endet mit unserem Spruch.",
        "- "Abschiede sind bei uns immer besonders.",
        "- "Unsere Insider-Sprüche versteht kein anderer.",
        "- "Unser Kosenamen-Ritual bleibt immer gleich.",
        "- "Immer dieses xx am Ende von uns beiden.",
        "Wiederkehrende, ritualisierte Nähe-Gesten und Insider im Chat; private Codes, Grußformeln, feste Abschiede.",
        "beschreibung: >",
        "RITUAL_PROXIMITY_SEM:",
        "- "Wir machen sofort einen Neustart nach Konflikt.",
        "- "Unsere Versöhnungen laufen fast automatisch.",
        "- "Nach Missverständnissen finden wir wieder zusammen.",
        "- "Gut, dass wir uns immer schnell wieder vertragen.",
        "- "Sorry für vorhin, das war nicht okay.",
        "Wiederherstellung nach Streit, bewusste Reparatur, schnelle Versöhnung oder Rückkehr zur Harmonie nach Störung.",
        "beschreibung: >",
        "REBOUND_SEM:",
        "- "Unsere Gespräche sind jetzt viel tiefer.",
        "- "Wir reflektieren unser Kommunikationsverhalten.",
        "- "Ich finde, wir sind zusammen gewachsen.",
        "- "Wir reden irgendwie anders als früher.",
        "- "Fällt dir auf, wie sich unser Ton verändert hat?",
        "Selbstbezug und Reflexion über den Chat, bewusste Kommentare zu Dynamik, Stil oder Entwicklung des Gesprächs.",
        "beschreibung: >",
        "META_REFLEX_SEM:",
        "- "Du bringst ganz neue Impulse rein.",
        "- "Unser Chat ist heute besonders kreativ.",
        "- "Heute überrascht du mich mit neuen Themen.",
        "- "Plötzlich ganz neue Gedanken von dir.",
        "- "Das hast du so noch nie gesagt!",
        "Unerwartete, kreative oder neuartige Beiträge im Chat; ungewöhnliche Formulierungen, neue Ideen, Innovationssprünge.",
        "beschreibung: >",
        "NOVELTY_BURST_SEM:",
        "- "Wir verlieren regelmäßig den roten Faden.",
        "- "Das Thema wechselt ohne Vorwarnung.",
        "- "Unsere Gespräche wechseln ständig das Thema.",
        "- "Du springst heute oft von einem Thema zum anderen.",
        "- "Jetzt sind wir schon wieder bei einem neuen Thema.",
        "Plötzlicher Themenwechsel, überraschende Richtungsänderung, bewusste oder intuitive Themensprünge im Dialog.",
        "beschreibung: >",
        "TOPIC_SHIFT_SEM:",
        "- "Wir warten oft, bis beide bereit sind zu antworten.",
        "- "Wenn du langsamer bist, werde ich auch langsamer.",
        "- "Oft tippen wir parallel.",
        "- "Wir reagieren gleichzeitig.",
        "- "Unsere Antworten kommen im selben Takt.",
        "Adaptierte oder bewusst gleichzeitige Antwortzeiten, Rhythmus-Anpassung, auffallende Parallelität in der Chat-Interaktion.",
        "beschreibung: >",
        "RESPONSE_TIMING_SEM:",
        "- "Wir experimentieren gemeinsam mit neuen Emojis.",
        "- "Unsere Stimmung zeigt sich in gleichen Symbolen.",
        "- "Beide benutzen ❤️ und 😁 zur selben Zeit.",
        "- "Unsere Emoji-Reihenfolge ist auffällig synchron.",
        "- "Wir schicken uns die gleichen Emojis.",
        "Spiegelung und Angleichung im Emoji-Verhalten; identische oder aufeinander folgende Emoji-Nutzung als Zeichen emotionaler Verbundenheit.",
        "beschreibung: >",
        "EMOJI_ALIGNMENT_SEM:",
        "- "Selbst unsere Emojis sind identisch platziert.",
        "- "Wir nutzen jetzt dieselben Abkürzungen.",
        "- "Sogar die Satzlänge passt sich an.",
        "- "Unsere Sätze spiegeln sich gegenseitig.",
        "- "Ich erkenne kaum noch, ob du oder ich das geschrieben habe.",
        "- "Unsere Wortwahl wird immer ähnlicher.",
        "- "Wir schreiben mittlerweile fast gleich.",
        "Synchroner oder adaptierter Schreibstil im Dialog; auffallend ähnliche Wortwahl, gleiche Satzstruktur, Wiederholung typischer Formulierungen.",
        "beschreibung: >",
        "STYLE_SYNC_SEM:",
        "output_path.name",
        "yaml.dump(data, f, allow_unicode=True)",
        "with open(output_path, "w", encoding="utf-8") as f:",
        "output_path = Path("/mnt/data/semantic_grabber_library_FINAL_FULL.yaml")",
        "# Neue Datei speichern",
        "data.append(guardian_marker)",
        "# Marker hinzufügen",
        "- Pattern Interrupt (Ericksonian Technique): disrupting habitual negative loops to enable change",
        "- Psychological Safety (Amy Edmondson): protecting openness and inclusion\n",
        "- Process Facilitation: guiding a group through healthy communication patterns\n",
        "- Meta-Position (Virginia Satir): observing the system instead of reacting from within it\n",
        "It draws on concepts like:\n",
        "This marker reflects advanced meta-communication skills, system awareness, and the ability to interrupt group dysfunctions.",
        "psychological_context": (",
        "rule_5: Detect playful or visual interruptions (e.g. 🛡️, ⚖️, 'Foulspiel!') as system awareness signals.",
        "rule_4: Identify principle or value references during team conflict or pressure.",",
        "rule_3: Recognize reflective process-oriented language (e.g. 'Sind wir noch im Lösungsmodus?').",",
        "rule_2: Find interruption cues followed by systemic reference (e.g. 'Stopp... unser Prinzip...')",",
        "rule_1: Detect meta-phrases like 'Ich nehme hier mal die Guardian-Rolle ein', 'Kurze Meta-Frage', 'Ich möchte auf die Metaebene wechseln'.",",
        "patterns": [",
        "⚖️",
        "Zeit für den Schiedsrichter. pfeift Foulspiel!",",
        "Sind wir noch im Lösungsmodus oder im Rechtfertigungsmodus?",",
        "Unser gemeinsames Betriebssystem hat gerade einen Bug.",",
        "Als Scrum Master muss ich hier einschreiten: ...",",
        "Wir vermeiden gerade ein wichtiges Thema.",",
        "Wir machen Witze auf Kosten von jemand anderem.",",
        "Wir haben Toms Idee dreimal besprochen, aber Miri und Jan ignoriert.",",
        "Ich glaube, wir ziehen uns gerade gegenseitig runter.",",
        "Können wir bitte aufhören, über Lisa zu reden, wenn sie nicht da ist?",",
        "Der Ton wird gerade schärfer. Lass uns aufpassen. ❤️",",
        "Mir ist aufgefallen, dass wir Entscheidungen oft ohne Emotionen treffen.",",
        "Ich glaube, wir brauchen eine Regel für solche Gespräche.",",
        "Ich möchte sicherstellen, dass wir beide gleich viel Redezeit haben.",",
        "Fällt dir auf, dass wir beide gerade versuchen, den anderen zu 'gewinnen'?",",
        "Ich möchte kurz auf die Metaebene wechseln: Geht es wirklich um Geld oder um Angst?",",
        "Ich glaube, wir texten gerade aneinander vorbei.",",
        "Wir treffen eine Entscheidung für die Kinder über deren Köpfe hinweg.",",
        "Ich merke gerade, ich rede nur noch über meine Probleme. Das ist unfair.",",
        "Warte mal kurz. Stopp. Ich glaube, wir spielen gerade unser altes Spiel.",",
        "Prozess-Check?",",
        "Ich mache mir Sorgen, dass wir mit dem Verhalten psychologische Sicherheit untergraben.",",
        "Wir haben noch 10 Minuten und sind erst bei Punkt 2. Wie wollen wir damit umgehen?",",
        "Check-in: Lasst uns Entscheidungen wieder transparent dokumentieren.",",
        "Ich habe den Eindruck, wir ignorieren gerade das Design-Team.",",
        "Ich nehme hier mal die Guardian-Rolle ein: ...",",
        "Moment mal. Bevor wir weitermachen, möchte ich sicherstellen, dass wir hier gerade nicht in ein Groupthink-Muster verfallen.",",
        "Ich spreche mal als Hüter unseres Prozesses: ...",",
        "Kurze Meta-Frage: Entscheiden wir das hier gerade wirklich im Chat?",",
        "Stopp, ich glaube, wir verlieren gerade den Fokus.",",
        "examples": [",
        "pattern-interrupt",
        "principle-reminder",",
        "system-check",",
        "meta-position",",
        "guardian-role",",
        "semantic_tags": [",
        "semantic_grabber_id": "LIFE_ROLE_GUARDIAN_SEM",",
        "category": "Meta Communication",",
        "),",
        "This includes interrupting dysfunctional loops, referencing shared values or norms, and raising awareness about group dynamics.",
        "to protect the integrity, fairness, or psychological safety of a team process, decision, or communication pattern.",
        "Detects statements in which a speaker steps out of content-level discussion and takes a meta-position",
        "description": (",
        "name": "System Guardian",",
        "id": "M2002",",
        "guardian_marker = {",
        "# Neuer Marker "LIFE_ROLE_GUARDIAN_SEM",
        "data = yaml.safe_load(f)",
        "with open(input_path, "r", encoding="utf-8") as f:",
        "input_path = Path("/mnt/data/semantic_grabber_library_FINAL.yaml")",
        "# Datei erneut laden",
        "from pathlib import Path",]
    
    patterns = [
        re.compile(r"(muster.*wird.*ergänzt)", re.IGNORECASE)
    ]
    
    semantic_grabber_id = "AUTO_GENERATED"
    
    def match(self, text):
        """Prüft ob der Text zum Marker passt"""
        for pattern in self.patterns:
            if pattern.search(text):
                return True
        return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/SemanticGrabberLibrary/SemanticGrabberLibrary_MARKER.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/AI_BOTS_SEMANTIC_DETECT.py
# ====================================================================

marker: AI_BOTS_SEMANTIC_DETECT.py_MARKER
beschreibung: >
  beschreibung: >
  # -*- coding: utf-8 -*-

# #############################################################################
#  SEMANTIC GRAB DEFINITIONEN FÜR KOMMUNIKATIONSMARKER
#
#  Zweck:
#  Diese Datei enthält eine zentrale Sammlung von "Semantic Grab"-Mustern
#  für verschiedene psychologische Kommunikationsmarker. Jedes Muster ist
#  darauf ausgelegt, von einem KI-System (z.B. via Regex oder semantischer
#  Suche) verwendet zu werden, um komplexe Dynamiken in Texten zu erkennen.
#
#  Struktur:
#  Ein zentrales Dictionary `SEMANTIC_GRAB_PATTERNS`, dessen Schlüssel die
#  ID des Markers ist. Der Wert ist jeweils ein weiteres Dictionary mit:
#    - 'description': Eine Beschreibung dessen, was die KI suchen soll.
#    - 'patterns': Eine Liste von Regeln, Mustern und Beispielen.
# #############################################################################
beispiele:
beispiele:
  - "- "SEMANTIC_GRAB_PATTERNS = {""
  - "- ""self_sabotage_loop": {""
  - "- ""description": "Erkennt eine Kombination aus negativen Selbstbewertungen, der Ablehnung von Chancen, dem Zurückweisen von Hilfe und dem Fokus auf potenzielles Scheitern. Eine KI sollte hier nicht nur Keywords, sondern die semantische Ähnlichkeit zu diesen Mustern bewerten.",""
  - "- ""patterns": [""
  - "- "{""
  - "- ""rule": "PREEMPTIVE_SURRENDER",""
  - "- ""pattern": r"(aber ich glaube|ich habe Angst|ich sollte lieber) \+ (ich schaffe das nicht|sagen wir es lieber ab|bin nicht gut genug)",""
  - "- ""example": "Das Date wäre toll, aber ich glaube, ich bin nicht interessant genug und sage lieber ab."""
  - "- "},""
  - "- "{""
  - "- ""rule": "FOCUS_ON_FAILURE_POTENTIAL",""
  - "- ""pattern": r"(bevor ich alles kaputt mache|damit ich niemanden enttäusche|ich würde es nur ruinieren) \+ (lasse ich es lieber gleich|mache ich es erst gar nicht)",""
  - "- ""example": "Bevor ich dich nur enttäusche, fangen wir lieber gar nichts Ernstes an."""
  - "- "},""
  - "- "{""
  - "- ""rule": "DEVALUING_OPPORTUNITIES",""
  - "- ""pattern": r"(Die Beförderung|Das Kompliment|Die Chance) \+ (wäre eh zu viel|meinte er nicht so|ist nichts für mich)",""
  - "- ""example": "Das Kompliment meines Chefs habe ich ignoriert. Er meinte das sicher nicht so."""
  - "- "},""
  - "- "{""
  - "- ""rule": "WORTHINESS_NEGATION",""
  - "- ""pattern": r"(Du verdienst jemanden, der besser ist|Ich bin es nicht wert|Ich bin zu kompliziert für dich)",""
  - "- ""example": "Ich bin es nicht wert, dass man sich so um mich bemüht."""
  - "- "},""
  - "- "{""
  - "- ""rule": "PASSIVE_SABOTAGE_RATIONALIZATION",""
  - "- ""pattern": r"(habe die Frist verpasst|nicht geschafft anzurufen|vergessen zu antworten) \+ (war wohl nicht sein soll|war eh besser so)",""
  - "- ""example": "Ich habe die Bewerbungsfrist verpasst. War wohl einfach nicht sein soll."""
  - "- "}""
  - "- "]""
  - "- "},""
  - "- ""guilt_tripping": {""
  - "- ""description": "Erkennt Aussagen, die eine Diskrepanz zwischen dem Gesagten und dem Gemeinten aufweisen und oft das Leiden der sprechenden Person oder die undankbare Tat der anderen Person betonen.",""
  - "- ""patterns": [""
  - "- "{""
  - "- ""rule": "SACRIFICE_REFERENCE",""
  - "- ""pattern": r"(Nach allem, was ich getan habe|Ich habe auf so vieles verzichtet|Ich opfere mich auf) \+ (und das ist der Dank|dann würdest du|kannst du nicht mal)",""
  - "- ""example": "Nach allem, was ich für dich getan habe, kannst du mir nicht mal das gönnen?"""
  - "- "},""
  - "- "{""
  - "- ""rule": "PASSIVE_AGGRESSIVE_PERMISSION",""
  - "- ""pattern": r"(Geh ruhig|Mach nur|Feier du) \+ (ich sitze dann allein|ich mach das schon|ist ja nicht so schlimm)",""
  - "- ""example": "Geh ruhig zu deinen Freunden, ich mache hier schon allein weiter."""
  - "- "},""
  - "- "{""
  - "- ""rule": "CONDITIONAL_LOVE_TEST",""
  - "- ""pattern": r"(Wenn du mich wirklich lieben würdest|Jemand, der liebt) \+ (würdest du das tun|würdest du nicht)",""
  - "- ""example": "Jemand, der mich liebt, würde das für mich tun."""
  - "- "},""
  - "- "{""
  - "- ""rule": "DISAPPOINTMENT_DECLARATION",""
  - "- ""pattern": r"(Ich bin so enttäuscht von dir|Ich hätte nie gedacht, dass du so bist|Deine Worte verletzen mich)",""
  - "- ""example": "Ich bin einfach nur enttäuscht. Ich hätte nie gedacht, dass du so egoistisch sein kannst."""
  - "- "}""
  - "- "]""
  - "- "},""
  - "- ""escalation": {""
  - "- ""description": "Erkennt eine hohe Dichte an Wörtern, die starke Emotionen (Wut, Zorn) und körperliche Stressreaktionen (Herzrasen, Zittern, keine Luft) beschreiben, oft in Kombination mit einem Abbruch des Gesprächs.",""
  - "- ""patterns": [""
  - "- "{""
  - "- ""rule": "PHYSIOLOGICAL_STRESS_INDICATOR",""
  - "- ""pattern": r"Mein (Herz rast|Puls hämmert)|(ich zittere|kriege keine Luft|mein Blut kocht)",""
  - "- ""example": "Mein Herz hämmert, ich zittere am ganzen Körper."""
  - "- "},""
  - "- "{""
  - "- ""rule": "PEAK_EMOTION_DECLARATION",""
  - "- ""pattern": r"(ich raste aus|ich explodiere|ich platze gleich|ich koche vor Zorn|ich kann nicht mehr)",""
  - "- ""example": "Ich platze gleich vor Wut!"""
  - "- "},""
  - "- "{""
  - "- ""rule": "ACCUSATION_TRIGGER",""
  - "- ""pattern": r"(weil du nicht zuhörst|du ignorierst mich|du verarschst mich|du hast null Respekt)",""
  - "- ""example": "Du hörst mir einfach nie zu!"""
  - "- "},""
  - "- "{""
  - "- ""rule": "COMBINED_ESCALATION_TERMINATION",""
  - "- ""pattern": r"(raste aus|reicht's jetzt|Schnauze voll) \+ (und ciao|ich bin raus|ich geh offline)",""
  - "- ""example": "Jetzt reicht's, ich bin hier raus."""
  - "- "}""
  - "- "]""
  - "- "},""
  - "- ""projective_identification": {""
  - "- ""description": "Erkennt stabile, negative 'Du-bist-so'-Zuschreibungen, die dem Gegenüber eine Eigenschaft als Wesensmerkmal unterstellen. Die KI muss den Kontext analysieren: Findet die Zuschreibung in einem Konflikt statt? Reagiert das Gegenüber, indem es die zugeschriebene Eigenschaft bestätigt?",""
  - "- ""patterns": [""
  - "- "{""
  - "- ""rule": "NEGATIVE_TRAIT_ASSIGNMENT",""
  - "- ""pattern": r"(Du bist so|Du bist einfach|Deine) \+ (unzuverlässig|unordentlich|eifersüchtig|überfordert|ungeduldig|kontrollsüchtig)",""
  - "- ""example": "Du bist einfach so ein Kontrollfreak."""
  - "- "},""
  - "- "{""
  - "- ""rule": "INCOMPETENCE_ACCUSATION",""
  - "- ""pattern": r"(Kannst du gar nichts richtig machen|Das hast du ja wieder toll hinbekommen|Du scheinst damit überfordert zu sein)",""
  - "- ""example": "Das hast du ja wieder toll hinbekommen."""
  - "- "},""
  - "- "{""
  - "- ""rule": "EMOTIONAL_STATE_PROJECTION",""
  - "- ""pattern": r"(Warum musst du immer so ausrasten|Du bist zu sensibel|Du übertreibst total)",""
  - "- ""example": "Warum musst du wegen jeder Kleinigkeit so ausrasten?"""
  - "- "},""
  - "- "{""
  - "- ""rule": "BEHAVIOR_LABELING",""
  - "- ""pattern": r"(Du klammerst|Du machst immer Probleme|Du lässt mich immer allein)",""
  - "- ""example": "Du klammerst so sehr, das macht mich fertig."""
  - "- "}""
  - "- "]""
  - "- "},""
  - "- ""ai_bot_scam": {""
  - "- ""description": "Erkennt die Kollision von semantischen Feldern: Hochemotionale/romantische Begriffe treten in unnatürlicher Nähe zu formaler, strukturierter oder analytischer Sprache auf.",""
  - "- ""patterns": [""
  - "- "{""
  - "- ""rule": "FORMAL_EMOTION_CLASH",""
  - "- ""pattern": r"(Liebe|Herz|Seele|Zuneigung|Schicksal) \+ (deshalb|folglich|zusätzlich|es ist zu beachten|des Weiteren)",""
  - "- ""example": "Ich spüre eine tiefe Verbindung zu deiner Seele. Folglich möchte ich den Kontakt intensivieren."""
  - "- "},""
  - "- "{""
  - "- ""rule": "OVERLY_STRUCTURED_COMPLIMENT",""
  - "- ""pattern": r"(Erstens|Zweitens|Abschließend) \+ (dein Lächeln|deine Augen|unsere Verbindung)",""
  - "- ""example": "Erstens, deine Augen sind faszinierend. Zweitens, dein Humor ist bezaubernd."""
  - "- "},""
  - "- "{""
  - "- ""rule": "GENERIC_ATTRIBUTE_PRAISE",""
  - "- ""pattern": r"(positive Energie|bemerkenswerte Ausstrahlung|tiefe Seele|faszinierendes Profil)",""
  - "- ""example": "Du strahlst eine unglaublich positive Energie aus."""
  - "- "},""
  - "- "{""
  - "- ""rule": "NON_IDIOMATIC_PHRASING",""
  - "- ""pattern": r"(offerieren|explorieren|signifikant|adäquat) \+ (Zuneigung|Liebe|Gespräch)",""
  - "- ""example": "Ich möchte unsere Konversation gerne weiter explorieren."""
  - "- "}""
  - "- "]""
  - "- "}""
  - "- "}""
  - "- "# Beispielhafter Aufruf, um die Daten zu verwenden und zu überprüfen:""
  - "- "if __name__ == "__main__":""
  - "- "print("="*50)""
  - "- "print("SEMANTISCHE MUSTER FÜR DIE KI-ANALYSE GELADEN")""
  - "- "print("="*50)""
  - "- "# Überprüfen, wie viele Marker-Definitionen geladen wurden""
  - "- "print(f"\nInsgesamt {len(SEMANTIC_GRAB_PATTERNS)} Marker-Definitionen gefunden.\n")""
  - "- "# Ein Beispiel im Detail ausgeben""
  - "- "marker_id_to_show = "self_sabotage_loop"""
  - "- "if marker_id_to_show in SEMANTIC_GRAB_PATTERNS:""
  - "- "print(f"--- Details für Marker: '{marker_id_to_show}' ---")""
  - "- "marker_data = SEMANTIC_GRAB_PATTERNS[marker_id_to_show]""
  - "- "print(f"Beschreibung: {marker_data['description']}\n")""
  - "- "print("Muster:")""
  - "- "for pattern_rule in marker_data['patterns']:""
  - "- "print(f"  - Regel: {pattern_rule['rule']}")""
  - "- "print(f"    Beispiel: {pattern_rule['example']}")""
  - "- "print(f"    Regex-Muster (abstrakt): {pattern_rule['pattern']}\n")""
  - "- "else:""
  - "- "print(f"Marker mit der ID '{marker_id_to_show}' nicht gefunden.")""
  - "semantic_grab:"
  - "description: "Erkennt Muster für ai_bot_semantic_detect.py""
  - "patterns:"
  - "- rule: "AUTO_PATTERN""
  - "pattern: r"(muster.*wird.*ergänzt)""
  - "tags: [neu_erstellt, needs_review]"

semantic_grab:
  description: "Erkennt Muster für ai_bots_semantic_detect.py"
  patterns:
    - rule: "AUTO_PATTERN"
      pattern: r"(muster.*wird.*ergänzt)"
tags: [neu_erstellt, needs_review]


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/AI_BOTS_SEMANTIC_DETECT.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/AMBIVALENCE_KNOT.py
# ====================================================================

import re

AMBIVALENCE_KNOT_PATTERNS = [
    r"(ich will .* aber|unbedingt .* aber|hilf mir .* aber lass|komm her .* aber lass mich|ich brauche dich .* aber|bleib .* aber|geh .* aber|ich liebe dich .* aber|ich hasse es .* aber|ich freue mich .* aber)",
    r"(kann nicht mit dir .* kann nicht ohne dich|will ehrlich sein .* kann es aber nicht|möchte dich sehen .* habe angst davor|nähe .* aber brauche abstand|will teilen .* aber nicht alles|ich bin froh .* aber es wäre besser, wenn du gehst|ich habe die entscheidung getroffen zu gehen .* aber kann die tür nicht zumachen)",
    r"(bitte komm .* aber erwarte nicht|melde dich .* aber nicht jetzt|ich ersticke .* aber du sollst nicht gehen|ich vertraue dir .* aber misstraue trotzdem)"
]

def detect_ambivalence_knot(text):
    for pattern in AMBIVALENCE_KNOT_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE | re.DOTALL):
            return True
    return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/AMBIVALENCE_KNOT.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/COMPLEX_MARKER_PATTERNS.py
# ====================================================================

import re
from collections import defaultdict

# ==============================================================================
# DEFINITION DER MUSTER FÜR KOMPLEXE METAMARKER
# Diese Muster basieren auf den zuvor generierten 20 Beispielen für jeden Marker.
# ==============================================================================

COMPLEX_MARKER_PATTERNS = {
    "REACTIVE_CONTROL_SPIRAL": [
        r"\b(ich bin (im moment|einfach) zu .* für dich)\b",
        r"\b(vielleicht sollte ich mich .* zurückziehen)\b",
        r"\b(mach dir keine sorgen um mich)\b",
        r"\b(ist mein problem, nicht deins)\b",
        r"\b(bin es gewohnt .* allein zu sein)\b",
        r"\b(will dem erfolg ja nicht im weg stehen)\b",
        r"\b(will keine große sache draus machen)\b",
        r"\b(tu einfach, was dich glücklich macht)\b",
        r"\b(ich bin die komplizierte)\b",
        r"\b(entschuldigung für die eigene existenz|entschuldige dass ich so bin)\b"
    ],
    "LIE_CONCEALMENT_MARKER": [
        r"\b(nicht so wichtig|nichts besonderes|nichts von belang)\b",
        r"\b(lange geschichte für einen anderen tag)\b",
        r"\b(warum ist das (jetzt|so) wichtig|warum willst du das wissen)\b",
        r"\b(nicht mehr (ganz )?sicher, wie das war)\b",
        r"\b(thema wechseln|nicht darüber sprechen)\b",
        r"\b(das ist kompliziert|du würdest es nicht verstehen)\b",
        r"\b(geht dich nichts an|meine privatsphäre)\b",
        r"\b(du verhörst mich|fühl mich wie im verhör)\b",
        r"\b(verwechselst da was)\b",
        r"\b(erledigt\. punkt\.)\b"
    ],
    "SEMANTISCHER_NEBEL": [
        # Sucht nach einer Häufung von abstrahierenden oder pseudo-intellektuellen Begriffen
        r"\b(konstrukt|dynamik|metaebene|struktur|energetische signatur|projektion)\b",
        r"\b(algorithisch|schleife|phänomen|ontologisch|archetypisch|binäre antwort)\b",
        r"\b(spektrum|osmotisch|systemisch|dialektik|resonanzraum|ambiguität)\b",
        r"\b(manifestiert sich|mitschwingt|konstellation|re-kalibrierung)\b"
    ],
    "DRAMA_TRIANGLE_MARKER": {
        # Dieser Marker wird erkannt, wenn Muster aus mind. 2 unterschiedlichen Rollen gefunden werden.
        "VERFOLGER": [
            r"\b(schon wieder du|wegen dir|deine schuld|typisch du)\b",
            r"\b(du bist das problem|unfähig|egoistisch|rücksichtslos)\b",
            r"\b(hör auf zu jammern|reiß dich zusammen)\b",
            r"\b(ich habs dir doch gesagt)\b"
        ],
        "OPFER": [
            r"\b(immer ich|warum immer mir|niemand (hilft|versteht) mir)\b",
            r"\b(ich bin (am ende|überfordert|zu schwach|die last))\b",
            r"\b(ich opfere mich auf|bekomme nur undankbarkeit)\b",
            r"\b(ich bin der sündenbock|kann nichts dafür)\b"
        ],
        "RETTER": [
            r"\b(lass mich das machen|ich regel das schon)\b",
            r"\b(komm her, du arme|ich helfe dir)\b",
            r"\b(ich rette dich|ich bügle das wieder aus)\b",
            r"\b(ohne mich geht es nicht|jemand muss es ja machen)\b"
        ]
    }
}

# ==============================================================================
# ANALYSEFUNKTIONEN
# ==============================================================================

def analyze_text_for_complex_markers(text: str) -> dict:
    """
    Analysiert einen Text auf das Vorhandensein der vier komplexen Metamarker.

    Args:
        text: Der zu analysierende Text.

    Returns:
        Ein Dictionary, das die erkannten Marker und die gefundenen Treffer auflistet.
    """
    results = defaultdict(list)

    # --- Test für einfache Marker (Kontrollspirale, Lügen, Nebel) ---
    simple_markers_to_check = ["REACTIVE_CONTROL_SPIRAL", "LIE_CONCEALMENT_MARKER"]
    for marker_name in simple_markers_to_check:
        for pattern in COMPLEX_MARKER_PATTERNS[marker_name]:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                results[marker_name].append(match.group(0).strip())

    # --- Test für Semantischen Nebel (basierend auf Worthäufigkeit) ---
    fog_hits = []
    for pattern in COMPLEX_MARKER_PATTERNS["SEMANTISCHER_NEBEL"]:
        for match in re.finditer(pattern, text, re.IGNORECASE):
            fog_hits.append(match.group(0).strip())
    if len(fog_hits) >= 2:  # Wird als erkannt gewertet, wenn mind. 2 Nebel-Wörter vorkommen
        results["SEMANTISCHER_NEBEL"] = fog_hits
        
    # --- Spezieller Test für Dramadreieck (Rollen-Rotation) ---
    detected_roles = set()
    role_hits = []
    for role, patterns in COMPLEX_MARKER_PATTERNS["DRAMA_TRIANGLE_MARKER"].items():
        for pattern in patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                detected_roles.add(role)
                role_hits.append(f"{role}: '{match.group(0).strip()}'")
    
    # Wird erkannt, wenn Muster aus mindestens zwei verschiedenen Rollen gefunden werden.
    if len(detected_roles) >= 2:
        results["DRAMA_TRIANGLE_MARKER"] = role_hits

    return dict(results)

# ==============================================================================
# DEMONSTRATION / ANWENDUNGSBEISPIEL
# ==============================================================================

if __name__ == "__main__":
    
    # --- Testfälle für jeden der vier Marker ---

    test_reactive_control = "Mach dir keine Sorgen um mich. Ich bin es gewohnt, allein zu sein. Ist mein Problem, nicht deins."
    
    test_drama_triangle = "Immer ich muss alles machen! (Opfer) Aber gut, ich helfe dir ja gerne. (Retter) Wenn du nur nicht so unfähig wärst! (Verfolger)"
    
    test_lie_concealment = "Warum willst du das jetzt wissen? Das ist nicht so wichtig. Lass uns bitte das Thema wechseln."
    
    test_semantic_fog = "Es geht hier nicht um Gefühle, sondern um die systemische Dynamik, die sich in unserer Interaktion manifestiert. Das ist ein komplexes Phänomen."

    print("--- STARTE ANALYSE FÜR KOMPLEXE METAMARKER ---\n")

    # --- Analyse durchführen und Ergebnisse ausgeben ---
    
    print("1. Test für: REACTIVE_CONTROL_SPIRAL")
    results1 = analyze_text_for_complex_markers(test_reactive_control)
    print(f"   Input: '{test_reactive_control}'")
    print(f"   Ergebnis: {results1}\n")

    print("-" * 50)
    
    print("2. Test für: DRAMA_TRIANGLE_MARKER")
    results2 = analyze_text_for_complex_markers(test_drama_triangle)
    print(f"   Input: '{test_drama_triangle}'")
    print(f"   Ergebnis: {results2}\n")
    
    print("-" * 50)
    
    print("3. Test für: LIE_CONCEALMENT_MARKER")
    results3 = analyze_text_for_complex_markers(test_lie_concealment)
    print(f"   Input: '{test_lie_concealment}'")
    print(f"   Ergebnis: {results3}\n")
    
    print("-" * 50)
    
    print("4. Test für: SEMANTISCHER_NEBEL")
    results4 = analyze_text_for_complex_markers(test_semantic_fog)
    print(f"   Input: '{test_semantic_fog}'")
    print(f"   Ergebnis: {results4}\n")

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/COMPLEX_MARKER_PATTERNS.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/CO_REGULATION_COLLAPSE.py
# ====================================================================

import re

CO_REGULATION_COLLAPSE_PATTERNS = [
    r"(ich kann nicht mehr|du auch nicht mehr|wir kommen nicht weiter|ich gebe auf|es ist alles gesagt|das gespräch ist beendet|wir schweigen uns an|lass mich einfach in ruhe|ich kapituliere|genug|wir haben uns festgefahren|jeder macht seins|es macht alles nur schlimmer|ich klinke mich aus|keine kraft mehr|wie mit einer wand reden)",
    r"(wir verletzen uns nur noch|wir drehen uns im kreis|es bleibt nur noch rückzug|wir sind beide raus|kommunikation (bricht|abgebrochen)|beide geben auf|nur noch schweigen|gegenseitiger rückzug|es gibt nichts mehr zu sagen|du gewinnst)"
]

def detect_co_regulation_collapse(text):
    for pattern in CO_REGULATION_COLLAPSE_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/CO_REGULATION_COLLAPSE.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/DETECT_MANEUVER_COMPONENTS.py
# ====================================================================

import re

# ==============================================================================
# DEFINITION DER SEMANTISCHEN KOMPONENTEN
# Diese "Zutaten" machen das Hinhaltemanöver aus.
# ==============================================================================

MANEUVER_COMPONENTS = {
    "ZUSTIMMUNG_VORDERGRÜNDIG": [
        r"\b(ja klar|unbedingt|auf jeden fall|klingt super|gerne|tolle idee|wäre schön)\b"
    ],
    "VERANTWORTUNGS_VERSCHIEBUNG": [
        r"\b(du entscheidest|such du aus|was meinst du denn|was willst du denn|meld du dich|überrasch mich|wie du magst)\b"
    ],
    "VAGE_ZEITPLANUNG": [
        r"\b(mal schauen|später|irgendwann|bald|demnächst|bei Gelegenheit|im Auge behalten|eher spontan|lose anpeilen)\b"
    ],
    "ENTHUSIASTISCHE_UNVERBINDLICHKEIT": [
        r"\b(freu mich schon|unbedingt|riesig|total gern|auf jeden Fall)\b"
        # Dieser Marker wird stark, wenn er mit einer der anderen Komponenten kombiniert wird.
    ]
}

# ==============================================================================
# ANALYSEFUNKTION
# ==============================================================================

def detect_ambivalent_stalling(text: str) -> dict:
    """
    Analysiert einen Text semantisch auf das "Ambivalente Hinhaltemanöver".

    Die Funktion prüft, ob mehrere Komponenten des Manövers im Text vorkommen,
    um eine hohe Treffsicherheit zu gewährleisten.

    Args:
        text: Der zu analysierende Text (z.B. eine Chat-Nachricht, eine Aussage).

    Returns:
        Ein Dictionary mit dem Analyseergebnis.
    """
    found_components = set()
    
    # Durchsuche den Text nach jeder Komponente des Manövers
    for component_name, patterns in MANEUVER_COMPONENTS.items():
        for pattern in patterns:
            if re.search(pattern, text, re.IGNORECASE):
                found_components.add(component_name)
                break  # Ein Treffer pro Komponente reicht

    score = len(found_components)
    is_detected = score >= 2  # Marker wird als erkannt gewertet, wenn mind. 2 Komponenten zutreffen

    analysis = {
        "marker_id": "AMBIVALENT_STALLING_MANEUVER",
        "is_detected": is_detected,
        "confidence_score": score / len(MANEUVER_COMPONENTS),
        "found_components": list(found_components)
    }

    if is_detected:
        explanation = "Die Aussage enthält mehrere Signale für ein Hinhaltemanöver. "
        if "ZUSTIMMUNG_VORDERGRÜNDIG" in found_components and "VERANTWORTUNGS_VERSCHIEBUNG" in found_components:
            explanation += "Einer scheinbaren Zustimmung folgt eine sofortige Rückgabe der Verantwortung."
        elif "ZUSTIMMUNG_VORDERGRÜNDIG" in found_components and "VAGE_ZEITPLANUNG" in found_components:
            explanation += "Eine positive Reaktion wird mit einer unverbindlichen Zeitangabe kombiniert, um eine konkrete Festlegung zu vermeiden."
        else:
            explanation += "Durch die Kombination verschiedener vager Elemente wird eine klare Zusage vermieden."
        analysis["explanation"] = explanation

    return analysis

# ==============================================================================
# DEMONSTRATION / ANWENDUNGSBEISPIEL
# ==============================================================================

if __name__ == "__main__":
    
    test_cases = [
        # Klassischer Fall: Zustimmung + Verantwortungs-Verschiebung
        "Ja klar, super Idee! Was schwebt dir denn so vor?",
        # Klassischer Fall: Zustimmung + vage Zeitplanung
        "Uhm, ja, unbedingt. Lass uns das mal im Auge behalten und dann eher spontan schauen.",
        # Subtiler Fall: Enthusiasmus + Verantwortungs-Verschiebung
        "Auf jeden Fall! Freu mich schon riesig. Melde dich einfach, wenn du losfährst!",
        # Nur eine Komponente -> sollte nicht erkannt werden
        "Vielleicht könnten wir ins Kino gehen.",
        # Eindeutige Zusage -> sollte nicht erkannt werden
        "Ja, lass uns am Samstag um 19 Uhr ins Kino gehen. Ich buche die Karten.",
        # Komplexerer Fall
        "Ich würde total gern mitkommen! Ich muss nur mal schauen, wie lange ich arbeiten muss. Sag du doch mal, wann es dir am besten passen würde."
    ]

    print("--- Semantische Analyse auf Hinhaltemanöver ---\n")

    for i, text in enumerate(test_cases):
        result = detect_ambivalent_stalling(text)
        print(f"Testfall #{i+1}: \"{text}\"")
        if result["is_detected"]:
            print(f"  ▶️ Marker erkannt: JA")
            print(f"  ▶️ Erklärung: {result.get('explanation', 'N/A')}")
            print(f"  ▶️ Gefundene Komponenten: {result['found_components']}")
        else:
            print(f"  ▶️ Marker erkannt: NEIN (Nur {len(result['found_components'])}/2 Komponenten gefunden)")
        print("-" * 50)

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/DETECT_MANEUVER_COMPONENTS.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/DETECT_SYSTEM_TURNING_POINT.py
# ====================================================================

marker: DETECT_SYSTEM_TURNING_POINT.py_MARKER
beschreibung: >
  Metamarker-Cluster aus mehreren Einzelmarker-Ereignissen berechnen.
Beispiel: SYSTEM_TURNING_POINT

Vorstufen: Häufung von LATENT_INSTABILITY_MARKER + ESCALATION_TENSION_MARKER + Rückgang von MICRO_CARE_MARKER

Metamarker-Trigger: Schwelle überschritten → Visualisierung als Kipppunkt/Übergang

Beispiel: SYSTEM_ROBUSTNESS

Vorstufen: Häufung von SYSTEM_RESILIENCE_MARKER + MICRO_CARE_MARKER + INTEGRATION_PROGRESSION_MARKER

Metamarker: Stabile, resiliente Beziehungsdynamik, wird als positiver Pol in Visualisierung angezeigt
beispiele:
  - "def detect_system_turning_point(marker_log):"
  - "instability = marker_log.count("LATENT_INSTABILITY_MARKER")"
  - "escalation = marker_log.count("ESCALATION_TENSION_MARKER")"
  - "micro_care = marker_log.count("MICRO_CARE_MARKER")"
  - "# Beispielschwelle: Instabilität/Eskalation 5+, Micro-Care <2"
  - "if instability >= 3 and escalation >= 2 and micro_care < 2:"
  - "return True"
  - "return False"

semantic_grab:
  description: "Erkennt Muster für detect_system_turning_point.py"
  patterns:
    - rule: "AUTO_PATTERN"
      pattern: r"(muster.*wird.*ergänzt)"
tags: [neu_erstellt, needs_review]


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/DETECT_SYSTEM_TURNING_POINT.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/FAMILIENDYNAMIK_MARKER.py
# ====================================================================

import re

FAMILIENDYNAMIK_PATTERNS = [
    r"(typisch für unsere familie|du bist das schwarze schaf|oma hat dich immer bevorzugt|beim erbe leer ausgegangen|du klingst wie mutter|in dieser familie|deine sturheit kommt von papa|geschichten von vor 20 jahren|ich bin immer der kleine bruder|friedensstifter in dieser familie|maßregelst mich vor meinen kindern|wir halten nur zusammen, weil wir müssen|hier wird erfolg misstrauisch beäugt|keiner redet tacheles|hinterrücks gelästert|nie 'ich liebe dich' gesagt|bei uns wurde nichts ausgesprochen|jede feier endet im streit|du behandelst mich wie 12|älteste bestimmt immer|keiner hört mir zu|das perfekte familienbild|du warst nie da, als ich dich gebraucht habe)"
]

def detect_family_dynamics(text):
    for pattern in FAMILIENDYNAMIK_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/FAMILIENDYNAMIK_MARKER.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/FLIRT_DRIFT_DETECTOR.py
# ====================================================================

# ---------- Hilfsfunktionen ----------
def openness_score(msg):
    """Sehr grobe Heuristik: je mehr Self-Disclosure-Marker, desto höher."""
    return (
        2 * msg.count("A_RAPID_SELF_DISCLOSURE")
        + 1 * msg.count("A_SOFT_FLIRT")
        + 0.5 * msg.count("A_SOFT_COMMITMENT")
    )

def time_diff(t1, t2):
    return abs((t2 - t1).total_seconds())

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/FLIRT_DRIFT_DETECTOR.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/FRAUD_MARKER_PATTERNS.py
# ====================================================================

import re


    "CRYPTO_SCAM": [r"(krypto|bitcoin|ethereum).*investition.*garantiert.*gewinn"],
    "AI_TRADING_SCAM": [r"künstliche.*intelligenz.*trading.*roboter"],
    "WAR_ROMANCE_SCAM": [r"ukraine.*krieg.*militär.*einsatz.*geld.*brauche"],
    "CRYPTO_SCAM": [r"(krypto|bitcoin|ethereum).*investition.*garantiert.*gewinn"],
    "AI_TRADING_SCAM": [r"künstliche.*intelligenz.*trading.*roboter"],
    "WAR_ROMANCE_SCAM": [r"ukraine.*krieg.*militär.*einsatz.*geld.*brauche"],
    "CRYPTO_SCAM": [r"(krypto|bitcoin|ethereum).*investition.*garantiert.*gewinn"],
    "AI_TRADING_SCAM": [r"künstliche.*intelligenz.*trading.*roboter"],
    "WAR_ROMANCE_SCAM": [r"ukraine.*krieg.*militär.*einsatz.*geld.*brauche"],# ==============================================================================
# DEFINITION DER MARKER-MUSTER
# Zusammengefasste und komprimierte Erkennungsmuster für die angeforderten Marker.
# ==============================================================================

FRAUD_MARKER_PATTERNS = {
    "PLATFORM_SWITCH": [
        r"lass uns auf (WhatsApp|Telegram|Signal) wechseln",
        r"app (funktioniert nicht|ist langsam|spinnt)",
        r"mitgliedschaft .* läuft aus",
        r"hier (unpersönlich|unsicher)",
        r"gib mir deine (nummer|e-mail)",
        r"per (mail|signal) viel (besser|sicherer|persönlicher)"
    ],
    "CRISIS_MONEY_REQUEST": [
        r"brauche dringend geld",
        r"konto (gesperrt|eingefroren)",
        r"paket .* zoll",
        r"(krankenhaus|operation|arzt) .* bezahlen",
        r"in (notlage|schwierigkeiten|gefahr)",
        r"alles verloren",
        r"kannst du mir (helfen|leihen)"
    ],
    "URGENCY_SCARCITY": [
        r"(sofort|dringend|schnell|jetzt sofort)",
        r"nur noch (heute|wenige stunden|kurze zeit)",
        r"frist läuft (heute|bald|gleich) ab",
        r"einmalige (chance|gelegenheit)",
        r"bevor es zu spät ist",
        r"akku ist fast leer"
    ],
    "WEBCAM_EXCUSE": [
        r"kamera (ist kaputt|funktioniert nicht)",
        r"internet .* zu (schlecht|langsam)",
        r"(militärbasis|ölplattform|mission) .* verboten",
        r"sehe (furchtbar|schrecklich) aus",
        r"bin zu (schüchtern|müde)",
        r"vielleicht (morgen|später)"
    ],
    "UNTRACEABLE_PAYMENT_METHOD": [
        r"(geschenkkarte|gift card|gutschein)",
        r"(Apple|Google Play|Steam|Amazon)-?karten?",
        r"(codes|nummern) schicken",
        r"(Western Union|MoneyGram)",
        r"(Bitcoin|Krypto|BTC)",
        r"nicht rückverfolgbar|anonym|diskret"
    ],
    "GASLIGHTING_GUILT": [
        r"du (bist zu sensibel|übertreibst|bist verrückt)",
        r"bildest dir das nur ein",
        r"das habe ich nie gesagt",
        r"dein misstrauen (zerstört|verletzt)",
        r"wenn du mich (lieben|mir vertrauen) würdest",
        r"mach aus einer mücke keinen elefanten"
    ],
    "TRANSLATION_ARTIFACT": [
        # Sucht nach ungrammatischen oder seltsam formulierten Phrasen
        r"frau von hoher qualität",
        r"mein herz ist springen",
        r"mache arbeit als",
        r"ich bin warten auf",
        r"treffen in person gesicht",
        r"liebe dich mehr als das leben selbst tut"
    ]
}

# ==============================================================================
# ANALYSEFUNKTION
# ==============================================================================

def detect_fraud_markers(text: str) -> list:
    """
    Analysiert einen Text und identifiziert eine Liste von Betrugs-Markern.

    Args:
        text: Der zu analysierende Textstring.

    Returns:
        Eine Liste mit den Namen der erkannten Marker.
    """
    detected_markers = []
    
    # Iteriere durch jeden Marker und seine zugehörigen Muster
    for marker_name, patterns in FRAUD_MARKER_PATTERNS.items():
        # Prüfe jedes Muster für den aktuellen Marker
        for pattern in patterns:
            if re.search(pattern, text, re.IGNORECASE):
                # Wenn ein Muster gefunden wird, füge den Marker zur Ergebnisliste hinzu
                # und gehe zum nächsten Marker-Typ über (um Doppelungen zu vermeiden).
                if marker_name not in detected_markers:
                    detected_markers.append(marker_name)
    
    return detected_markers

# ==============================================================================
# DEMONSTRATION / ANWENDUNGSBEISPIEL
# ==============================================================================

if __name__ == "__main__":
    
    # Ein typischer, fiktiver Scam-Chatverlauf, der mehrere Marker enthält
    scam_chat_example = """
    Hallo meine Königin, du bist eine Frau von hoher Qualität. Ich bin so froh, dich kennengelernt zu haben.
    Aber diese App ist wirklich langsam und unpersönlich. Lass uns auf WhatsApp wechseln, das ist viel besser.
    Ich würde dich gerne anrufen, aber meine Kamera ist leider kaputt, vielleicht morgen.
    Du, es ist etwas Schreckliches passiert. Mein Sohn hatte einen Unfall und ich brauche dringend Geld für das Krankenhaus.
    Die Zahlung muss sofort erfolgen, sonst operieren die Ärzte nicht. Bitte, wenn du mir vertrauen würdest, würdest du mir helfen.
    Am einfachsten geht es mit Apple-Geschenkkarten aus dem Supermarkt.
    """
    
    print("--- Analysiere Scam-Beispieltext ---")
    
    # Rufe die Analysefunktion auf
    found_flags = detect_fraud_markers(scam_chat_example)
    
    # Gib die Ergebnisse aus
    if found_flags:
        print(f"\n⚠️ Es wurden {len(found_flags)} verdächtige Marker im Text identifiziert:\n")
        for i, flag in enumerate(found_flags):
            print(f"  {i+1}. {flag}")
        print("\nDas kombinierte Auftreten dieser Marker stellt ein hohes Risiko dar.")
    else:
        print("\n✅ Keine der definierten Betrugs-Marker im Text gefunden.")

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/FRAUD_MARKER_PATTERNS.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/META_OVERINTELLECTUALIZATION.py
# ====================================================================

import re

META_OVERINTELLECTUALIZATION_PATTERNS = [
    r"(diskrepanz|dissonanz|paradigma|systemisch|homöostase|sozial(?:es|e|er)? konstrukt|evolutionär|algorithmus|ontologisch|hypothese|commitment-level|reziprok[ea]? investition|metaebene|struktur(?:ell)?|dekonstruier(?:e|st|t)|kognitiv|symbiose|psychodynamik|soziobiologisch|feedback(-| )?schleife|somatisch|paradigmenwechsel)",
    r"(ich beobachte nur|ich analysiere|ich dekonstruiere|ich nehme wahr|betrachten wir das|wir sollten erst die begrifflichkeiten klären|das ist keine emotion, sondern)",
    r"(klassische spannung (zwischen|von)|systemisches beispiel|mikrokosmos|ontologisch betrachtet|definition des begriffs|hypothese über dich|commitment[- ]?level|reziproke investition)"
]

def detect_meta_overintellectualization(text):
    for pattern in META_OVERINTELLECTUALIZATION_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/META_OVERINTELLECTUALIZATION.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/PIPELINE.py
# ====================================================================

# ==============================  pipeline.py  ==============================
"""
Minimal-Pipeline zum Erkennen von Flirt-/Ambivalenz-Drift.
1.   Daten-Ingest  – Chat-Records ◁ JSON / DB / API
2.   Marker-Scoring – Regex-/ML-Tagger → marker_lookup
3.   Detector-Stage – ruft unsere Detector-Funktionen
4.   Reporting      – schreibt Flags in DB oder sendet Webhook
"""
from datetime import datetime, timedelta
from detector_utils import linear_slope
from detectors import (
    detect_sustained_contact,
    detect_secret_bonding,
    detect_adaptive_polarization,
    detect_flirt_dance_drift,
)

# --------------------------------------------------------------------------
# 1)  Simulierter Chat-Stream (chronologisch)
# --------------------------------------------------------------------------
CHAT_LOG = [
    {"id": "m1", "ts": datetime(2025, 7, 1, 9,  0), "text": "Du bist süß ;)",     },
    {"id": "m2", "ts": datetime(2025, 7, 1, 9,  1), "text": "Nicht verraten!",     },
    {"id": "m3", "ts": datetime(2025, 7, 2, 22, 0), "text": "Gute Nacht 🌙",       },
    {"id": "m4", "ts": datetime(2025, 7, 3,  8, 0), "text": "Dir zum ersten Mal …"},
    {"id": "m5", "ts": datetime(2025, 7, 3, 22, 0), "text": "Lösch den Chat 😊",   },
    {"id": "m6", "ts": datetime(2025, 7, 4, 20, 0), "text": "Du fehlst mir – egal",},
    {"id": "m7", "ts": datetime(2025, 7, 5, 21, 0), "text": "VIP-Tattoo? 😏",      },
]

# --------------------------------------------------------------------------
# 2)  VERY simple Marker-Tagging (RegEx-Demo) -------------------------------
#     → in der Praxis ersetzt du das durch dein echtes Marker-Engine-API.
# --------------------------------------------------------------------------
import re
PATTERNS = {
    "A_SOFT_FLIRT": re.compile(r"\b(süß|😉|😊|😏)\b", re.I),
    "S_MENTION_OF_SECRECY": re.compile(r"(nicht verraten|unter uns|lösch)", re.I),
    "C_ADAPTIVE_POLARIZATION": re.compile(r"du fehlst mir.*egal", re.I),
    "C_RAPID_SELF_DISCLOSURE": re.compile(r"zum ersten mal|noch nie erzählt", re.I),
}
marker_lookup = {}

for m in CHAT_LOG:
    found = [mid for mid, rx in PATTERNS.items() if rx.search(m["text"])]
    marker_lookup[m["id"]] = found

# --------------------------------------------------------------------------
# 3)  Detector-Stage --------------------------------------------------------
# --------------------------------------------------------------------------
detected = {}

ok, info = detect_sustained_contact(CHAT_LOG)
detected["C_SUSTAINED_CONTACT"] = ok

ok, info = detect_secret_bonding(CHAT_LOG, marker_lookup)
detected["C_SECRET_BONDING"] = ok

ok, info = detect_adaptive_polarization(CHAT_LOG, marker_lookup)
detected["C_ADAPTIVE_POLARIZATION"] = ok

ok, span = detect_flirt_dance_drift(CHAT_LOG, marker_lookup)
detected["MM_FLIRT_DANCE_DRIFT"] = ok

# --------------------------------------------------------------------------
# 4)  Report / Action -------------------------------------------------------
# --------------------------------------------------------------------------
if detected["MM_FLIRT_DANCE_DRIFT"]:
    print("⚠️  FLIRT-DANCE-DRIFT erkannt!")
    for msg in span:
        print(f"  {msg['ts'].strftime('%Y-%m-%d %H:%M')} – {msg['text']}")
else:
    print("Keine kritische Flirt-Drift erkannt.")

# ============================  Ende pipeline.py  ===========================


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/PIPELINE.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/PROJECTIVE_IDENTIFICATION_SEMANTIC_DETECTION.py
# ====================================================================

marker: PROJECTIVE_IDENTIFICATION_SEMANTIC_DETECTIO.py_MARKER
beschreibung: >
  # -*- coding: utf-8 -*-

# #############################################################################
#  STRUKTURIERTE DEFINITIONEN PSYCHOLOGISCHER KOMMUNIKATIONSMARKER
#
#  Zweck:
#  Diese Datei enthält eine Liste von detailliert definierten Markern für die
#  semantische Analyse von Kommunikation. Jeder Marker ist als Python-Dictionary
#  strukturiert und enthält:
#    - Beschreibung und typische Szenarien
#    - Realistische Beispiele
#    - Mögliche Kombinationen zu Meta-Markern
#    - "Semantic Grab": Konkrete Muster für die KI-Erkennung
#    - Psychologischen Hintergrund zur theoretischen Verankerung
#
#  Diese Struktur ist darauf ausgelegt, von KI-Systemen, insbesondere
#  NLP-Modellen, verarbeitet zu werden, um tiefere Einblicke in
#  Gesprächsdynamiken zu gewinnen.
# #############################################################################
beispiele:
  - "STRUKTURIERTE_MARKER_DEFINITIONEN = ["
  - "{"
  - ""id": "self_sabotage_loop","
  - ""name": "SELF_SABOTAGE_LOOP_MARKER","
  - ""beschreibung": "Ein Verhaltens- oder Kommunikationsmuster, bei dem eine Person ihre eigenen Chancen auf Erfolg, Nähe oder eine positive Problemlösung aktiv oder passiv untergräbt.","
  - ""szenarien": ["
  - ""Dating & Beziehungsanbahnung (z.B. Absagen von Verabredungen)","
  - ""Berufsleben & Karriere (z.B. Ablehnen von Beförderungen, Verpassen von Fristen)","
  - ""Konfliktlösung (z.B. Gesprächsabbruch aus Angst, alles schlimmer zu machen)","
  - ""Therapie- oder Coaching-Prozesse""
  - "],"
  - ""dynamik": "Die treibende Kraft ist eine tiefsitzende Angst (z.B. vor Versagen, Ablehnung, Intimität), die durch die Selbstsabotage 'kontrolliert' werden soll. Es ist eine dysfunktionale Schutzstrategie nach dem Motto: 'Wenn ich es selbst zerstöre, kontrolliere ich den Schmerz und die Enttäuschung'. Dies führt zu einer sich selbst erfüllenden Prophezeiung.","
  - ""beispiele": ["
  - ""Beziehung/Dating: 'Du verdienst jemanden, der besser für dich ist als ich.'","
  - ""Beruf: 'Ach, die Beförderung sollen sie jemand anderem geben. Der Druck wäre mir eh zu groß.'","
  - ""Konflikt: 'Lass uns das Thema beenden, ich mache doch eh alles nur noch schlimmer.'","
  - ""Hilfe ablehnen: 'Nein, danke für die Hilfe. Ich will dir keine Umstände machen.'","
  - ""Passive Sabotage: 'Ich habe die Bewerbungsfrist verpasst. War wohl einfach nicht sein soll.'","
  - ""Proaktive Sabotage: 'Ich habe das Projekt absichtlich nicht perfekt gemacht, damit die Erwartungen beim nächsten Mal nicht so hoch sind.'","
  - ""Digitale Variante (Chat): 'Ich antworte ihm extra nicht, dann merkt er vielleicht von selbst, dass ich zu kompliziert bin.'""
  - "],"
  - ""metamarker_kombinationen": ["
  - "{"
  - ""name": "Destruktive Hilflosigkeit","
  - ""kombination_mit": "Emotionaler Rückzugsdruck","
  - ""szenario": "Die Selbstsabotage wird zur Waffe. Beispiel: 'Ich weiß, ich kriege das Projekt nicht hin. Ich ziehe mich zurück, bevor ich alles ruiniere. Dir scheint das ja ohnehin egal zu sein.'""
  - "},"
  - "{"
  - ""name": "Proaktive Resignation","
  - ""kombination_mit": "PLAYING_VICTIM_MARKER","
  - ""szenario": "Die Selbstsabotage wird als unabwendbares Schicksal dargestellt, das Mitleid erregen soll. Beispiel: 'Ich fange mit der Diät lieber erst gar nicht an, ich halte das eh nicht durch. Bei mir scheitert ja sowieso immer alles, egal was ich versuche.'""
  - "}"
  - "],"
  - ""semantic_grab": {"
  - ""description": "Erkennt eine Kombination aus negativen Selbstbewertungen, der Ablehnung von Chancen, dem Zurückweisen von Hilfe und dem Fokus auf potenzielles Scheitern. Eine KI sollte hier nicht nur Keywords, sondern die semantische Ähnlichkeit zu diesen Mustern bewerten.","
  - ""patterns": ["
  - "{"
  - ""rule": "PREEMPTIVE_SURRENDER","
  - ""pattern": "(aber ich glaube|ich habe Angst|ich sollte lieber) + (ich schaffe das nicht|sagen wir es lieber ab|bin nicht gut genug)","
  - ""example": "Das Date wäre toll, aber ich glaube, ich bin nicht interessant genug und sage lieber ab.""
  - "},"
  - "{"
  - ""rule": "FOCUS_ON_FAILURE_POTENTIAL","
  - ""pattern": "(bevor ich alles kaputt mache|damit ich niemanden enttäusche|ich würde es nur ruinieren) + (lasse ich es lieber gleich|mache ich es erst gar nicht)","
  - ""example": "Bevor ich dich nur enttäusche, fangen wir lieber gar nichts Ernstes an.""
  - "},"
  - "{"
  - ""rule": "DEVALUING_OPPORTUNITIES","
  - ""pattern": "(Die Beförderung|Das Kompliment|Die Chance) + (wäre eh zu viel|meinte er nicht so|ist nichts für mich)","
  - ""example": "Das Kompliment meines Chefs habe ich ignoriert. Er meinte das sicher nicht so.""
  - "},"
  - "{"
  - ""rule": "WORTHINESS_NEGATION","
  - ""pattern": "(Du verdienst jemanden, der besser ist|Ich bin es nicht wert|Ich bin zu kompliziert für dich)","
  - ""example": "Ich bin es nicht wert, dass man sich so um mich bemüht.""
  - "},"
  - "{"
  - ""rule": "PASSIVE_SABOTAGE_RATIONALIZATION","
  - ""pattern": "(habe die Frist verpasst|nicht geschafft anzurufen|vergessen zu antworten) + (war wohl nicht sein soll|war eh besser so)","
  - ""example": "Ich habe die Bewerbungsfrist verpasst. War wohl einfach nicht sein soll.""
  - "}"
  - "]"
  - "},"
  - ""psychologischer_hintergrund": ["
  - "{"theorie": "Schematherapie (z.B. Schema 'Versagen' oder 'Unzulänglichkeit')"},"
  - "{"theorie": "Bindungstheorie (oft bei desorganisiertem oder vermeidendem Bindungsstil)"},"
  - "{"abwehrmechanismus": "Agieren (das Ausleben eines unbewussten Konflikts), Regression"}"
  - "]"
  - "},"
  - "{"
  - ""id": "guilt_tripping","
  - ""name": "GUILT_TRIPPING_MARKER (Schuldinduktion)","
  - ""beschreibung": "Die manipulative Taktik, bei einer anderen Person gezielt Schuldgefühle zu erzeugen, um sie zu einem bestimmten Verhalten zu bewegen. Dies geschieht durch offene oder subtile Vorwürfe, Appelle an Pflichtgefühl oder die Betonung des eigenen Leidens.","
  - ""szenarien": ["
  - ""Familiäre Konflikte","
  - ""Paarbeziehungen","
  - ""Arbeitsumfeld (z.B. bei der Verteilung von Aufgaben)""
  - "],"
  - ""dynamik": "Die 'Formel' lautet oft: Appell an Pflicht/Liebe + Implizite Androhung von emotionalem Entzug + Erzeugung von Schuldgefühlen. Der andere soll sich moralisch verpflichtet fühlen, den Wünschen des Manipulators zu entsprechen.","
  - ""beispiele": ["
  - ""Passiv-aggressiv: 'Schon gut, feier du deinen Geburtstag groß. Ich sitze dann halt an dem Tag allein zu Hause, macht nichts.'","
  - ""Offener Vorwurf: 'Nach all den Jahren, die ich für diese Firma geopfert habe, kannst du mir nicht mal diesen einen freien Tag gönnen?'","
  - ""Liebe an Bedingungen knüpfen: 'Wenn du mich wirklich lieben würdest, würdest du mich heute nicht allein lassen wollen, wo ich so einen schweren Tag hatte.'","
  - ""Versteckter Appell: 'Du musst mir nicht helfen. Ich will ja nicht, dass du dich meinetwegen zu etwas gezwungen fühlst.'","
  - ""Bezug auf Dritte: 'Dein Vater wäre sehr traurig, wenn er sehen würde, wie du mit mir umgehst.'""
  - "],"
  - ""metamarker_kombinationen": [],"
  - ""semantic_grab": {"
  - ""description": "Erkennt Aussagen, die eine Diskrepanz zwischen dem Gesagten und dem Gemeinten aufweisen und oft das Leiden der sprechenden Person oder die undankbare Tat der anderen Person betonen.","
  - ""patterns": ["
  - "{"
  - ""rule": "SACRIFICE_REFERENCE","
  - ""pattern": "(Nach allem, was ich getan habe|Ich habe auf so vieles verzichtet|Ich opfere mich auf) + (und das ist der Dank|dann würdest du|kannst du nicht mal)","
  - ""example": "Nach allem, was ich für dich getan habe, kannst du mir nicht mal das gönnen?""
  - "},"
  - "{"
  - ""rule": "PASSIVE_AGGRESSIVE_PERMISSION","
  - ""pattern": "(Geh ruhig|Mach nur|Feier du) + (ich sitze dann allein|ich mach das schon|ist ja nicht so schlimm)","
  - ""example": "Geh ruhig zu deinen Freunden, ich mache hier schon allein weiter.""
  - "},"
  - "{"
  - ""rule": "CONDITIONAL_LOVE_TEST","
  - ""pattern": "(Wenn du mich wirklich lieben würdest|Jemand, der liebt) + (würdest du das tun|würdest du nicht)","
  - ""example": "Jemand, der mich liebt, würde das für mich tun.""
  - "},"
  - "{"
  - ""rule": "DISAPPOINTMENT_DECLARATION","
  - ""pattern": "(Ich bin so enttäuscht von dir|Ich hätte nie gedacht, dass du so bist|Deine Worte verletzen mich)","
  - ""example": "Ich bin einfach nur enttäuscht. Ich hätte nie gedacht, dass du so egoistisch sein kannst.""
  - "}"
  - "]"
  - "},"
  - ""psychologischer_hintergrund": ["
  - "{"theorie": "Transaktionsanalyse"},"
  - "{"abwehrmechanismus": "Projektion, Externalisierung"}"
  - "]"
  - "},"
  - "{"
  - ""id": "escalation","
  - ""name": "ESCALATION_MARKER","
  - ""beschreibung": "Ein plötzlicher, intensiver Anstieg der emotionalen und/oder physiologischen Erregung, der oft zu einem Gesprächsabbruch oder einer Drohung führt. Die Person fühlt sich überfordert, nicht gehört oder provoziert.","
  - ""szenarien": ["
  - ""Streit (online oder offline)","
  - ""Hitzige Diskussionen","
  - ""Konfrontation mit einem Trigger","
  - ""Technische Frustration (z.B. im Gaming)""
  - "],"
  - ""dynamik": "Die Kommunikation verlässt die rationale Ebene und wird von Wut, Panik oder starker Frustration dominiert. Oft folgt ein TERMINATION_MARKER, um die als unerträglich empfundene Situation zu beenden ('Ich bin raus').","
  - ""beispiele": ["
  - ""Kombination aus Physiologie, Emotion und Abbruch: 'Mein Puls ist auf 180! Wenn Sie das jetzt nicht sofort klären, dann eskaliert das hier gewaltig, das garantiere ich Ihnen!'","
  - ""Wut & Abbruch: 'Alter, ich raste aus vor Wut, fühl mich komplett verarscht und jetzt reicht’s, ciao.'","
  - ""Technische Frustration: 'ALTER, DAS LAGGT SO HARD! ICH RASTE AUS! FUCK IT, ICH BIN RAUS FÜR HEUTE!'","
  - ""Angst/Panik: 'Ich krieg keine Luft mehr, alles dreht sich, ich glaube, ich kippe um, irgendwas stimmt nicht!'","
  - ""Digitale Variante: '🤬🤬🤬 ICH HASSE DAS. BIN WEG. 🚪'""
  - "],"
  - ""metamarker_kombinationen": ["
  - "{"
  - ""name": "Fight-or-Flight-Response","
  - ""kombination_mit": "TERMINATION_MARKER","
  - ""szenario": "Die Eskalation ist so hoch, dass die einzige verbleibende Handlung der sofortige Rückzug ist. Beispiel: 'Ich zittere am ganzen Körper, weil du mir schon wieder nicht zuhörst. Ich geh jetzt, bevor ich was Falsches sage!'""
  - "}"
  - "],"
  - ""semantic_grab": {"
  - ""description": "Erkennt eine hohe Dichte an Wörtern, die starke Emotionen (Wut, Zorn) und körperliche Stressreaktionen (Herzrasen, Zittern, keine Luft) beschreiben, oft in Kombination mit einem Abbruch des Gesprächs.","
  - ""patterns": ["
  - "{"
  - ""rule": "PHYSIOLOGICAL_STRESS_INDICATOR","
  - ""pattern": "Mein (Herz rast|Puls hämmert)|(ich zittere|kriege keine Luft|mein Blut kocht)","
  - ""example": "Mein Herz hämmert, ich zittere am ganzen Körper.""
  - "},"
  - "{"
  - ""rule": "PEAK_EMOTION_DECLARATION","
  - ""pattern": "(ich raste aus|ich explodiere|ich platze gleich|ich koche vor Zorn|ich kann nicht mehr)","
  - ""example": "Ich platze gleich vor Wut!""
  - "},"
  - "{"
  - ""rule": "ACCUSATION_TRIGGER","
  - ""pattern": "(weil du nicht zuhörst|du ignorierst mich|du verarschst mich|du hast null Respekt)","
  - ""example": "Du hörst mir einfach nie zu!""
  - "},"
  - "{"
  - ""rule": "COMBINED_ESCALATION_TERMINATION","
  - ""pattern": "(raste aus|reicht's jetzt|Schnauze voll) + (und ciao|ich bin raus|ich geh offline)","
  - ""example": "Jetzt reicht's, ich bin hier raus.""
  - "}"
  - "]"
  - "},"
  - ""psychologischer_hintergrund": ["
  - "{"theorie": "Polyvagal-Theorie (sympathische Mobilisierung)"},"
  - "{"abwehrmechanismus": "Affektdurchbruch, Agieren"}"
  - "]"
  - "},"
  - "{"
  - ""id": "projective_identification","
  - ""name": "PROJECTIVE_IDENTIFICATION_MARKER","
  - ""beschreibung": "Ein komplexer Abwehrmechanismus, bei dem eine Person eigene, oft unakzeptable Gefühle oder Eigenschaften nicht nur auf eine andere Person projiziert, sondern diese unbewusst dazu bringt, sich entsprechend dieser Projektion zu fühlen und zu verhalten.","
  - ""szenarien": ["
  - ""Intensive Paarbeziehungen","
  - ""Eltern-Kind-Dynamiken","
  - ""Therapeutische Beziehungen","
  - ""Toxische Arbeitsumgebungen""
  - "],"
  - ""dynamik": "Es ist ein interpersoneller Prozess. Beispiel: Person A fühlt sich innerlich inkompetent. Statt dieses Gefühl bei sich wahrzunehmen, behandelt sie Person B herablassend und kritisiert sie als inkompetent ('Kannst du gar nichts richtig machen?'). Person B fühlt sich dadurch verunsichert und fängt an, Fehler zu machen, wodurch sie sich tatsächlich inkompetent fühlt und die Projektion von Person A bestätigt.","
  - ""beispiele": ["
  - ""Projektion von Wertlosigkeit: A: 'Schon wieder die falsche Milch gekauft. Kannst du gar nichts richtig machen?' B: 'Ich hab das Gefühl, ich bin für dich nichts wert!' A: 'Siehst du? Du hast ein Problem mit deinem Selbstwert, nicht ich.'","
  - ""Projektion von Aggression: A verhält sich passiv-aggressiv. B wird wütend. A sagt: 'Ich verstehe nicht, warum du immer so ausrasten musst.'","
  - ""Projektion von Eifersucht: A kontrolliert B permanent und sagt dann: 'Deine krankhafte Eifersucht macht unsere Beziehung kaputt.'","
  - ""Projektion von Unzuverlässigkeit: 'Du bist so unzuverlässig, ich kann mich nie auf dich verlassen.'""
  - "],"
  - ""metamarker_kombinationen": [],"
  - ""semantic_grab": {"
  - ""description": "Erkennt stabile, negative 'Du-bist-so'-Zuschreibungen, die dem Gegenüber eine Eigenschaft als Wesensmerkmal unterstellen. Die KI muss den Kontext analysieren: Findet die Zuschreibung in einem Konflikt statt? Reagiert das Gegenüber, indem es die zugeschriebene Eigenschaft bestätigt?","
  - ""patterns": ["
  - "{"
  - ""rule": "NEGATIVE_TRAIT_ASSIGNMENT","
  - ""pattern": "(Du bist so|Du bist einfach|Deine) + (unzuverlässig|unordentlich|eifersüchtig|überfordert|ungeduldig|kontrollsüchtig)","
  - ""example": "Du bist einfach so ein Kontrollfreak.""
  - "},"
  - "{"
  - ""rule": "INCOMPETENCE_ACCUSATION","
  - ""pattern": "(Kannst du gar nichts richtig machen|Das hast du ja wieder toll hinbekommen|Du scheinst damit überfordert zu sein)","
  - ""example": "Das hast du ja wieder toll hinbekommen.""
  - "},"
  - "{"
  - ""rule": "EMOTIONAL_STATE_PROJECTION","
  - ""pattern": "(Warum musst du immer so ausrasten|Du bist zu sensibel|Du übertreibst total)","
  - ""example": "Warum musst du wegen jeder Kleinigkeit so ausrasten?""
  - "},"
  - "{"
  - ""rule": "BEHAVIOR_LABELING","
  - ""pattern": "(Du klammerst|Du machst immer Probleme|Du lässt mich immer allein)","
  - ""example": "Du klammerst so sehr, das macht mich fertig.""
  - "}"
  - "]"
  - "},"
  - ""psychologischer_hintergrund": ["
  - "{"theorie": "Objektbeziehungstheorie (Melanie Klein, Otto Kernberg)"},"
  - "{"abwehrmechanismus": "Projektive Identifizierung (gilt als primitiver Abwehrmechanismus)"}"
  - "]"
  - "}"
  - "]"
  - "# Beispielhafter Aufruf, um die Daten zu verwenden:"
  - "if __name__ == "__main__":"
  - "for marker in STRUKTURIERTE_MARKER_DEFINITIONEN:"
  - "print(f"--- Marker: {marker['name']} ---")"
  - "print(f"Beschreibung: {marker['beschreibung']}")"
  - "print(f"Beispiel: {marker['beispiele'][0]}")"
  - "if marker['semantic_grab']['patterns']:"
  - "print(f"Erstes semantisches Pattern ({marker['semantic_grab']['patterns'][0]['rule']}):")"
  - "print(f"  {marker['semantic_grab']['patterns'][0]['pattern']}")"
  - "print("\n" + "="*50 + "\n")"

semantic_grab:
  description: "Erkennt Muster für projective_identification_semantic_detectio.py"
  patterns:
    - rule: "AUTO_PATTERN"
      pattern: r"(muster.*wird.*ergänzt)"
tags: [neu_erstellt, needs_review]


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/PROJECTIVE_IDENTIFICATION_SEMANTIC_DETECTION.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/PSEUDOMETA_PATTERN.py
# ====================================================================

import re

PSEUDOMETA_PATTERN_PATTERNS = [
    r"(metaebene|kommunikationsmatrix|semantik|axiome|double-bind|projektionen|beziehungsebene|interaktionsmuster|wir reden aneinander vorbei|das ist kein inhaltliches, sondern ein beziehungsproblem|unsere begrifflichkeiten klären|wir analysieren nur|symptom für ein strukturelles problem|die art, wie wir streiten, ist ein spiegel|sprachliche ebene|supervision für kommunikation|wir sprechen unterschiedliche sprachen der liebe)",
    r"(wir sollten nicht über den müll reden, sondern über das symbol dahinter|das eigentliche thema ist größer|soziologisch hochinteressant|wir sind beide nur schauspieler in einem drama|strukturproblem, kein gefühlsproblem)"
]

def detect_pseudometa_pattern(text):
    for pattern in PSEUDOMETA_PATTERN_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/PSEUDOMETA_PATTERN.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/PYTHON_MARKER.py
# ====================================================================

"""
PYTHON_MARKER - Semantic Marker
Python-basierter Marker: PYTHON_MARKER
"""

import re

class PYTHON_MARKER:
    """
    Python-basierter Marker: PYTHON_MARKER
    """
    
    examples = [
    ]
    
    patterns = [
        re.compile(r"(muster.*wird.*ergänzt)", re.IGNORECASE)
    ]
    
    semantic_grabber_id = "AUTO_GENERATED"
    
    def match(self, text):
        """Prüft ob der Text zum Marker passt"""
        for pattern in self.patterns:
            if pattern.search(text):
                return True
        return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/PYTHON_MARKER.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/REACTIVE_CONTROL_SPIRAL.py
# ====================================================================

import re

REACTIVE_CONTROL_SPIRAL_PATTERNS = [
    r"(du hast angefangen|nein, du hast angefangen|du willst stur sein|ich kann sturer sein|jedes mal, wenn ich|toll, weil du eingeschnappt bist, bin ich jetzt auch eingeschnappt|ich gehe erst einen schritt, wenn du einen gehst|ich senke meinen ton erst, wenn du deinen senkst|ich schreie durch die tür|du manipulierst|nein, du zwingst mich dazu|wenn du provozierst, kann ich für nichts garantieren|du willst es auf die harte tour|solange du dich nicht entschuldigst, entschuldige ich mich auch nicht|kontrolle behalten|es endet erst, wenn einer aufgibt)",
    r"(du willst ehrlich sein, okay, hier ist die ehrliche meinung|du bist das problem|du willst immer das letzte wort|ich hab dir zehnmal geschrieben|ich ignoriere jetzt mal deine anrufe|jeder deiner vorwürfe gibt mir nur mehr munition)"
]

def detect_reactive_control_spiral(text):
    for pattern in REACTIVE_CONTROL_SPIRAL_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/REACTIVE_CONTROL_SPIRAL.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/RESONANCE.py
# ====================================================================

import re

RESONANCE_PATTERNS = [
    r"(ich verstehe dich|ich sehe dich|ich fühle mit dir|wow, das eröffnet mir eine neue perspektive|das kenne ich auch|mir geht es genauso|ich urteile nicht|ich höre dir zu|danke, dass du das sagst|du sprichst mir aus der seele|ich fühle mich verbunden|du gibst mir neue einsicht|das ergibt sinn|genau das ist der punkt|ich bin froh, dass du es aussprichst|deine worte berühren mich|du bist nicht allein damit|ich kann zu 100 prozent nachfühlen)",
    r"(das fühlt sich so wahr an|ich muss dir nicht mal zustimmen, aber ich verstehe dich|ich kann dir keine lösung geben, aber ich bin da|die energie hat sich gerade verändert|du bist wirklich gesehen)"
]

def detect_resonance(text):
    for pattern in RESONANCE_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/RESONANCE.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/SCAMMER_BEHAVIOUR_DETECT.py
# ====================================================================

kategorie: SEMANTIC
marker:
  category: Semantic
  context_rule: []
  description: Automatisch generierte Beschreibung für SCAMMER_BEHAVIOUR_DETECT.py_MARKER
  examples:
  - 'marker_name: SCAMMER_BEHAVIOUR_DETECT.py_MARKER'
  - 'marker: SCAMMER_BEHAVIOUR_DETECT.py_MARKER'
  - Erkennt manipulative Verhaltensmuster aus Romance-Scam-Situationen.
  - Die Marker basieren auf semantischen Strategien wie Love Bombing, Future Faking,
    Schuldumkehr, Isolation und Krisenmanipulation.
  - Diese Muster treten häufig in digitaler Kommunikation auf und lassen sich nicht
    auf einzelne Schlüsselwörter reduzieren.
  id: S_SCAMMER_BEHAVIOUR_DETECTPY
  level: 2
  name: SCAMMER_BEHAVIOUR_DETECT.py
  pattern: []
  semantic_grabber_id: AUTO_SEM_20250714_EF92
  semantic_tags:
  - scammer-behaviour-detect.py-marker
marker_name: SCAMMER_BEHAVIOUR_DETECT.py_MARKER


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/SCAMMER_BEHAVIOUR_DETECT.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/SCAMMER_DETECTIO.py
# ====================================================================

kategorie: SEMANTIC
marker:
  category: Semantic
  context_rule: []
  description: Automatisch generierte Beschreibung für SCAMMER_DETECTIO.py_MARKER
  examples:
  - 'marker_name: SCAMMER_DETECTIO.py_MARKER'
  - 'marker: SCAMMER_DETECTIO.py_MARKER'
  - Erkennt manipulative Verhaltensmuster aus Romance-Scam-Situationen.
  - Die Marker basieren auf semantischen Strategien wie Love Bombing, Future Faking,
    Schuldumkehr, Isolation und Krisenmanipulation.
  - Diese Muster treten häufig in digitaler Kommunikation auf und lassen sich nicht
    auf einzelne Schlüsselwörter reduzieren.
  id: S_SCAMMER_DETECTIOPY
  level: 2
  name: SCAMMER_DETECTIO.py
  pattern: []
  semantic_grabber_id: AUTO_SEM_20250714_2F39
  semantic_tags:
  - scammer-detectio.py-marker
marker_name: SCAMMER_DETECTIO.py_MARKER


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/SCAMMER_DETECTIO.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/SOCIAL_INTERACTION.py
# ====================================================================

marker: SOCIAL_INTERACTION.py_MARKER
beschreibung: >
  "beschreibung": (
            "Erkennt kommunikative Muster von freundlich-sympathischem Flirten ohne Grenzüberschreitung. "
            "Typisch sind spielerische Komplimente, Humor, harmlose Wortspiele, kleine Grenztests, "
            "bei denen Rückzug akzeptiert wird. Kein Drängen, keine Pseudo-Intimität."
beispiele:
  - "import yaml"
  - "# Wir definieren die Marker erneut nach dem Kernel-Reset"
  - "new_markers = ["
  - "{"
  - ""marker": "FRIENDLY_FLIRTING_MARKER","
  - "),"
  - ""beispiele": ["
  - ""Na, du bist aber heute charmant drauf.","
  - ""Haha, das war jetzt aber ein Kompliment, oder?","
  - ""Du bist echt lustig, das mag ich.","
  - ""Du scheinst ein cooler Mensch zu sein.","
  - ""So viel Humor am Morgen? Respekt.","
  - ""Sorry, falls das zu direkt war – wollte nur ein Kompliment machen.""
  - "],"
  - ""kategorie": "SOZIALE_INTERAKTION","
  - ""tags": ["flirt", "freundlich", "humor", "abgrenzung"],"
  - ""risk_score": 1"
  - "},"
  - "{"
  - ""marker": "OFFENSIVE_FLIRTING_MARKER","
  - ""beschreibung": ("
  - ""Erkennt kommunikative Muster von offensivem, teils grenzüberschreitendem Flirten. ""
  - ""Typisch sind direkte Anspielungen auf Aussehen, Sexualität, oder das schnelle Andeuten ""
  - ""einer (körperlichen) Beziehung. Oft gepaart mit Pacing, Übertreibung und ""
  - ""nachgeschobenen Rückziehern („war nur Spaß“).""
  - "),"
  - ""beispiele": ["
  - ""Mit dir würde ich sofort ans Meer fahren, ganz ehrlich.","
  - ""Wenn ich dich jetzt neben mir hätte, wäre es sicher nicht langweilig.","
  - ""Bist du eigentlich immer so sexy, oder nur im Chat?","
  - ""Sorry, das war vielleicht ein bisschen zu viel. Aber du bist wirklich heiß.","
  - ""Wollen wir nicht einfach durchbrennen?","
  - ""Das klingt jetzt vielleicht verrückt, aber du machst mich echt an.""
  - "],"
  - ""kategorie": "SOZIALE_INTERAKTION","
  - ""tags": ["flirt", "offensiv", "grenztest", "sexualisierung"],"
  - ""risk_score": 2"
  - "},"
  - "{"
  - ""marker": "DEEPENING_BY_QUESTIONING_ADVANCED_MARKER","
  - ""beschreibung": ("
  - ""Erkennt eine dialogische Strategie, in der durch wiederholt gezielte, ""
  - ""oft introspektive oder persönliche Fragen immer tiefere Informationen ""
  - ""über das Gegenüber gewonnen werden. Das Ziel ist nicht nur ""
  - ""authentische Nähe, sondern der systematische Aufbau eines Persönlichkeitsprofils ""
  - ""(Mapping). Die Fragen wirken empathisch, dienen aber im Kontext der Gesprächsführung ""
  - ""der Sammlung maximaler Resonanzdaten (Social Engineering).""
  - "),"
  - ""beispiele": ["
  - ""Was war dein prägendstes Erlebnis in den letzten Jahren?","
  - ""Wovor hast du im Leben am meisten Angst?","
  - ""Wie würdest du dich mit drei Worten beschreiben?","
  - ""Was war deine größte Enttäuschung in Beziehungen?","
  - ""Was gibt dir am meisten Kraft, wenn es dir schlecht geht?","
  - ""Welche Eigenschaft schätzt du an anderen am meisten?","
  - ""Was hast du noch niemandem erzählt?","
  - ""Was suchst du wirklich in einer Partnerschaft?""
  - "],"
  - ""kategorie": "MANIPULATION","
  - ""tags": ["questioning", "data_mining", "mapping", "tiefe"],"
  - ""risk_score": 3"
  - "},"
  - "{"
  - ""marker": "RELATIONSHIP_AUTHENTICITY_MARKER","
  - ""beschreibung": ("
  - ""Erkennt, ob im Gesprächsverlauf Hinweise darauf bestehen, dass sich die Gesprächspartner ""
  - ""bereits kennen (z. B. durch geteilte Erinnerungen, Insider, konkrete Bezugnahmen auf reale, ""
  - ""gemeinsam erlebte Situationen, Verwandte, Orte). Fehlen diese Ankerpunkte trotz hoher ""
  - ""emotionaler Resonanz, ist dies ein Risiko-Indikator für Scripting/Fake-Konstellation.""
  - "),"
  - ""beispiele": ["
  - ""Wann haben wir uns das letzte Mal gesehen?","
  - ""Wie war das noch mal damals auf der Party?","
  - ""Grüß deine Schwester von mir!","
  - ""Weißt du noch, wie wir zusammen im Urlaub waren?","
  - ""Damals in der WG-Küche ...","
  - ""Weißt du eigentlich noch, wo das Restaurant war, in dem wir letztes Mal waren?""
  - "],"
  - ""kategorie": "AUTHENTIZITÄT","
  - ""tags": ["realitätsbezug", "insider", "ankergeschichten"],"
  - ""risk_score": 2"
  - "},"
  - "{"
  - ""marker": "RESONANZ_MATCHING_MARKER","
  - ""beschreibung": ("
  - ""Erkennt eine auffallend hohe Übereinstimmung oder perfekte Resonanz zwischen ""
  - ""den Äußerungen beider Gesprächspartner – insbesondere dann, wenn kein ""
  - ""gemeinsamer Erfahrungshintergrund sichtbar wird. Typisch sind nahtlose ""
  - ""Spiegelungen von Werten, Einstellungen, Sprachstil und Interessen, die ""
  - ""statistisch unplausibel erscheinen. Dies ist ein Indikator für KI-gestützte ""
  - ""Gesprächsoptimierung oder gezielte manipulative Strategie.""
  - "),"
  - ""beispiele": ["
  - ""Das sehe ich ganz genauso, das ist auch mein Lebensmotto.","
  - ""Du sprichst mir aus der Seele, ich habe exakt das gleiche erlebt.","
  - ""Genau so denke ich auch – es ist, als würdest du meine Gedanken lesen.","
  - ""Wahnsinn, wir ticken ja wirklich identisch!","
  - ""Das ist verrückt – wir teilen genau die gleichen Werte.","
  - ""Es ist, als hätten wir uns schon ewig gekannt.""
  - "],"
  - ""kategorie": "MANIPULATION","
  - ""tags": ["resonanz", "spiegelung", "matching", "synchronizität"],"
  - ""risk_score": 3"
  - "}"
  - "]"
  - "# Speicherpfad definieren"
  - "markers_path = "/mnt/data/ADVANCED_CHAT_MARKERS.yaml""
  - "# Datei speichern"
  - "with open(markers_path, "w", encoding="utf-8") as f:"
  - "yaml.dump(new_markers, f, allow_unicode=True)"
  - "markers_path"

semantic_grab:
  description: "Erkennt Muster für social_interaction.py"
  patterns:
    - rule: "AUTO_PATTERN"
      pattern: r"(muster.*wird.*ergänzt)"
tags: [neu_erstellt, needs_review]


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/SOCIAL_INTERACTION.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/SOCIAL_RESONANCE_DETECTOR.py
# ====================================================================

marker: SOCIAL_RESONANCE_DETECTOR.py_MARKER
beschreibung: >
  Semantic greb pattern
beispiele:
  - "SEMANTIC_GRAB_PATTERNS = {"
  - ""FRIENDLY_FLIRTING_MARKER": FRIENDLY_FLIRTING_GRAB,"
  - ""OFFENSIVE_FLIRTING_MARKER": OFFENSIVE_FLIRTING_GRAB,"
  - ""DEEPENING_BY_QUESTIONING_ADVANCED_MARKER": DEEPENING_BY_QUESTIONING_ADVANCED_GRAB,"
  - ""RELATIONSHIP_AUTHENTICITY_MARKER": RELATIONSHIP_AUTHENTICITY_GRAB,"
  - ""RESONANZ_MATCHING_MARKER": RESONANZ_MATCHING_GRAB,"
  - "}"

semantic_grab:
  description: "Erkennt Muster für social_resonance_detector.py"
  patterns:
    - rule: "AUTO_PATTERN"
      pattern: r"(muster.*wird.*ergänzt)"
tags: [neu_erstellt, needs_review]


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/SOCIAL_RESONANCE_DETECTOR.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/SYMBOLIC_ROLE_SWAP.py
# ====================================================================

import re

# Hinweis: Für sehr gute Erkennung ggf. Named Entity Recognition/Spacy etc. nutzen.
# Hier: Typische Muster für übertragene Objekte/Projektionen.
SYMBOLIC_ROLE_SWAP_PATTERNS = [
    r"(pflanze|haustier|hund|katze|fisch|socke|tasse|baum|navi|auto|akku|rad|gerät|kopfhörer|blume|radio|projekt|tür|fenster|wlan|handy|gerät|dokument|puzzleteil|ast|fahrrad|bank|sessel)",
    r"(funktioniert nicht|springt nicht an|bleibt stehen|eiert|schwimmt im kreis|geht kaputt|quietscht|reagiert nicht|leiser|verliert die balance|scheitert|einsam|geht leer aus|wird nie voll|ist nie synchron)",
    r"(als ob|wie wenn|genau wie|typisch für|kommt mir vor wie|es ist wie|fühlt sich an wie)"
]

def detect_symbolic_role_swap(text):
    for pattern in SYMBOLIC_ROLE_SWAP_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/SYMBOLIC_ROLE_SWAP.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/aggregation_models.py
# ====================================================================

"""Datenmodelle für die Zeitreihen-Aggregation."""

from typing import List, Dict, Optional, Any, Union
from datetime import datetime, timedelta
from pydantic import BaseModel, Field, validator
from enum import Enum

import pandas as pd


class AggregationPeriod(str, Enum):
    """Verfügbare Aggregations-Zeiträume."""
    
    HOURLY = "hourly"
    DAILY = "daily"
    WEEKLY = "weekly"
    MONTHLY = "monthly"
    QUARTERLY = "quarterly"
    YEARLY = "yearly"
    CUSTOM = "custom"


class TimeSeriesPoint(BaseModel):
    """Ein einzelner Punkt in einer Zeitreihe."""
    
    timestamp: datetime = Field(..., description="Zeitpunkt")
    period_start: datetime = Field(..., description="Start des Aggregationszeitraums")
    period_end: datetime = Field(..., description="Ende des Aggregationszeitraums")
    
    values: Dict[str, float] = Field(
        default_factory=dict,
        description="Werte für verschiedene Metriken"
    )
    
    counts: Dict[str, int] = Field(
        default_factory=dict,
        description="Anzahlen (z.B. Marker-Counts)"
    )
    
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="Zusätzliche Metadaten"
    )
    
    @validator('period_end')
    def validate_period(cls, v, values):
        """Stelle sicher, dass period_end nach period_start liegt."""
        if 'period_start' in values and v <= values['period_start']:
            raise ValueError('period_end muss nach period_start liegen')
        return v


class TimeSeriesData(BaseModel):
    """Container für Zeitreihen-Daten."""
    
    series_id: str = Field(..., description="Eindeutige ID der Zeitreihe")
    name: str = Field(..., description="Name der Zeitreihe")
    metric_type: str = Field(..., description="Art der Metrik (score, count, etc.)")
    
    period: AggregationPeriod = Field(..., description="Aggregationszeitraum")
    
    data_points: List[TimeSeriesPoint] = Field(
        default_factory=list,
        description="Die eigentlichen Datenpunkte"
    )
    
    statistics: Dict[str, Any] = Field(
        default_factory=dict,
        description="Statistische Zusammenfassung"
    )
    
    def to_dataframe(self) -> pd.DataFrame:
        """Konvertiert die Zeitreihe in einen Pandas DataFrame."""
        if not self.data_points:
            return pd.DataFrame()
        
        records = []
        for point in self.data_points:
            record = {
                'timestamp': point.timestamp,
                'period_start': point.period_start,
                'period_end': point.period_end
            }
            record.update(point.values)
            record.update({f"{k}_count": v for k, v in point.counts.items()})
            records.append(record)
        
        df = pd.DataFrame(records)
        df.set_index('timestamp', inplace=True)
        return df
    
    def get_trend(self, metric: str) -> Optional[str]:
        """Berechnet den Trend für eine bestimmte Metrik."""
        if len(self.data_points) < 2:
            return None
        
        values = [p.values.get(metric, 0) for p in self.data_points]
        if not values:
            return None
        
        # Einfache Trendberechnung
        first_half = sum(values[:len(values)//2]) / (len(values)//2)
        second_half = sum(values[len(values)//2:]) / (len(values) - len(values)//2)
        
        if second_half > first_half * 1.1:
            return "increasing"
        elif second_half < first_half * 0.9:
            return "decreasing"
        else:
            return "stable"


class AggregationConfig(BaseModel):
    """Konfiguration für die Zeitreihen-Aggregation."""
    
    period: AggregationPeriod = Field(
        default=AggregationPeriod.DAILY,
        description="Standard-Aggregationszeitraum"
    )
    
    custom_period_hours: Optional[int] = Field(
        None,
        description="Stunden für custom period"
    )
    
    metrics_to_aggregate: List[str] = Field(
        default_factory=lambda: [
            "manipulation_index",
            "relationship_health",
            "fraud_probability",
            "marker_count"
        ],
        description="Zu aggregierende Metriken"
    )
    
    aggregation_functions: Dict[str, List[str]] = Field(
        default_factory=lambda: {
            "default": ["mean", "min", "max", "sum", "count"],
            "scores": ["mean", "min", "max", "std"],
            "counts": ["sum", "mean"]
        },
        description="Aggregationsfunktionen pro Metrik-Typ"
    )
    
    include_zero_periods: bool = Field(
        default=True,
        description="Ob Zeiträume ohne Daten inkludiert werden sollen"
    )
    
    smooth_data: bool = Field(
        default=False,
        description="Ob Daten geglättet werden sollen"
    )
    
    smoothing_window: int = Field(
        default=3,
        description="Fenster für Glättung"
    )


class HeatmapData(BaseModel):
    """Daten für Heatmap-Visualisierung."""
    
    title: str = Field(..., description="Titel der Heatmap")
    
    x_labels: List[str] = Field(
        ...,
        description="X-Achsen-Labels (z.B. Zeiträume)"
    )
    
    y_labels: List[str] = Field(
        ...,
        description="Y-Achsen-Labels (z.B. Marker-Kategorien)"
    )
    
    values: List[List[float]] = Field(
        ...,
        description="2D-Array der Werte"
    )
    
    color_scale: str = Field(
        default="RdYlGn",
        description="Farbskala für Heatmap"
    )
    
    annotations: Optional[List[List[str]]] = Field(
        None,
        description="Text-Annotationen für Zellen"
    )
    
    metadata: Dict[str, Any] = Field(default_factory=dict)


class ComparisonData(BaseModel):
    """Daten für Zeitreihen-Vergleiche."""
    
    comparison_type: str = Field(
        ...,
        description="Art des Vergleichs (period, speaker, category)"
    )
    
    series: List[TimeSeriesData] = Field(
        ...,
        description="Zu vergleichende Zeitreihen"
    )
    
    baseline_index: Optional[int] = Field(
        None,
        description="Index der Baseline-Serie"
    )
    
    differences: Optional[Dict[str, List[float]]] = Field(
        None,
        description="Berechnete Differenzen"
    )
    
    correlation_matrix: Optional[List[List[float]]] = Field(
        None,
        description="Korrelationsmatrix zwischen Serien"
    )


class AggregationResult(BaseModel):
    """Gesamtergebnis der Zeitreihen-Aggregation."""
    
    time_series: Dict[str, TimeSeriesData] = Field(
        default_factory=dict,
        description="Aggregierte Zeitreihen pro Metrik"
    )
    
    heatmaps: Dict[str, HeatmapData] = Field(
        default_factory=dict,
        description="Generierte Heatmap-Daten"
    )
    
    comparisons: List[ComparisonData] = Field(
        default_factory=list,
        description="Zeitreihen-Vergleiche"
    )
    
    summary_statistics: Dict[str, Dict[str, float]] = Field(
        default_factory=dict,
        description="Zusammenfassende Statistiken"
    )
    
    export_data: Optional[pd.DataFrame] = Field(
        None,
        description="Exportierbare Daten als DataFrame"
    )
    
    processing_time: float = Field(0.0)
    
    class Config:
        """Pydantic Konfiguration."""
        arbitrary_types_allowed = True  # Für pandas DataFrame
    
    def to_csv(self, filepath: str):
        """Exportiert die Daten als CSV."""
        if self.export_data is not None:
            self.export_data.to_csv(filepath)
        else:
            # Erstelle DataFrame aus time_series
            dfs = []
            for metric, series in self.time_series.items():
                df = series.to_dataframe()
                df.columns = [f"{metric}_{col}" if col not in ['period_start', 'period_end'] 
                             else col for col in df.columns]
                dfs.append(df)
            
            if dfs:
                combined_df = pd.concat(dfs, axis=1)
                combined_df.to_csv(filepath)
    
    def to_json(self, filepath: str):
        """Exportiert die Daten als JSON."""
        import json
        
        # Konvertiere zu serialisierbarem Format
        data = {
            'time_series': {
                name: {
                    'series_id': ts.series_id,
                    'name': ts.name,
                    'metric_type': ts.metric_type,
                    'period': ts.period.value,
                    'data_points': [
                        {
                            'timestamp': point.timestamp.isoformat(),
                            'period_start': point.period_start.isoformat(),
                            'period_end': point.period_end.isoformat(),
                            'values': point.values,
                            'counts': point.counts
                        }
                        for point in ts.data_points
                    ],
                    'statistics': ts.statistics
                }
                for name, ts in self.time_series.items()
            },
            'summary_statistics': self.summary_statistics,
            'processing_time': self.processing_time
        }
        
        with open(filepath, 'w') as f:
            json.dump(data, f, indent=2)

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/aggregation_models.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/config_loader.py
# ====================================================================

"""Konfigurationslader für Marker-Definitionen aus verschiedenen Formaten."""

import json
import yaml
import re
from pathlib import Path
from typing import List, Dict, Any, Union, Optional
import logging
from dataclasses import dataclass

from ..matcher.marker_models import MarkerDefinition, MarkerPattern, MarkerCategory, MarkerSeverity


logger = logging.getLogger(__name__)


@dataclass
class MarkerConfig:
    """Globale Konfiguration für das Marker-System."""
    
    marker_directories: List[Path] = None
    auto_reload: bool = True
    cache_enabled: bool = True
    fuzzy_matching_default: bool = True
    default_context_words: int = 10
    
    def __post_init__(self):
        if self.marker_directories is None:
            self.marker_directories = [Path("markers")]


class MarkerLoader:
    """Lädt und verwaltet Marker-Definitionen aus verschiedenen Quellen."""
    
    def __init__(self, config: Optional[MarkerConfig] = None):
        self.config = config or MarkerConfig()
        self._markers: Dict[str, MarkerDefinition] = {}
        self._file_cache: Dict[str, float] = {}  # Datei -> Zeitstempel
        
    def load_all_markers(self) -> Dict[str, MarkerDefinition]:
        """Lädt alle Marker aus den konfigurierten Verzeichnissen."""
        self._markers.clear()
        
        for directory in self.config.marker_directories:
            if not directory.exists():
                logger.warning(f"Marker-Verzeichnis existiert nicht: {directory}")
                continue
                
            # YAML-Dateien
            for yaml_file in directory.glob("*.yaml"):
                try:
                    markers = self.load_yaml_markers(yaml_file)
                    self._merge_markers(markers)
                except Exception as e:
                    logger.error(f"Fehler beim Laden von {yaml_file}: {e}")
            
            # JSON-Dateien
            for json_file in directory.glob("*.json"):
                try:
                    markers = self.load_json_markers(json_file)
                    self._merge_markers(markers)
                except Exception as e:
                    logger.error(f"Fehler beim Laden von {json_file}: {e}")
            
            # TXT-Dateien (einfaches Format)
            for txt_file in directory.glob("*.txt"):
                try:
                    markers = self.load_txt_markers(txt_file)
                    self._merge_markers(markers)
                except Exception as e:
                    logger.error(f"Fehler beim Laden von {txt_file}: {e}")
        
        logger.info(f"Gesamt {len(self._markers)} Marker geladen")
        return self._markers
    
    def load_yaml_markers(self, filepath: Path) -> List[MarkerDefinition]:
        """Lädt Marker aus einer YAML-Datei."""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        
        markers = []
        if isinstance(data, dict) and 'markers' in data:
            marker_list = data['markers']
        elif isinstance(data, list):
            marker_list = data
        else:
            raise ValueError(f"Unerwartetes YAML-Format in {filepath}")
        
        for marker_data in marker_list:
            marker = self._parse_marker_data(marker_data)
            if marker:
                markers.append(marker)
        
        logger.info(f"Geladen: {len(markers)} Marker aus {filepath}")
        return markers
    
    def load_json_markers(self, filepath: Path) -> List[MarkerDefinition]:
        """Lädt Marker aus einer JSON-Datei."""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        markers = []
        if isinstance(data, dict) and 'markers' in data:
            marker_list = data['markers']
        elif isinstance(data, list):
            marker_list = data
        else:
            raise ValueError(f"Unerwartetes JSON-Format in {filepath}")
        
        for marker_data in marker_list:
            marker = self._parse_marker_data(marker_data)
            if marker:
                markers.append(marker)
        
        logger.info(f"Geladen: {len(markers)} Marker aus {filepath}")
        return markers
    
    def load_txt_markers(self, filepath: Path) -> List[MarkerDefinition]:
        """Lädt Marker aus einer einfachen TXT-Datei.
        
        Format:
        # Kommentar
        MARKER_ID | Kategorie | Name | Pattern1, Pattern2, ...
        """
        markers = []
        
        with open(filepath, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                
                # Überspringe leere Zeilen und Kommentare
                if not line or line.startswith('#'):
                    continue
                
                parts = [p.strip() for p in line.split('|')]
                if len(parts) < 4:
                    logger.warning(f"Zeile {line_num} in {filepath} hat ungültiges Format")
                    continue
                
                marker_id, category, name, patterns = parts[0], parts[1], parts[2], parts[3]
                
                # Parse patterns
                pattern_list = [p.strip() for p in patterns.split(',')]
                
                try:
                    marker = MarkerDefinition(
                        id=marker_id,
                        name=name,
                        category=self._parse_category(category),
                        description=f"Marker aus {filepath.name}",
                        keywords=pattern_list,
                        weight=1.0
                    )
                    markers.append(marker)
                except Exception as e:
                    logger.error(f"Fehler beim Parsen von Zeile {line_num}: {e}")
        
        logger.info(f"Geladen: {len(markers)} Marker aus {filepath}")
        return markers
    
    def _parse_marker_data(self, data: Dict[str, Any]) -> Optional[MarkerDefinition]:
        """Parst Marker-Daten aus einem Dictionary."""
        try:
            # Basis-Felder
            marker_id = data.get('id', data.get('marker_id'))
            if not marker_id:
                logger.warning("Marker ohne ID gefunden, überspringe")
                return None
            
            # Patterns verarbeiten
            patterns = []
            if 'patterns' in data:
                for p in data['patterns']:
                    if isinstance(p, str):
                        patterns.append(MarkerPattern(pattern=p))
                    elif isinstance(p, dict):
                        patterns.append(MarkerPattern(**p))
            
            # Keywords
            keywords = data.get('keywords', [])
            if isinstance(keywords, str):
                keywords = [keywords]
            
            # Kategorie parsen
            category = self._parse_category(data.get('category', 'manipulation'))
            
            # Severity parsen
            severity = self._parse_severity(data.get('severity', 'medium'))
            
            marker = MarkerDefinition(
                id=marker_id,
                name=data.get('name', marker_id),
                category=category,
                severity=severity,
                description=data.get('description', ''),
                patterns=patterns,
                keywords=keywords,
                examples=data.get('examples', []),
                weight=float(data.get('weight', 1.0)),
                metadata=data.get('metadata', {}),
                active=data.get('active', True)
            )
            
            return marker
            
        except Exception as e:
            logger.error(f"Fehler beim Parsen von Marker-Daten: {e}")
            return None
    
    def _parse_category(self, category_str: str) -> MarkerCategory:
        """Parst eine Kategorie-String zu MarkerCategory Enum."""
        category_str = category_str.lower().strip()
        
        # Mapping für alternative Namen
        category_map = {
            'scam': MarkerCategory.FRAUD,
            'romance_scam': MarkerCategory.FRAUD,
            'betrug': MarkerCategory.FRAUD,
            'manipulativ': MarkerCategory.MANIPULATION,
            'abuse': MarkerCategory.EMOTIONAL_ABUSE,
            'positiv': MarkerCategory.POSITIVE,
            'wertschätzung': MarkerCategory.POSITIVE,
            'unterstützung': MarkerCategory.SUPPORT,
        }
        
        # Erst im Mapping suchen
        if category_str in category_map:
            return category_map[category_str]
        
        # Dann versuchen direkt zu parsen
        try:
            return MarkerCategory(category_str)
        except ValueError:
            logger.warning(f"Unbekannte Kategorie '{category_str}', nutze MANIPULATION")
            return MarkerCategory.MANIPULATION
    
    def _parse_severity(self, severity_str: str) -> MarkerSeverity:
        """Parst eine Severity-String zu MarkerSeverity Enum."""
        severity_str = severity_str.lower().strip()
        
        severity_map = {
            'niedrig': MarkerSeverity.LOW,
            'gering': MarkerSeverity.LOW,
            'mittel': MarkerSeverity.MEDIUM,
            'hoch': MarkerSeverity.HIGH,
            'kritisch': MarkerSeverity.CRITICAL,
            'schwer': MarkerSeverity.HIGH,
        }
        
        if severity_str in severity_map:
            return severity_map[severity_str]
        
        try:
            return MarkerSeverity(severity_str)
        except ValueError:
            logger.warning(f"Unbekannte Severity '{severity_str}', nutze MEDIUM")
            return MarkerSeverity.MEDIUM
    
    def _merge_markers(self, new_markers: List[MarkerDefinition]):
        """Fügt neue Marker zum bestehenden Set hinzu."""
        for marker in new_markers:
            if marker.id in self._markers:
                logger.warning(f"Marker {marker.id} existiert bereits, wird überschrieben")
            self._markers[marker.id] = marker
    
    def get_marker(self, marker_id: str) -> Optional[MarkerDefinition]:
        """Gibt einen spezifischen Marker zurück."""
        return self._markers.get(marker_id)
    
    def get_markers_by_category(self, category: MarkerCategory) -> List[MarkerDefinition]:
        """Gibt alle Marker einer Kategorie zurück."""
        return [m for m in self._markers.values() if m.category == category]
    
    def get_active_markers(self) -> List[MarkerDefinition]:
        """Gibt nur aktive Marker zurück."""
        return [m for m in self._markers.values() if m.active]
    
    def reload_if_changed(self) -> bool:
        """Lädt Marker neu, wenn sich Dateien geändert haben."""
        if not self.config.auto_reload:
            return False
        
        changed = False
        for directory in self.config.marker_directories:
            if not directory.exists():
                continue
            
            for file in directory.glob("*.{yaml,json,txt}"):
                mtime = file.stat().st_mtime
                if file.as_posix() not in self._file_cache or \
                   self._file_cache[file.as_posix()] < mtime:
                    changed = True
                    break
        
        if changed:
            logger.info("Änderungen erkannt, lade Marker neu")
            self.load_all_markers()
            
        return changed

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/config_loader.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/create_marker_master.py
# ====================================================================

#!/usr/bin/env python3
"""
Marker Master Export Generator
Konsolidiert alle Marker aus verschiedenen Quellen in eine Master-YAML-Datei
"""

import os
import yaml
import json
import re
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging

# Logging konfigurieren
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MarkerCollector:
    """Sammelt und konsolidiert Marker aus verschiedenen Quellen"""
    
    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        self.markers = {}
        self.semantic_detectors = {}
        self.duplicate_count = 0
        
    def collect_all_markers(self) -> Dict[str, Any]:
        """Hauptmethode: Sammelt alle Marker aus verschiedenen Quellen"""
        logger.info("Starte Marker-Sammlung...")
        
        # Definiere die zu durchsuchenden Ordner
        search_dirs = [
            "Assist_TXT_marker_py:/ALL_NEWMARKER01",
            "Assist_TXT_marker_py: 2/ALL_NEWMARKER01",
            "Assist_TXT_marker_py: 2/MARKERBOOK_YAML_CANVAS",
            "Assist_TXT_marker_py: 2/tension",
            "Assist_TXT_marker_py: 2/resonance",
            "Assist_TXT_marker_py: 3/ALL_NEWMARKER01",
            "Assist_TXT_marker_py: 3/MARKERBOOK_YAML_CANVAS",
            "Assist_TXT_marker_py: 3/tension",
            "Assist_TXT_marker_py: 3/resonance",
            "Assist_YAML_marker_py: 4",
            "Assist_YAML_marker_py: 4/MARKERBOOK_YAML_CANVAS",
            "Assist_YAML_marker_py: 4/tension",
            "Assist_YAML_marker_py: 4/resonance",
        ]
        
        # Sammle Marker aus jedem Verzeichnis
        for dir_path in search_dirs:
            full_path = self.base_path / dir_path
            if full_path.exists():
                logger.info(f"Durchsuche: {dir_path}")
                self._process_directory(full_path)
            else:
                logger.warning(f"Verzeichnis nicht gefunden: {dir_path}")
        
        # Sammle semantische Detektoren
        self._collect_semantic_detectors()
        
        # Verknüpfe Marker mit Detektoren
        self._link_detectors_to_markers()
        
        logger.info(f"Sammlung abgeschlossen. {len(self.markers)} eindeutige Marker gefunden.")
        logger.info(f"{self.duplicate_count} Duplikate übersprungen.")
        
        return self.markers 

    def _process_directory(self, directory: Path):
        """Verarbeitet alle Marker-Dateien in einem Verzeichnis"""
        for file_path in directory.iterdir():
            if file_path.is_file() and self._is_marker_file(file_path):
                try:
                    self._process_file(file_path)
                except Exception as e:
                    logger.error(f"Fehler beim Verarbeiten von {file_path}: {e}")
    
    def _is_marker_file(self, file_path: Path) -> bool:
        """Prüft, ob eine Datei eine Marker-Datei ist"""
        name = file_path.name.lower()
        # Überspringe Backup-Dateien und System-Dateien
        if any(skip in name for skip in ['.backup', '.ds_store', '__pycache__']):
            return False
        # Akzeptiere Marker-Dateien
        return any(marker_id in name for marker_id in ['marker', 'knot', 'spiral', 'pattern'])
    
    def _process_file(self, file_path: Path):
        """Verarbeitet eine einzelne Marker-Datei"""
        logger.debug(f"Verarbeite: {file_path.name}")
        
        if file_path.suffix == '.yaml' or file_path.suffix == '.yml':
            self._process_yaml_file(file_path)
        elif file_path.suffix == '.txt':
            self._process_txt_file(file_path)
        elif file_path.suffix == '.json':
            self._process_json_file(file_path)
    
    def _process_yaml_file(self, file_path: Path):
        """Verarbeitet YAML-Dateien"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                
            if isinstance(data, dict):
                # Prüfe ob es ein einzelner Marker ist
                if 'marker' in data:
                    self._add_marker(data)
                # Oder eine Liste von Markern
                elif any(key.lower().endswith('marker') for key in data.keys()):
                    for key, value in data.items():
                        if isinstance(value, list):
                            for item in value:
                                self._add_marker_from_list_format(key, item)
        except Exception as e:
            logger.error(f"Fehler beim Parsen von YAML {file_path}: {e}")
    
    def _process_txt_file(self, file_path: Path):
        """Verarbeitet TXT-Dateien mit Marker-Definitionen"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Versuche strukturierte Marker zu extrahieren
            marker_data = self._extract_marker_from_txt(content, file_path.stem)
            if marker_data:
                self._add_marker(marker_data)
        except Exception as e:
            logger.error(f"Fehler beim Lesen von {file_path}: {e}")
    
    def _process_json_file(self, file_path: Path):
        """Verarbeitet JSON-Dateien"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, dict):
                if 'marker' in data:
                    self._add_marker(data)
                elif isinstance(data, list):
                    for item in data:
                        if isinstance(item, dict) and 'marker' in item:
                            self._add_marker(item)
        except Exception as e:
            logger.error(f"Fehler beim Parsen von JSON {file_path}: {e}")

    def _extract_marker_from_txt(self, content: str, file_stem: str) -> Optional[Dict[str, Any]]:
        """Extrahiert Marker-Daten aus TXT-Dateien"""
        marker_data = {}
        
        # Versuche Marker-Namen zu extrahieren
        marker_match = re.search(r'marker:\s*(\w+)', content, re.IGNORECASE)
        if marker_match:
            marker_data['marker'] = marker_match.group(1)
        else:
            # Verwende Dateinamen als Marker-Namen
            marker_name = file_stem.replace('_MARKER', '').replace('.txt', '')
            marker_data['marker'] = marker_name
        
        # Extrahiere Beschreibung
        desc_match = re.search(r'beschreibung:\s*(.+?)(?=\n\w+:|$)', content, re.IGNORECASE | re.DOTALL)
        if desc_match:
            marker_data['beschreibung'] = desc_match.group(1).strip()
        
        # Extrahiere Beispiele
        beispiele = []
        beispiel_section = re.search(r'beispiele:(.*?)(?=\n\w+:|$)', content, re.IGNORECASE | re.DOTALL)
        if beispiel_section:
            beispiel_text = beispiel_section.group(1)
            # Finde alle Zeilen mit - am Anfang
            beispiele = re.findall(r'-\s*"([^"]+)"', beispiel_text)
            if not beispiele:
                # Alternative: Zeilen mit Aufzählungszeichen
                beispiele = re.findall(r'-\s*(.+)', beispiel_text)
        
        if beispiele:
            marker_data['beispiele'] = [b.strip() for b in beispiele]
        
        # Extrahiere Tags
        tags_match = re.search(r'tags:\s*\[(.*?)\]', content, re.IGNORECASE)
        if tags_match:
            tags = [t.strip() for t in tags_match.group(1).split(',')]
            marker_data['tags'] = tags
        
        return marker_data if 'marker' in marker_data else None
    
    def _add_marker(self, marker_data: Dict[str, Any]):
        """Fügt einen Marker zur Sammlung hinzu"""
        marker_name = marker_data.get('marker', '').upper()
        
        if not marker_name:
            return
        
        # Prüfe auf Duplikate
        if marker_name in self.markers:
            self.duplicate_count += 1
            # Merge Daten wenn nötig
            existing = self.markers[marker_name]
            # Füge neue Beispiele hinzu
            if 'beispiele' in marker_data:
                existing_examples = set(existing.get('beispiele', []))
                new_examples = set(marker_data.get('beispiele', []))
                existing['beispiele'] = list(existing_examples | new_examples)
            # Merge Tags
            if 'tags' in marker_data:
                existing_tags = set(existing.get('tags', []))
                new_tags = set(marker_data.get('tags', []))
                existing['tags'] = list(existing_tags | new_tags)
        else:
            # Normalisiere Marker-Struktur
            normalized = {
                'marker': marker_name,
                'beschreibung': marker_data.get('beschreibung', ''),
                'beispiele': marker_data.get('beispiele', []),
                'tags': marker_data.get('tags', []),
                'kategorie': marker_data.get('kategorie', 'UNCATEGORIZED'),
                'psychologischer_hintergrund': marker_data.get('psychologischer_hintergrund', ''),
                'dynamik_absicht': marker_data.get('dynamik_absicht', ''),
                'szenarien': marker_data.get('szenarien', []),
                'risk_score': marker_data.get('risk_score', 1)
            }
            
            # Füge semantische Detektor-Info hinzu, falls vorhanden
            if 'semantic_grab' in marker_data:
                normalized['semantic_patterns'] = marker_data['semantic_grab']
            
            self.markers[marker_name] = normalized
    
    def _add_marker_from_list_format(self, key: str, item: Dict[str, Any]):
        """Fügt Marker aus Listen-Format hinzu (z.B. Ambivalenzmarker)"""
        if isinstance(item, dict) and 'input' in item:
            marker_name = key.upper()
            if marker_name not in self.markers:
                self.markers[marker_name] = {
                    'marker': marker_name,
                    'beschreibung': f'Automatisch generiert aus {key}',
                    'beispiele': [],
                    'tags': [key],
                    'kategorie': 'PATTERN',
                    'risk_score': 1
                }
            
            # Füge Beispiel hinzu
            if 'input' in item:
                self.markers[marker_name]['beispiele'].append(item['input'])
    
    def _collect_semantic_detectors(self):
        """Sammelt alle Python-Detektoren"""
        detector_dirs = [
            "Assist_TXT_marker_py: 2/SEMANTIC_DETECTORS_PYTHO",
            "Assist_TXT_marker_py: 3/SEMANTIC_DETECTORS_PYTHO"
        ]
        
        for dir_path in detector_dirs:
            full_path = self.base_path / dir_path
            if full_path.exists():
                for file_path in full_path.glob("*.py"):
                    if file_path.stem not in ['__init__', 'setup_py']:
                        self.semantic_detectors[file_path.stem.upper()] = file_path.name
    
    def _link_detectors_to_markers(self):
        """Verknüpft Detektoren mit passenden Markern"""
        for marker_name, marker_data in self.markers.items():
            # Suche nach passendem Detektor
            for detector_name, detector_file in self.semantic_detectors.items():
                if detector_name in marker_name or marker_name in detector_name:
                    marker_data['semantics_detector'] = detector_file
                    break 

    def export_to_yaml(self, output_file: str = "marker_master_export.yaml"):
        """Exportiert alle Marker in eine YAML-Datei"""
        # Konvertiere Marker-Dictionary in Liste für bessere Lesbarkeit
        marker_list = list(self.markers.values())
        
        # Sortiere nach Marker-Namen
        marker_list.sort(key=lambda x: x['marker'])
        
        # Füge Metadaten hinzu
        export_data = {
            'version': '2.0',
            'generated_at': datetime.now().isoformat(),
            'total_markers': len(marker_list),
            'categories': self._get_categories(),
            'risk_levels': {
                'green': 'Kein oder nur unkritischer Marker',
                'yellow': '1-2 moderate Marker, erste Drift erkennbar',
                'blinking': '3+ Marker oder ein Hochrisiko-Marker, klare Drift/Manipulation',
                'red': 'Hochrisiko-Kombination, massive Drift/Manipulation'
            },
            'markers': marker_list
        }
        
        # Schreibe YAML-Datei
        with open(output_file, 'w', encoding='utf-8') as f:
            yaml.dump(export_data, f, 
                     default_flow_style=False, 
                     allow_unicode=True,
                     sort_keys=False,
                     width=120)
        
        logger.info(f"YAML-Export erstellt: {output_file}")
        return output_file
    
    def export_to_json(self, output_file: str = "marker_master_export.json"):
        """Exportiert alle Marker in eine JSON-Datei"""
        # Konvertiere Marker-Dictionary in Liste
        marker_list = list(self.markers.values())
        
        # Sortiere nach Marker-Namen
        marker_list.sort(key=lambda x: x['marker'])
        
        # Füge Metadaten hinzu
        export_data = {
            'version': '2.0',
            'generated_at': datetime.now().isoformat(),
            'total_markers': len(marker_list),
            'categories': self._get_categories(),
            'risk_levels': {
                'green': 'Kein oder nur unkritischer Marker',
                'yellow': '1-2 moderate Marker, erste Drift erkennbar',
                'blinking': '3+ Marker oder ein Hochrisiko-Marker, klare Drift/Manipulation',
                'red': 'Hochrisiko-Kombination, massive Drift/Manipulation'
            },
            'markers': marker_list
        }
        
        # Schreibe JSON-Datei
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, 
                     ensure_ascii=False, 
                     indent=2)
        
        logger.info(f"JSON-Export erstellt: {output_file}")
        return output_file
    
    def _get_categories(self) -> List[str]:
        """Sammelt alle verwendeten Kategorien"""
        categories = set()
        for marker in self.markers.values():
            categories.add(marker.get('kategorie', 'UNCATEGORIZED'))
        return sorted(list(categories))
    
    def generate_report(self):
        """Generiert einen Bericht über die gesammelten Marker"""
        print("\n" + "="*60)
        print("MARKER MASTER EXPORT - BERICHT")
        print("="*60)
        print(f"\nGesammelte Marker: {len(self.markers)}")
        print(f"Duplikate übersprungen: {self.duplicate_count}")
        print(f"Semantische Detektoren gefunden: {len(self.semantic_detectors)}")
        
        # Kategorien-Übersicht
        categories = {}
        for marker in self.markers.values():
            cat = marker.get('kategorie', 'UNCATEGORIZED')
            categories[cat] = categories.get(cat, 0) + 1
        
        print("\nKategorien:")
        for cat, count in sorted(categories.items()):
            print(f"  - {cat}: {count} Marker")
        
        # Marker mit Detektoren
        markers_with_detectors = sum(1 for m in self.markers.values() if 'semantics_detector' in m)
        print(f"\nMarker mit semantischen Detektoren: {markers_with_detectors}")
        
        # Top 10 Marker nach Anzahl der Beispiele
        sorted_by_examples = sorted(
            self.markers.items(), 
            key=lambda x: len(x[1].get('beispiele', [])), 
            reverse=True
        )[:10]
        
        print("\nTop 10 Marker nach Anzahl der Beispiele:")
        for name, data in sorted_by_examples:
            print(f"  - {name}: {len(data.get('beispiele', []))} Beispiele")


def main():
    """Hauptfunktion"""
    print("Marker Master Export Generator")
    print("==============================\n")
    
    # Erstelle Collector-Instanz
    collector = MarkerCollector()
    
    # Sammle alle Marker
    markers = collector.collect_all_markers()
    
    if not markers:
        logger.error("Keine Marker gefunden!")
        return
    
    # Generiere Bericht
    collector.generate_report()
    
    # Exportiere in beide Formate
    yaml_file = collector.export_to_yaml()
    json_file = collector.export_to_json()
    
    print(f"\nExport abgeschlossen:")
    print(f"  - YAML: {yaml_file}")
    print(f"  - JSON: {json_file}")
    
    # Erstelle README für die Verwendung
    readme_content = """# Marker Master Export

Diese Dateien enthalten das vollständige Marker-Masterset für den semantisch-psychologischen Resonanz- und Manipulations-Detektor.

## Verwendung

### Import in Python:
```python
import yaml
with open('marker_master_export.yaml', 'r', encoding='utf-8') as f:
    data = yaml.safe_load(f)
    
markers = data['markers']
```

### Struktur eines Markers:
- `marker`: Name/ID des Markers
- `beschreibung`: Klartext-Beschreibung
- `beispiele`: Liste typischer Formulierungen
- `kategorie`: Thematische Einordnung
- `tags`: Klassifikations-Tags
- `risk_score`: Risiko-Gewichtung (1-5)
- `semantics_detector`: Optional - Python-Detektor-Datei

### Risiko-Level:
- **Grün**: Kein oder nur unkritischer Marker
- **Gelb**: 1-2 moderate Marker, erste Drift erkennbar
- **Blinkend**: 3+ Marker oder ein Hochrisiko-Marker
- **Rot**: Hochrisiko-Kombination, massive Manipulation

Generiert am: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    with open("MARKER_MASTER_README.md", "w", encoding="utf-8") as f:
        f.write(readme_content)
    
    print("  - README: MARKER_MASTER_README.md")


if __name__ == "__main__":
    main() 

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/create_marker_master.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/detect_adaptive_polarization.py
# ====================================================================

def detect_adaptive_polarization(messages, marker_lookup, window=20, min_cycles=2):
    """
    Sucht OPENNESS→WITHDRAWAL-Zyklen im letzten 'window' Nachrichten.
    """
    states = []
    for msg in messages[-window:]:
        mset = set(marker_lookup[msg["id"]])
        if "OPENNESS" in mset:
            states.append(1)
        elif "WITHDRAWAL" in mset:
            states.append(-1)

    # Zähle Sign-Flips (1→-1 oder -1→1)
    flips = sum(1 for i in range(1, len(states)) if states[i] != states[i-1])
    if flips >= min_cycles * 2:     # zwei vollständige Hin-und-Her-Bewegungen
        return True, {"flips": flips}
    return False, {}


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/detect_adaptive_polarization.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/detect_secret_bonding.py
# ====================================================================

def detect_secret_bonding(messages, marker_lookup):
    """
    Trigger, wenn sustained_contact + >3 Erwähnungen von secrecy/exclusion
    innerhalb von 30 Nachrichten vorkommen.
    """
    secrecy_markers = {"S_MENTION_OF_SECRECY", "C_EXCLUSION_OF_OTHERS"}
    hits = 0
    for msg in messages[-30:]:
        if secrecy_markers & set(marker_lookup[msg["id"]]):
            hits += 1
    if hits >= 3:
        return True, {"secrecy_refs": hits}
    return False, {}


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/detect_secret_bonding.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/detect_sustain_contact.py
# ====================================================================

def detect_sustained_contact(messages, min_per_week=3, duration_days=60):
    """
    Prüft, ob über 'duration_days' eine durchschnittliche Frequenz
    von 'min_per_week' Kontakten erreicht wird.
    messages: chronologische Liste [{ts, id, markers:[...]}]
    """
    if not messages:
        return False, {}
    duration = (messages[-1]["ts"] - messages[0]["ts"]).days
    if duration < duration_days:
        return False, {}

    weeks = max(1, duration // 7)
    if len(messages) / weeks >= min_per_week:
        return True, {"weeks": weeks, "count": len(messages)}
    return False, {}


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/detect_sustain_contact.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/detector.utils.py
# ====================================================================

from collections import deque
from datetime import timedelta
import numpy as np

def sliding_window(items, window_sec, key=lambda x: x["ts"]):
    """Yield items that fall inside a moving time-window."""
    window = deque()
    for it in items:
        window.append(it)
        while window and (key(it) - key(window[0])).total_seconds() > window_sec:
            window.popleft()
        yield list(window)

def linear_slope(values):
    """Returns slope of a simple linear regression over equally spaced points."""
    if len(values) < 2:
        return 0.0
    x = np.arange(len(values))
    return np.polyfit(x, values, 1)[0]


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/detector.utils.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/fuzzy_engine.py
# ====================================================================

"""Fuzzy-Matching Engine für flexible Marker-Erkennung."""

import re
from typing import List, Tuple, Optional, Dict
from difflib import SequenceMatcher
import logging

try:
    from fuzzywuzzy import fuzz, process
    FUZZYWUZZY_AVAILABLE = True
except ImportError:
    FUZZYWUZZY_AVAILABLE = False
    logging.warning("fuzzywuzzy nicht installiert, verwende Fallback")

logger = logging.getLogger(__name__)


class FuzzyMatcher:
    """Engine für Fuzzy-String-Matching."""
    
    def __init__(self, threshold: float = 0.85):
        self.threshold = threshold
        self.use_fuzzywuzzy = FUZZYWUZZY_AVAILABLE
        
    def find_fuzzy_matches(
        self,
        text: str,
        patterns: List[str],
        threshold: Optional[float] = None
    ) -> List[Tuple[str, int, int, float]]:
        """Findet Fuzzy-Matches in einem Text.
        
        Args:
            text: Der zu durchsuchende Text
            patterns: Liste von Patterns zum Suchen
            threshold: Mindest-Ähnlichkeit (0-1)
            
        Returns:
            Liste von (matched_text, start_pos, end_pos, confidence)
        """
        if threshold is None:
            threshold = self.threshold
            
        matches = []
        text_lower = text.lower()
        words = text.split()
        
        for pattern in patterns:
            pattern_lower = pattern.lower()
            pattern_words = pattern.split()
            pattern_len = len(pattern_words)
            
            # Exakte Substring-Suche zuerst
            start = 0
            while True:
                pos = text_lower.find(pattern_lower, start)
                if pos == -1:
                    break
                matches.append((
                    text[pos:pos + len(pattern)],
                    pos,
                    pos + len(pattern),
                    1.0  # Exakter Match = 100% Konfidenz
                ))
                start = pos + 1
            
            # Fuzzy-Matching auf Wort-Ebene
            if pattern_len <= len(words):
                for i in range(len(words) - pattern_len + 1):
                    window = words[i:i + pattern_len]
                    window_text = ' '.join(window)
                    
                    similarity = self._calculate_similarity(
                        window_text.lower(),
                        pattern_lower
                    )
                    
                    if similarity >= threshold:
                        # Finde Position im Original-Text
                        start_pos = text.find(window[0], 0 if i == 0 else len(' '.join(words[:i])))
                        end_pos = text.find(window[-1], start_pos) + len(window[-1])
                        
                        matches.append((
                            text[start_pos:end_pos],
                            start_pos,
                            end_pos,
                            similarity
                        ))
        
        # Deduplizierung - behalte nur beste Matches für überlappende Bereiche
        matches = self._deduplicate_matches(matches)
        
        return matches
    
    def find_semantic_matches(
        self,
        text: str,
        semantic_groups: Dict[str, List[str]],
        threshold: Optional[float] = None
    ) -> List[Tuple[str, str, int, int, float]]:
        """Findet semantisch ähnliche Matches.
        
        Args:
            text: Der zu durchsuchende Text
            semantic_groups: Dict von Gruppe -> Synonyme/Varianten
            threshold: Mindest-Ähnlichkeit
            
        Returns:
            Liste von (group_name, matched_text, start_pos, end_pos, confidence)
        """
        if threshold is None:
            threshold = self.threshold
            
        semantic_matches = []
        
        for group_name, variants in semantic_groups.items():
            matches = self.find_fuzzy_matches(text, variants, threshold)
            
            for match_text, start, end, conf in matches:
                semantic_matches.append((
                    group_name,
                    match_text,
                    start,
                    end,
                    conf
                ))
        
        return semantic_matches
    
    def _calculate_similarity(self, str1: str, str2: str) -> float:
        """Berechnet Ähnlichkeit zwischen zwei Strings."""
        if self.use_fuzzywuzzy:
            # Verwende verschiedene Fuzzy-Metriken und nimm Maximum
            ratio = fuzz.ratio(str1, str2) / 100.0
            partial = fuzz.partial_ratio(str1, str2) / 100.0
            token_sort = fuzz.token_sort_ratio(str1, str2) / 100.0
            
            return max(ratio, partial, token_sort)
        else:
            # Fallback: SequenceMatcher
            return SequenceMatcher(None, str1, str2).ratio()
    
    def _deduplicate_matches(
        self,
        matches: List[Tuple[str, int, int, float]]
    ) -> List[Tuple[str, int, int, float]]:
        """Entfernt überlappende Matches, behält die mit höchster Konfidenz."""
        if not matches:
            return []
        
        # Sortiere nach Konfidenz (absteigend) und dann Position
        sorted_matches = sorted(matches, key=lambda x: (-x[3], x[1]))
        
        deduplicated = []
        used_ranges = []
        
        for match in sorted_matches:
            _, start, end, _ = match
            
            # Prüfe Überlappung mit bereits verwendeten Bereichen
            overlaps = False
            for used_start, used_end in used_ranges:
                if not (end <= used_start or start >= used_end):
                    overlaps = True
                    break
            
            if not overlaps:
                deduplicated.append(match)
                used_ranges.append((start, end))
        
        # Sortiere nach Position für Output
        return sorted(deduplicated, key=lambda x: x[1])


class RegexMatcher:
    """Engine für Regex-basiertes Matching."""
    
    def __init__(self):
        self._compiled_patterns: Dict[str, re.Pattern] = {}
    
    def find_regex_matches(
        self,
        text: str,
        patterns: List[str],
        case_sensitive: bool = False
    ) -> List[Tuple[str, int, int]]:
        """Findet Regex-Matches in einem Text.
        
        Args:
            text: Der zu durchsuchende Text
            patterns: Liste von Regex-Patterns
            case_sensitive: Groß-/Kleinschreibung beachten
            
        Returns:
            Liste von (matched_text, start_pos, end_pos)
        """
        matches = []
        flags = 0 if case_sensitive else re.IGNORECASE
        
        for pattern in patterns:
            try:
                # Compile und cache Pattern
                cache_key = f"{pattern}_{flags}"
                if cache_key not in self._compiled_patterns:
                    self._compiled_patterns[cache_key] = re.compile(pattern, flags)
                
                regex = self._compiled_patterns[cache_key]
                
                for match in regex.finditer(text):
                    matches.append((
                        match.group(),
                        match.start(),
                        match.end()
                    ))
                    
            except re.error as e:
                logger.error(f"Ungültiges Regex-Pattern '{pattern}': {e}")
                continue
        
        return matches
    
    def extract_context(
        self,
        text: str,
        position: int,
        context_words: int = 10
    ) -> str:
        """Extrahiert Kontext um eine Position im Text.
        
        Args:
            text: Der Gesamttext
            position: Position im Text
            context_words: Anzahl Wörter vor/nach der Position
            
        Returns:
            Kontext-String
        """
        words = text.split()
        word_positions = []
        current_pos = 0
        
        # Finde Wort-Positionen
        for word in words:
            word_start = text.find(word, current_pos)
            word_end = word_start + len(word)
            word_positions.append((word_start, word_end))
            current_pos = word_end
        
        # Finde Wort, das die Position enthält
        target_word_idx = 0
        for i, (start, end) in enumerate(word_positions):
            if start <= position <= end:
                target_word_idx = i
                break
        
        # Extrahiere Kontext
        start_idx = max(0, target_word_idx - context_words)
        end_idx = min(len(words), target_word_idx + context_words + 1)
        
        context_words_list = words[start_idx:end_idx]
        
        # Markiere Target
        if start_idx < target_word_idx < end_idx:
            relative_idx = target_word_idx - start_idx
            context_words_list[relative_idx] = f"**{context_words_list[relative_idx]}**"
        
        return ' '.join(context_words_list)

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/fuzzy_engine.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/logging_config.py
# ====================================================================

"""Logging-Konfiguration für die Marker Analysis Engine."""

import logging
import logging.handlers
import sys
from pathlib import Path
from typing import Optional
import colorlog


def setup_logging(
    level: str = "INFO",
    log_file: Optional[Path] = None,
    console: bool = True,
    colored: bool = True,
    rotation: bool = True,
    max_bytes: int = 10 * 1024 * 1024,  # 10MB
    backup_count: int = 5
) -> logging.Logger:
    """Konfiguriert das Logging-System.
    
    Args:
        level: Logging-Level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        log_file: Pfad zur Log-Datei (optional)
        console: Ob auf Konsole geloggt werden soll
        colored: Ob farbige Ausgabe verwendet werden soll
        rotation: Ob Log-Rotation aktiviert werden soll
        max_bytes: Maximale Größe einer Log-Datei
        backup_count: Anzahl der Backup-Dateien
    
    Returns:
        Konfigurierter Root-Logger
    """
    # Root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, level.upper()))
    
    # Entferne existierende Handler
    root_logger.handlers.clear()
    
    # Format für Logs
    detailed_format = (
        '%(asctime)s - %(name)s - %(levelname)s - '
        '%(filename)s:%(lineno)d - %(funcName)s() - %(message)s'
    )
    simple_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    # Console Handler
    if console:
        if colored and colorlog:
            console_handler = colorlog.StreamHandler(sys.stdout)
            console_format = colorlog.ColoredFormatter(
                '%(log_color)s%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S',
                log_colors={
                    'DEBUG': 'cyan',
                    'INFO': 'green',
                    'WARNING': 'yellow',
                    'ERROR': 'red',
                    'CRITICAL': 'red,bg_white',
                }
            )
            console_handler.setFormatter(console_format)
        else:
            console_handler = logging.StreamHandler(sys.stdout)
            console_handler.setFormatter(logging.Formatter(simple_format))
        
        root_logger.addHandler(console_handler)
    
    # File Handler
    if log_file:
        # Erstelle Log-Verzeichnis falls nötig
        log_file = Path(log_file)
        log_file.parent.mkdir(parents=True, exist_ok=True)
        
        if rotation:
            file_handler = logging.handlers.RotatingFileHandler(
                log_file,
                maxBytes=max_bytes,
                backupCount=backup_count,
                encoding='utf-8'
            )
        else:
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
        
        file_handler.setFormatter(logging.Formatter(detailed_format))
        root_logger.addHandler(file_handler)
    
    # Spezielle Logger-Konfigurationen
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('matplotlib').setLevel(logging.WARNING)
    
    return root_logger


def get_logger(name: str) -> logging.Logger:
    """Gibt einen benannten Logger zurück.
    
    Args:
        name: Name des Loggers (normalerweise __name__)
    
    Returns:
        Logger-Instanz
    """
    return logging.getLogger(name)


class LogContext:
    """Context Manager für temporäre Logging-Level-Änderungen."""
    
    def __init__(self, logger: logging.Logger, level: str):
        self.logger = logger
        self.new_level = getattr(logging, level.upper())
        self.old_level = None
    
    def __enter__(self):
        self.old_level = self.logger.level
        self.logger.setLevel(self.new_level)
        return self.logger
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.logger.setLevel(self.old_level)


def log_function_call(func):
    """Decorator zum Loggen von Funktionsaufrufen."""
    logger = logging.getLogger(func.__module__)
    
    def wrapper(*args, **kwargs):
        logger.debug(f"Calling {func.__name__} with args={args}, kwargs={kwargs}")
        try:
            result = func(*args, **kwargs)
            logger.debug(f"{func.__name__} returned {result}")
            return result
        except Exception as e:
            logger.error(f"{func.__name__} raised {type(e).__name__}: {e}")
            raise
    
    return wrapper


# Standard-Setup beim Import
if not logging.getLogger().handlers:
    setup_logging()

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/logging_config.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/main.py
# ====================================================================

from fastapi import FastAPI, File, UploadFile
from fastapi.responses import HTMLResponse
from pymongo import MongoClient
import yaml, datetime

app = FastAPI()

# ➡️ MongoDB-Zugangsdaten HIER eintragen (ersetze die Platzhalter!):
client = MongoClient("mongodb+srv://<USER>:<PASS>@<CLUSTER>.mongodb.net/mydb?retryWrites=true&w=majority")
db = client["mydb"]["markerdocs"]

def valid_marker(m):
    return all(x in m for x in ("id", "description", "examples"))

@app.get("/", response_class=HTMLResponse)
def upload_form():
    return """
    <html>
    <body>
    <h2>Marker Datei Upload</h2>
    <form action="/upload/" enctype="multipart/form-data" method="post">
      <input name="file" type="file">
      <input type="submit">
    </form>
    </body>
    </html>
    """

@app.post("/upload/", response_class=HTMLResponse)
async def upload_file(file: UploadFile = File(...)):
    content = await file.read()
    data = yaml.safe_load(content)
    items = data if isinstance(data, list) else [data]
    imported, failed = [], []
    for m in items:
        if valid_marker(m):
            m["_audit"] = {"ts": datetime.datetime.utcnow().isoformat(), "src": file.filename}
            db.replace_one({"id": m["id"]}, m, upsert=True)
            imported.append(m["id"])
        else:
            failed.append(m.get("id", "?"))
    return f"<b>Importiert:</b> {imported}<br><b>Fehlerhaft:</b> {failed}"

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/main.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/marker_matcher.py
# ====================================================================

#!/usr/bin/env python3
"""
Marker Matcher - Semantisch-psychologischer Resonanz- und Manipulations-Detektor
Analysiert Texte basierend auf dem Marker Master Export
"""

import yaml
import json
import re
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from pathlib import Path
import logging
from datetime import datetime
from collections import defaultdict

# Logging konfigurieren
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class MarkerMatch:
    """Repräsentiert einen gefundenen Marker-Treffer"""
    marker_name: str
    marker_beschreibung: str
    matched_text: str
    position: Tuple[int, int]  # (start, end)
    confidence_score: float = 1.0
    kontext: str = ""
    pattern_type: str = "exact"  # exact, fuzzy, semantic
    tags: List[str] = field(default_factory=list)
    risk_score: int = 1


@dataclass
class AnalysisResult:
    """Ergebnis der Marker-Analyse"""
    text: str
    timestamp: str
    gefundene_marker: List[MarkerMatch]
    risk_level: str
    risk_level_color: str
    total_risk_score: int
    categories_found: Dict[str, int]
    summary: str
    
    def to_dict(self) -> Dict[str, Any]:
        """Konvertiert das Ergebnis in ein Dictionary"""
        return {
            'timestamp': self.timestamp,
            'text_preview': self.text[:100] + '...' if len(self.text) > 100 else self.text,
            'gefundene_marker': [
                {
                    'marker': m.marker_name,
                    'beschreibung': m.marker_beschreibung,
                    'matched_text': m.matched_text,
                    'position': m.position,
                    'confidence': m.confidence_score,
                    'tags': m.tags
                } for m in self.gefundene_marker
            ],
            'risk_level': self.risk_level,
            'risk_level_color': self.risk_level_color,
            'total_risk_score': self.total_risk_score,
            'categories': self.categories_found,
            'summary': self.summary
        }


class MarkerMatcher:
    """Hauptklasse für Marker-basierte Textanalyse"""
    
    def __init__(self, marker_file: str = "marker_master_export.yaml"):
        """Initialisiert den Matcher mit Marker-Daten"""
        self.markers = {}
        self.semantic_detectors = {}
        self.risk_thresholds = {
            'green': (0, 1),
            'yellow': (2, 5),
            'blinking': (6, 10),
            'red': (11, float('inf'))
        }
        
        # Lade Marker-Daten
        self._load_markers(marker_file)
        
    def _load_markers(self, marker_file: str):
        """Lädt Marker-Daten aus YAML-Datei"""
        try:
            with open(marker_file, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
            
            # Extrahiere Marker
            for marker in data.get('markers', []):
                self.markers[marker['marker']] = marker
                
            # Extrahiere Risk-Level-Definitionen
            if 'risk_levels' in data:
                self.risk_level_descriptions = data['risk_levels']
                
            logger.info(f"{len(self.markers)} Marker geladen")
            
        except Exception as e:
            logger.error(f"Fehler beim Laden der Marker-Datei: {e}")
            raise
    
    def analyze_text(self, text: str) -> AnalysisResult:
        """Analysiert einen Text auf Marker"""
        logger.debug(f"Analysiere Text mit {len(text)} Zeichen")
        
        # Initialisiere Ergebnis
        matches = []
        categories_count = defaultdict(int)
        total_risk_score = 0
        
        # Durchsuche Text nach jedem Marker
        for marker_name, marker_data in self.markers.items():
            marker_matches = self._find_marker_in_text(text, marker_data)
            
            for match in marker_matches:
                matches.append(match)
                categories_count[marker_data.get('kategorie', 'UNCATEGORIZED')] += 1
                total_risk_score += match.risk_score
        
        # Bestimme Risk-Level
        risk_level, risk_color = self._calculate_risk_level(total_risk_score, len(matches))
        
        # Erstelle Zusammenfassung
        summary = self._generate_summary(matches, risk_level)
        
        return AnalysisResult(
            text=text,
            timestamp=datetime.now().isoformat(),
            gefundene_marker=matches,
            risk_level=risk_level,
            risk_level_color=risk_color,
            total_risk_score=total_risk_score,
            categories_found=dict(categories_count),
            summary=summary
        )
    
    def _find_marker_in_text(self, text: str, marker_data: Dict[str, Any]) -> List[MarkerMatch]:
        """Sucht nach einem spezifischen Marker im Text"""
        matches = []
        
        # Suche nach Beispielen
        for beispiel in marker_data.get('beispiele', []):
            if not beispiel:  # Überspringe leere Beispiele
                continue
                
            # Normalisiere für besseres Matching
            beispiel_normalized = beispiel.lower().strip()
            text_lower = text.lower()
            
            # Exakte Suche
            if beispiel_normalized in text_lower:
                start = text_lower.find(beispiel_normalized)
                end = start + len(beispiel_normalized)
                
                match = MarkerMatch(
                    marker_name=marker_data['marker'],
                    marker_beschreibung=marker_data.get('beschreibung', ''),
                    matched_text=text[start:end],
                    position=(start, end),
                    confidence_score=1.0,
                    kontext=self._extract_context(text, start, end),
                    pattern_type='exact',
                    tags=marker_data.get('tags', []),
                    risk_score=marker_data.get('risk_score', 1)
                )
                matches.append(match)
            
            # Fuzzy-Matching für teilweise Übereinstimmungen
            elif self._fuzzy_match(beispiel_normalized, text_lower):
                # Vereinfachtes Fuzzy-Matching
                words = beispiel_normalized.split()
                if len(words) > 2 and sum(1 for w in words if w in text_lower) >= len(words) * 0.7:
                    match = MarkerMatch(
                        marker_name=marker_data['marker'],
                        marker_beschreibung=marker_data.get('beschreibung', ''),
                        matched_text=beispiel[:50] + '...',
                        position=(0, 0),
                        confidence_score=0.7,
                        pattern_type='fuzzy',
                        tags=marker_data.get('tags', []),
                        risk_score=marker_data.get('risk_score', 1)
                    )
                    matches.append(match)
        
        # Semantische Patterns (falls vorhanden)
        if 'semantic_patterns' in marker_data:
            semantic_matches = self._apply_semantic_patterns(text, marker_data)
            matches.extend(semantic_matches)
        
        return matches
    
    def _fuzzy_match(self, pattern: str, text: str) -> bool:
        """Einfaches Fuzzy-Matching"""
        # Sehr vereinfachte Version - kann erweitert werden
        words = pattern.split()
        return len(words) > 2 and sum(1 for w in words if w in text) >= len(words) * 0.6
    
    def _apply_semantic_patterns(self, text: str, marker_data: Dict[str, Any]) -> List[MarkerMatch]:
        """Wendet semantische Patterns an (falls definiert)"""
        matches = []
        semantic_data = marker_data.get('semantic_patterns', {})
        
        if 'patterns' in semantic_data:
            for pattern_rule in semantic_data['patterns']:
                if 'pattern' in pattern_rule:
                    try:
                        # Kompiliere Regex-Pattern
                        regex = re.compile(pattern_rule['pattern'], re.IGNORECASE)
                        
                        for match in regex.finditer(text):
                            marker_match = MarkerMatch(
                                marker_name=marker_data['marker'],
                                marker_beschreibung=marker_data.get('beschreibung', ''),
                                matched_text=match.group(0),
                                position=(match.start(), match.end()),
                                confidence_score=0.9,
                                kontext=self._extract_context(text, match.start(), match.end()),
                                pattern_type='semantic',
                                tags=marker_data.get('tags', []),
                                risk_score=marker_data.get('risk_score', 1)
                            )
                            matches.append(marker_match)
                    except Exception as e:
                        logger.debug(f"Fehler bei Regex-Pattern: {e}")
        
        return matches
    
    def _extract_context(self, text: str, start: int, end: int, context_size: int = 50) -> str:
        """Extrahiert Kontext um einen Match"""
        context_start = max(0, start - context_size)
        context_end = min(len(text), end + context_size)
        
        context = text[context_start:context_end]
        
        # Markiere den gefundenen Text
        if context_start > 0:
            context = "..." + context
        if context_end < len(text):
            context = context + "..."
            
        return context
    
    def _calculate_risk_level(self, total_score: int, marker_count: int) -> Tuple[str, str]:
        """Berechnet das Risiko-Level basierend auf Score und Anzahl"""
        # Berücksichtige sowohl Score als auch Anzahl
        adjusted_score = total_score + (marker_count * 0.5)
        
        for level, (min_score, max_score) in self.risk_thresholds.items():
            if min_score <= adjusted_score <= max_score:
                return level, self._get_color_for_level(level)
        
        return 'red', '#FF0000'
    
    def _get_color_for_level(self, level: str) -> str:
        """Gibt die Farbe für ein Risk-Level zurück"""
        colors = {
            'green': '#00FF00',
            'yellow': '#FFFF00',
            'blinking': '#FFA500',  # Orange für "blinkend"
            'red': '#FF0000'
        }
        return colors.get(level, '#808080')
    
    def _generate_summary(self, matches: List[MarkerMatch], risk_level: str) -> str:
        """Generiert eine Zusammenfassung der Analyse"""
        if not matches:
            return "Keine kritischen Marker gefunden. Die Kommunikation erscheint neutral."
        
        # Zähle Marker-Typen
        marker_types = defaultdict(int)
        for match in matches:
            marker_types[match.marker_name] += 1
        
        # Top 3 Marker
        top_markers = sorted(marker_types.items(), key=lambda x: x[1], reverse=True)[:3]
        
        summary = f"Risiko-Level: {risk_level.upper()}\n"
        summary += f"Gefundene Marker: {len(matches)}\n"
        summary += f"Häufigste Muster: "
        summary += ", ".join([f"{name} ({count}x)" for name, count in top_markers])
        
        # Füge spezifische Warnungen hinzu
        if risk_level in ['blinking', 'red']:
            summary += "\n⚠️ WARNUNG: Deutliche Anzeichen für manipulative Kommunikation erkannt!"
        
        return summary
    
    def analyze_batch(self, texts: List[str]) -> List[AnalysisResult]:
        """Analysiert mehrere Texte"""
        results = []
        for text in texts:
            results.append(self.analyze_text(text))
        return results


def main():
    """Demo-Funktion"""
    # Erstelle Matcher
    matcher = MarkerMatcher()
    
    # Test-Texte
    test_texts = [
        "Das hast du dir nur eingebildet. Ich habe nie gesagt, dass ich mitkomme.",
        "Ich liebe dich über alles! Du bist meine Seelenverwandte. Niemand versteht mich so wie du.",
        "Ich bin hin- und hergerissen zwischen Bleiben und Gehen – ich weiß nicht mehr weiter.",
        "Hey, wie geht's dir? Schönes Wetter heute, nicht wahr?",
        "Du bist zu empfindlich. Das war doch nur ein Scherz. Du übertreibst mal wieder."
    ]
    
    print("\nMARKER MATCHER - DEMO")
    print("="*60)
    
    for i, text in enumerate(test_texts, 1):
        print(f"\nText {i}: {text[:50]}...")
        result = matcher.analyze_text(text)
        
        print(f"Risk-Level: {result.risk_level} ({result.risk_level_color})")
        print(f"Gefundene Marker: {len(result.gefundene_marker)}")
        
        for match in result.gefundene_marker:
            print(f"  - {match.marker_name}: '{match.matched_text}'")
        
        print(f"\nZusammenfassung:\n{result.summary}")
        print("-"*60)


if __name__ == "__main__":
    main() 

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/marker_matcher.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/marker_models.py
# ====================================================================

"""Datenmodelle für Marker-Definitionen und -Treffer."""

from typing import List, Optional, Dict, Any, Union
from datetime import datetime
from enum import Enum
from pydantic import BaseModel, Field, validator


class MarkerCategory(str, Enum):
    """Kategorien für verschiedene Marker-Typen."""
    
    FRAUD = "fraud"
    MANIPULATION = "manipulation"
    GASLIGHTING = "gaslighting"
    LOVE_BOMBING = "love_bombing"
    EMOTIONAL_ABUSE = "emotional_abuse"
    FINANCIAL_ABUSE = "financial_abuse"
    POSITIVE = "positive"
    EMPATHY = "empathy"
    SUPPORT = "support"
    CONFLICT_RESOLUTION = "conflict_resolution"
    BOUNDARY_SETTING = "boundary_setting"
    SELF_CARE = "self_care"


class MarkerSeverity(str, Enum):
    """Schweregrad eines Markers."""
    
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class MarkerPattern(BaseModel):
    """Ein einzelnes Pattern zur Marker-Erkennung."""
    
    pattern: str = Field(..., description="Regex oder Text-Pattern")
    is_regex: bool = Field(default=False, description="Ob Pattern als Regex interpretiert werden soll")
    case_sensitive: bool = Field(default=False, description="Groß-/Kleinschreibung beachten")
    fuzzy_threshold: Optional[float] = Field(
        default=0.85, 
        ge=0.0, 
        le=1.0,
        description="Schwellenwert für Fuzzy-Matching (0-1)"
    )
    context_words: Optional[int] = Field(
        default=10,
        description="Anzahl Wörter für Kontext-Extraktion"
    )


class MarkerDefinition(BaseModel):
    """Definition eines Markers mit allen relevanten Eigenschaften."""
    
    id: str = Field(..., description="Eindeutige Marker-ID")
    name: str = Field(..., description="Menschenlesbarer Name")
    category: MarkerCategory = Field(..., description="Marker-Kategorie")
    severity: MarkerSeverity = Field(default=MarkerSeverity.MEDIUM)
    description: str = Field(..., description="Ausführliche Beschreibung")
    
    patterns: List[MarkerPattern] = Field(
        default_factory=list,
        description="Liste von Patterns zur Erkennung"
    )
    
    keywords: List[str] = Field(
        default_factory=list,
        description="Schlüsselwörter für einfaches Matching"
    )
    
    examples: List[str] = Field(
        default_factory=list,
        description="Beispiele für diesen Marker"
    )
    
    weight: float = Field(
        default=1.0,
        ge=0.0,
        le=10.0,
        description="Gewichtung für Scoring (0-10)"
    )
    
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="Zusätzliche Metadaten"
    )
    
    active: bool = Field(default=True, description="Ob Marker aktiv ist")
    
    @validator('patterns')
    def validate_patterns(cls, v):
        """Stelle sicher, dass mindestens ein Pattern oder Keyword existiert."""
        if not v:
            return v
        return v
    
    def get_all_patterns(self) -> List[str]:
        """Gibt alle Patterns und Keywords zurück."""
        all_patterns = []
        for p in self.patterns:
            all_patterns.append(p.pattern)
        all_patterns.extend(self.keywords)
        return all_patterns


class MarkerMatch(BaseModel):
    """Ein gefundener Marker-Treffer in einem Text."""
    
    marker_id: str = Field(..., description="ID des gefundenen Markers")
    marker_name: str = Field(..., description="Name des Markers")
    category: MarkerCategory = Field(..., description="Kategorie des Markers")
    severity: MarkerSeverity = Field(..., description="Schweregrad")
    
    text: str = Field(..., description="Gefundener Text")
    context: str = Field(..., description="Kontext um den Fund")
    
    chunk_id: str = Field(..., description="ID des Text-Chunks")
    position: int = Field(..., description="Position im Text")
    
    confidence: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="Konfidenz des Matches (0-1)"
    )
    
    speaker: Optional[str] = Field(None, description="Sprecher/Autor")
    timestamp: Optional[datetime] = Field(None, description="Zeitstempel")
    
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="Zusätzliche Match-Metadaten"
    )


class MarkerStatistics(BaseModel):
    """Statistiken über Marker-Treffer."""
    
    total_matches: int = Field(default=0)
    matches_by_category: Dict[str, int] = Field(default_factory=dict)
    matches_by_severity: Dict[str, int] = Field(default_factory=dict)
    matches_by_speaker: Dict[str, int] = Field(default_factory=dict)
    
    average_confidence: float = Field(default=0.0)
    time_distribution: Dict[str, int] = Field(
        default_factory=dict,
        description="Verteilung über Zeit (z.B. pro Stunde/Tag)"
    )
    
    most_frequent_markers: List[Dict[str, Union[str, int]]] = Field(
        default_factory=list,
        description="Top Marker nach Häufigkeit"
    )


class MarkerProfile(BaseModel):
    """Profil einer Person basierend auf Marker-Analyse."""
    
    speaker_id: str = Field(..., description="ID des Sprechers")
    total_messages: int = Field(default=0)
    
    positive_markers: int = Field(default=0)
    negative_markers: int = Field(default=0)
    
    dominant_categories: List[str] = Field(default_factory=list)
    risk_score: float = Field(default=0.0, ge=0.0, le=10.0)
    
    marker_timeline: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Zeitliche Entwicklung der Marker"
    )
    
    behavioral_patterns: Dict[str, Any] = Field(
        default_factory=dict,
        description="Erkannte Verhaltensmuster"
    )

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/marker_models.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/role_models.py
# ====================================================================

"""Datenmodelle für Rollen- und Attribut-Tracking."""

from typing import List, Dict, Optional, Any, Set
from datetime import datetime
from pydantic import BaseModel, Field, validator
from enum import Enum

from ..matcher.marker_models import MarkerCategory


class AttributeType(str, Enum):
    """Typen von Personen

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/role_models.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/schema_loader.py
# ====================================================================

import json

class SchemaLoader:
    def __init__(self, schema_path):
        self.schema_path = schema_path
        self.schema = self.load_schema()

    def load_schema(self):
        with open(self.schema_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def get_included_marker_names(self):
        markers = []
        for group in ["atomic_markers", "cluster_markers", "semantic_markers", "meta_markers"]:
            entries = self.schema.get("marker_detection", {}).get(group, [])
            markers.extend([entry["name"] for entry in entries])
        return markers

    def get_drift_axes(self):
        return self.schema.get("drift_analysis", {}).get("drift_axes", [])

    def get_metrics(self):
        return self.schema.get("relationship_inference", {})

    def get_visualization_options(self):
        return self.schema.get("outputs", {}).get("visualization", [])

    def get_export_formats(self):
        return self.schema.get("outputs", {}).get("export_format", [])


# Beispielhafte Nutzung:
# loader = SchemaLoader("schemas/beziehungsanalyse_schema.json")
# print(loader.get_included_marker_names())
# print(loader.get_drift_axes())
# print(loader.get_metrics())


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/schema_loader.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/score_models.py
# ====================================================================

"""Datenmodelle für das Scoring-System."""

from typing import Dict, List, Optional, Any
from datetime import datetime
from pydantic import BaseModel, Field, validator
from enum import Enum

from ..matcher.marker_models import MarkerCategory, MarkerSeverity


class ScoreType(str, Enum):
    """Verschiedene Score-Typen für unterschiedliche Analysen."""
    
    MANIPULATION_INDEX = "manipulation_index"
    RELATIONSHIP_HEALTH = "relationship_health"
    FRAUD_PROBABILITY = "fraud_probability"
    EMOTIONAL_SUPPORT = "emotional_support"
    CONFLICT_LEVEL = "conflict_level"
    COMMUNICATION_QUALITY = "communication_quality"
    TRUST_LEVEL = "trust_level"
    RESOURCE_BALANCE = "resource_balance"


class ScoringModel(BaseModel):
    """Definition eines Scoring-Modells."""
    
    id: str = Field(..., description="Eindeutige Model-ID")
    name: str = Field(..., description="Name des Scoring-Modells")
    type: ScoreType = Field(..., description="Typ des Scores")
    description: str = Field(..., description="Beschreibung des Modells")
    
    # Gewichtungen für verschiedene Marker-Kategorien
    category_weights: Dict[MarkerCategory, float] = Field(
        default_factory=dict,
        description="Gewichtung pro Marker-Kategorie"
    )
    
    # Gewichtungen für Severity-Level
    severity_multipliers: Dict[MarkerSeverity, float] = Field(
        default_factory=lambda: {
            MarkerSeverity.LOW: 0.5,
            MarkerSeverity.MEDIUM: 1.0,
            MarkerSeverity.HIGH: 2.0,
            MarkerSeverity.CRITICAL: 3.0
        },
        description="Multiplikatoren für Schweregrade"
    )
    
    # Score-Berechnung
    scale_min: float = Field(default=1.0, description="Minimaler Score")
    scale_max: float = Field(default=10.0, description="Maximaler Score")
    inverse_scale: bool = Field(
        default=False,
        description="Ob höhere Marker-Counts zu niedrigeren Scores führen"
    )
    
    # Normalisierung
    normalization_factor: float = Field(
        default=100.0,
        description="Faktor zur Normalisierung der Rohwerte"
    )
    
    # Schwellenwerte für Interpretation
    thresholds: Dict[str, float] = Field(
        default_factory=lambda: {
            "critical": 8.0,
            "warning": 6.0,
            "normal": 4.0,
            "good": 2.0
        },
        description="Schwellenwerte für verschiedene Zustände"
    )
    
    active: bool = Field(default=True, description="Ob das Modell aktiv ist")


class ChunkScore(BaseModel):
    """Score für einen einzelnen Text-Chunk."""
    
    chunk_id: str = Field(..., description="ID des bewerteten Chunks")
    model_id: str = Field(..., description="ID des verwendeten Scoring-Modells")
    score_type: ScoreType = Field(..., description="Typ des Scores")
    
    raw_score: float = Field(..., description="Roher Score vor Normalisierung")
    normalized_score: float = Field(..., ge=1.0, le=10.0, description="Normalisierter Score (1-10)")
    
    contributing_markers: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Marker die zum Score beigetragen haben"
    )
    
    confidence: float = Field(
        default=1.0,
        ge=0.0,
        le=1.0,
        description="Konfidenz des Scores"
    )
    
    timestamp: Optional[datetime] = Field(None, description="Zeitstempel des Chunks")
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    @validator('normalized_score')
    def validate_normalized_score(cls, v, values):
        """Stelle sicher, dass Score im gültigen Bereich ist."""
        if 'score_type' in values:
            return max(1.0, min(10.0, v))
        return v


class AggregatedScore(BaseModel):
    """Aggregierter Score über mehrere Chunks/Zeiträume."""
    
    model_id: str = Field(..., description="ID des Scoring-Modells")
    score_type: ScoreType = Field(..., description="Typ des Scores")
    
    average_score: float = Field(..., ge=1.0, le=10.0)
    min_score: float = Field(..., ge=1.0, le=10.0)
    max_score: float = Field(..., ge=1.0, le=10.0)
    
    trend: str = Field(
        default="stable",
        description="Trend: improving, declining, stable"
    )
    trend_strength: float = Field(
        default=0.0,
        description="Stärke des Trends (-1 bis 1)"
    )
    
    chunk_count: int = Field(..., description="Anzahl bewerteter Chunks")
    time_range: Optional[Dict[str, datetime]] = Field(
        None,
        description="Zeitbereich der Aggregation"
    )
    
    distribution: Dict[str, int] = Field(
        default_factory=dict,
        description="Verteilung der Scores (z.B. 1-2: 5, 3-4: 10, ...)"
    )
    
    top_markers: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Häufigste beitragende Marker"
    )
    
    metadata: Dict[str, Any] = Field(default_factory=dict)


class ScoreComparison(BaseModel):
    """Vergleich von Scores zwischen Sprechern/Zeiträumen."""
    
    comparison_type: str = Field(..., description="speaker_comparison oder time_comparison")
    model_id: str = Field(..., description="Verwendetes Scoring-Modell")
    
    entities: List[str] = Field(
        ...,
        description="Verglichene Entitäten (Speaker-IDs oder Zeiträume)"
    )
    
    scores: Dict[str, AggregatedScore] = Field(
        ...,
        description="Scores pro Entität"
    )
    
    differences: Dict[str, float] = Field(
        default_factory=dict,
        description="Unterschiede zwischen Entitäten"
    )
    
    winner: Optional[str] = Field(
        None,
        description="Entität mit bestem Score (je nach Modell)"
    )
    
    insights: List[str] = Field(
        default_factory=list,
        description="Automatisch generierte Einsichten"
    )
    
    metadata: Dict[str, Any] = Field(default_factory=dict)


class ScoringResult(BaseModel):
    """Gesamtergebnis einer Scoring-Analyse."""
    
    chunk_scores: List[ChunkScore] = Field(
        default_factory=list,
        description="Einzelne Chunk-Scores"
    )
    
    aggregated_scores: Dict[str, AggregatedScore] = Field(
        default_factory=dict,
        description="Aggregierte Scores pro Modell"
    )
    
    speaker_scores: Dict[str, Dict[str, AggregatedScore]] = Field(
        default_factory=dict,
        description="Scores pro Sprecher und Modell"
    )
    
    timeline: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Zeitliche Entwicklung der Scores"
    )
    
    alerts: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Generierte Warnungen basierend auf Schwellenwerten"
    )
    
    summary: Dict[str, Any] = Field(
        default_factory=dict,
        description="Zusammenfassung der wichtigsten Erkenntnisse"
    )
    
    processing_time: float = Field(0.0)
    
    def get_score_by_type(self, score_type: ScoreType) -> Optional[AggregatedScore]:
        """Holt aggregierten Score für einen bestimmten Typ."""
        return self.aggregated_scores.get(score_type.value)
    
    def get_speaker_comparison(self, score_type: ScoreType) -> ScoreComparison:
        """Erstellt Vergleich zwischen Sprechern für einen Score-Typ."""
        comparison = ScoreComparison(
            comparison_type="speaker_comparison",
            model_id=score_type.value,
            entities=list(self.speaker_scores.keys()),
            scores={}
        )
        
        for speaker, scores in self.speaker_scores.items():
            if score_type.value in scores:
                comparison.scores[speaker] = scores[score_type.value]
        
        # Berechne Unterschiede
        if len(comparison.scores) == 2:
            speakers = list(comparison.scores.keys())
            diff = comparison.scores[speakers[0]].average_score - comparison.scores[speakers[1]].average_score
            comparison.differences[f"{speakers[0]}_vs_{speakers[1]}"] = diff
        
        return comparison

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/score_models.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/scoring_engine.py
# ====================================================================

"""Scoring Engine für die Bewertung von Text-Chunks basierend auf Marker-Treffern."""

import logging
import time
from typing import List, Dict, Optional, Tuple
from collections import defaultdict
from datetime import datetime
import numpy as np

from .score_models import (
    ScoringModel, ChunkScore, AggregatedScore, ScoringResult,
    ScoreType, ScoreComparison
)
from ..matcher.marker_models import MarkerMatch, MarkerCategory, MarkerSeverity
from ..chunker.chunk_models import TextChunk

logger = logging.getLogger(__name__)


class ScoringEngine:
    """Engine zur Berechnung von Scores basierend auf Marker-Matches."""
    
    def __init__(self):
        self.models: Dict[str, ScoringModel] = {}
        self._initialize_default_models()
    
    def _initialize_default_models(self):
        """Initialisiert Standard-Scoring-Modelle."""
        
        # Manipulations-Index
        self.models["manipulation_index"] = ScoringModel(
            id="manipulation_index",
            name="Manipulations-Index",
            type=ScoreType.MANIPULATION_INDEX,
            description="Misst den Grad manipulativer Kommunikation",
            category_weights={
                MarkerCategory.MANIPULATION: 2.0,
                MarkerCategory.GASLIGHTING: 3.0,
                MarkerCategory.EMOTIONAL_ABUSE: 2.5,
                MarkerCategory.LOVE_BOMBING: 1.5,
                MarkerCategory.FRAUD: 3.0,
                MarkerCategory.POSITIVE: -1.0,  # Positive Marker reduzieren Score
                MarkerCategory.EMPATHY: -0.5,
                MarkerCategory.SUPPORT: -0.5
            },
            inverse_scale=False  # Höher = mehr Manipulation
        )
        
        # Beziehungsgesundheit
        self.models["relationship_health"] = ScoringModel(
            id="relationship_health",
            name="Beziehungsgesundheit",
            type=ScoreType.RELATIONSHIP_HEALTH,
            description="Bewertet die Gesundheit der Beziehungskommunikation",
            category_weights={
                MarkerCategory.POSITIVE: 2.0,
                MarkerCategory.EMPATHY: 2.5,
                MarkerCategory.SUPPORT: 2.0,
                MarkerCategory.CONFLICT_RESOLUTION: 1.5,
                MarkerCategory.MANIPULATION: -2.0,
                MarkerCategory.GASLIGHTING: -3.0,
                MarkerCategory.EMOTIONAL_ABUSE: -2.5,
                MarkerCategory.FRAUD: -3.0
            },
            inverse_scale=True  # Positive Marker erhöhen Score
        )
        
        # Fraud-Wahrscheinlichkeit
        self.models["fraud_probability"] = ScoringModel(
            id="fraud_probability",
            name="Fraud-Wahrscheinlichkeit",
            type=ScoreType.FRAUD_PROBABILITY,
            description="Wahrscheinlichkeit für Betrug/Scam",
            category_weights={
                MarkerCategory.FRAUD: 3.0,
                MarkerCategory.FINANCIAL_ABUSE: 2.5,
                MarkerCategory.LOVE_BOMBING: 1.5,
                MarkerCategory.MANIPULATION: 1.0,
                MarkerCategory.POSITIVE: -0.5,
                MarkerCategory.EMPATHY: -0.3
            },
            thresholds={
                "critical": 7.0,
                "warning": 5.0,
                "normal": 3.0,
                "low": 1.5
            }
        )
        
        # Kommunikationsqualität
        self.models["communication_quality"] = ScoringModel(
            id="communication_quality",
            name="Kommunikationsqualität",
            type=ScoreType.COMMUNICATION_QUALITY,
            description="Qualität der Kommunikation",
            category_weights={
                MarkerCategory.POSITIVE: 1.5,
                MarkerCategory.EMPATHY: 2.0,
                MarkerCategory.SUPPORT: 1.5,
                MarkerCategory.CONFLICT_RESOLUTION: 2.0,
                MarkerCategory.BOUNDARY_SETTING: 1.0,
                MarkerCategory.SELF_CARE: 0.5,
                MarkerCategory.MANIPULATION: -1.5,
                MarkerCategory.GASLIGHTING: -2.0,
                MarkerCategory.EMOTIONAL_ABUSE: -2.0
            },
            inverse_scale=True
        )
    
    def calculate_scores(
        self,
        chunks: List[TextChunk],
        matches: List[MarkerMatch],
        models: Optional[List[str]] = None
    ) -> ScoringResult:
        """Berechnet Scores für gegebene Chunks und Matches.
        
        Args:
            chunks: Liste von Text-Chunks
            matches: Liste von Marker-Matches
            models: Spezifische Modelle zur Verwendung (None = alle)
            
        Returns:
            ScoringResult mit allen berechneten Scores
        """
        start_time = time.time()
        result = ScoringResult()
        
        # Wähle Modelle
        active_models = self._get_active_models(models)
        
        # Gruppiere Matches nach Chunk
        matches_by_chunk = self._group_matches_by_chunk(matches)
        
        # Berechne Scores pro Chunk
        for chunk in chunks:
            chunk_matches = matches_by_chunk.get(chunk.id, [])
            
            for model in active_models:
                chunk_score = self._calculate_chunk_score(
                    chunk,
                    chunk_matches,
                    model
                )
                result.chunk_scores.append(chunk_score)
        
        # Aggregiere Scores
        result.aggregated_scores = self._aggregate_scores(
            result.chunk_scores,
            active_models
        )
        
        # Berechne Speaker-Scores
        result.speaker_scores = self._calculate_speaker_scores(
            chunks,
            result.chunk_scores
        )
        
        # Erstelle Timeline
        result.timeline = self._create_timeline(result.chunk_scores)
        
        # Generiere Alerts
        result.alerts = self._generate_alerts(result.aggregated_scores)
        
        # Erstelle Summary
        result.summary = self._create_summary(result)
        
        result.processing_time = time.time() - start_time
        logger.info(f"Scoring abgeschlossen in {result.processing_time:.2f}s")
        
        return result
    
    def _calculate_chunk_score(
        self,
        chunk: TextChunk,
        matches: List[MarkerMatch],
        model: ScoringModel
    ) -> ChunkScore:
        """Berechnet Score für einen einzelnen Chunk."""
        raw_score = 0.0
        contributing_markers = []
        
        # Berechne gewichteten Score basierend auf Matches
        for match in matches:
            if match.category in model.category_weights:
                weight = model.category_weights[match.category]
                severity_mult = model.severity_multipliers.get(
                    match.severity,
                    1.0
                )
                
                # Marker-spezifisches Gewicht aus Metadaten
                marker_weight = match.metadata.get('weight', 1.0)
                
                contribution = weight * severity_mult * marker_weight * match.confidence
                raw_score += contribution
                
                contributing_markers.append({
                    'marker_id': match.marker_id,
                    'marker_name': match.marker_name,
                    'category': match.category.value,
                    'severity': match.severity.value,
                    'contribution': contribution,
                    'confidence': match.confidence
                })
        
        # Normalisiere Score
        normalized_score = self._normalize_score(raw_score, model, chunk.word_count)
        
        # Erstelle ChunkScore
        return ChunkScore(
            chunk_id=chunk.id,
            model_id=model.id,
            score_type=model.type,
            raw_score=raw_score,
            normalized_score=normalized_score,
            contributing_markers=contributing_markers,
            confidence=self._calculate_confidence(matches, model),
            timestamp=chunk.timestamp,
            metadata={
                'word_count': chunk.word_count,
                'marker_count': len(matches)
            }
        )
    
    def _normalize_score(
        self,
        raw_score: float,
        model: ScoringModel,
        word_count: int
    ) -> float:
        """Normalisiert einen Raw-Score auf die 1-10 Skala."""
        # Normalisiere basierend auf Wortanzahl
        if word_count > 0:
            normalized = (raw_score / word_count) * model.normalization_factor
        else:
            normalized = 0.0
        
        # Inverse Skalierung wenn nötig
        if model.inverse_scale:
            # Bei inverser Skala: Negative Werte = hoher Score
            if normalized < 0:
                score = model.scale_max + (normalized / 10)  # Skaliere negative Werte
            else:
                score = model.scale_max - (normalized * 2)  # Positive reduzieren Score
        else:
            # Normale Skala: Positive Werte = hoher Score
            score = model.scale_min + (normalized * 2)
        
        # Begrenze auf Min/Max
        return max(model.scale_min, min(model.scale_max, score))
    
    def _calculate_confidence(
        self,
        matches: List[MarkerMatch],
        model: ScoringModel
    ) -> float:
        """Berechnet Konfidenz des Scores basierend auf Matches."""
        if not matches:
            return 0.5  # Niedrige Konfidenz ohne Matches
        
        # Durchschnittliche Match-Konfidenz
        avg_confidence = sum(m.confidence for m in matches) / len(matches)
        
        # Anzahl relevanter Matches
        relevant_matches = [
            m for m in matches 
            if m.category in model.category_weights
        ]
        
        # Konfidenz steigt mit Anzahl relevanter Matches
        count_factor = min(1.0, len(relevant_matches) / 10)
        
        return avg_confidence * 0.7 + count_factor * 0.3
    
    def _aggregate_scores(
        self,
        chunk_scores: List[ChunkScore],
        models: List[ScoringModel]
    ) -> Dict[str, AggregatedScore]:
        """Aggregiert Chunk-Scores zu Gesamt-Scores."""
        aggregated = {}
        
        for model in models:
            model_scores = [
                cs for cs in chunk_scores 
                if cs.model_id == model.id
            ]
            
            if not model_scores:
                continue
            
            scores = [cs.normalized_score for cs in model_scores]
            
            # Berechne Trend
            trend, trend_strength = self._calculate_trend(scores)
            
            # Score-Verteilung
            distribution = self._calculate_distribution(scores)
            
            # Top Marker
            all_markers = []
            for cs in model_scores:
                all_markers.extend(cs.contributing_markers)
            
            marker_counts = defaultdict(int)
            for marker in all_markers:
                marker_counts[marker['marker_name']] += 1
            
            top_markers = [
                {'name': name, 'count': count}
                for name, count in sorted(
                    marker_counts.items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:5]
            ]
            
            aggregated[model.type.value] = AggregatedScore(
                model_id=model.id,
                score_type=model.type,
                average_score=np.mean(scores),
                min_score=min(scores),
                max_score=max(scores),
                trend=trend,
                trend_strength=trend_strength,
                chunk_count=len(model_scores),
                distribution=distribution,
                top_markers=top_markers
            )
        
        return aggregated
    
    def _calculate_trend(
        self,
        scores: List[float]
    ) -> Tuple[str, float]:
        """Berechnet Trend aus einer Score-Serie."""
        if len(scores) < 3:
            return "stable", 0.0
        
        # Einfache lineare Regression
        x = np.arange(len(scores))
        y = np.array(scores)
        
        # Berechne Steigung
        slope = np.polyfit(x, y, 1)[0]
        
        # Normalisiere Steigung
        normalized_slope = slope / np.mean(scores) if np.mean(scores) > 0 else 0
        
        if normalized_slope > 0.1:
            return "improving", min(1.0, normalized_slope)
        elif normalized_slope < -0.1:
            return "declining", max(-1.0, normalized_slope)
        else:
            return "stable", normalized_slope
    
    def _calculate_distribution(
        self,
        scores: List[float]
    ) -> Dict[str, int]:
        """Berechnet Score-Verteilung."""
        distribution = {
            "1-2": 0,
            "3-4": 0,
            "5-6": 0,
            "7-8": 0,
            "9-10": 0
        }
        
        for score in scores:
            if score <= 2:
                distribution["1-2"] += 1
            elif score <= 4:
                distribution["3-4"] += 1
            elif score <= 6:
                distribution["5-6"] += 1
            elif score <= 8:
                distribution["7-8"] += 1
            else:
                distribution["9-10"] += 1
        
        return distribution
    
    def _calculate_speaker_scores(
        self,
        chunks: List[TextChunk],
        chunk_scores: List[ChunkScore]
    ) -> Dict[str, Dict[str, AggregatedScore]]:
        """Berechnet Scores pro Sprecher."""
        speaker_scores = defaultdict(lambda: defaultdict(list))
        
        # Sammle Scores pro Sprecher
        for chunk, scores in zip(chunks, chunk_scores):
            if chunk.speaker:
                speaker_name = chunk.speaker.name
                for score in [s for s in chunk_scores if s.chunk_id == chunk.id]:
                    speaker_scores[speaker_name][score.model_id].append(score)
        
        # Aggregiere pro Sprecher
        result = {}
        for speaker, model_scores in speaker_scores.items():
            result[speaker] = {}
            for model_id, scores in model_scores.items():
                model = self.models[model_id]
                result[speaker][model.type.value] = self._aggregate_scores(
                    scores,
                    [model]
                )[model.type.value]
        
        return result
    
    def _create_timeline(
        self,
        chunk_scores: List[ChunkScore]
    ) -> List[Dict[str, Any]]:
        """Erstellt Timeline der Score-Entwicklung."""
        timeline = []
        
        # Gruppiere nach Zeitstempel
        scores_by_time = defaultdict(lambda: defaultdict(list))
        
        for score in chunk_scores:
            if score.timestamp:
                # Runde auf Stunde
                hour = score.timestamp.replace(minute=0, second=0, microsecond=0)
                scores_by_time[hour][score.model_id].append(score.normalized_score)
        
        # Erstelle Timeline-Einträge
        for timestamp in sorted(scores_by_time.keys()):
            entry = {
                'timestamp': timestamp.isoformat(),
                'scores': {}
            }
            
            for model_id, scores in scores_by_time[timestamp].items():
                model = self.models[model_id]
                entry['scores'][model.type.value] = {
                    'average': np.mean(scores),
                    'min': min(scores),
                    'max': max(scores),
                    'count': len(scores)
                }
            
            timeline.append(entry)
        
        return timeline
    
    def _generate_alerts(
        self,
        aggregated_scores: Dict[str, AggregatedScore]
    ) -> List[Dict[str, Any]]:
        """Generiert Alerts basierend auf Score-Schwellenwerten."""
        alerts = []
        
        for score_type, agg_score in aggregated_scores.items():
            model = self.models[agg_score.model_id]
            
            # Prüfe Schwellenwerte
            for level, threshold in model.thresholds.items():
                if model.inverse_scale:
                    # Bei inverser Skala: Niedrige Scores sind schlecht
                    if agg_score.average_score <= threshold and level in ['warning', 'critical']:
                        alerts.append({
                            'level': level,
                            'type': score_type,
                            'message': f"{model.name} ist {level}: {agg_score.average_score:.1f}",
                            'score': agg_score.average_score,
                            'threshold': threshold
                        })
                else:
                    # Normale Skala: Hohe Scores sind schlecht
                    if agg_score.average_score >= threshold and level in ['warning', 'critical']:
                        alerts.append({
                            'level': level,
                            'type': score_type,
                            'message': f"{model.name} ist {level}: {agg_score.average_score:.1f}",
                            'score': agg_score.average_score,
                            'threshold': threshold
                        })
        
        return sorted(alerts, key=lambda x: x['level'] == 'critical', reverse=True)
    
    def _create_summary(self, result: ScoringResult) -> Dict[str, Any]:
        """Erstellt Zusammenfassung der wichtigsten Erkenntnisse."""
        summary = {
            'total_chunks_analyzed': len(result.chunk_scores),
            'models_used': list(result.aggregated_scores.keys()),
            'critical_alerts': len([a for a in result.alerts if a['level'] == 'critical']),
            'warning_alerts': len([a for a in result.alerts if a['level'] == 'warning']),
            'speaker_count': len(result.speaker_scores),
            'key_insights': []
        }
        
        # Wichtigste Erkenntnisse
        for score_type, agg_score in result.aggregated_scores.items():
            insight = {
                'type': score_type,
                'score': agg_score.average_score,
                'trend': agg_score.trend,
                'interpretation': self._interpret_score(score_type, agg_score.average_score)
            }
            summary['key_insights'].append(insight)
        
        return summary
    
    def _interpret_score(self, score_type: str, score: float) -> str:
        """Interpretiert einen Score verbal."""
        model = next((m for m in self.models.values() if m.type.value == score_type), None)
        if not model:
            return "Unbekannt"
        
        if model.inverse_scale:
            if score >= 8:
                return "Ausgezeichnet"
            elif score >= 6:
                return "Gut"
            elif score >= 4:
                return "Durchschnittlich"
            elif score >= 2:
                return "Problematisch"
            else:
                return "Kritisch"
        else:
            if score <= 2:
                return "Sehr niedrig"
            elif score <= 4:
                return "Niedrig"
            elif score <= 6:
                return "Mittel"
            elif score <= 8:
                return "Hoch"
            else:
                return "Sehr hoch"
    
    def _get_active_models(self, model_ids: Optional[List[str]] = None) -> List[ScoringModel]:
        """Gibt aktive Scoring-Modelle zurück."""
        if model_ids:
            return [self.models[mid] for mid in model_ids if mid in self.models and self.models[mid].active]
        else:
            return [m for m in self.models.values() if m.active]
    
    def _group_matches_by_chunk(self, matches: List[MarkerMatch]) -> Dict[str, List[MarkerMatch]]:
        """Gruppiert Matches nach Chunk-ID."""
        grouped = defaultdict(list)
        for match in matches:
            grouped[match.chunk_id].append(match)
        return grouped
    
    def add_custom_model(self, model: ScoringModel):
        """Fügt ein benutzerdefiniertes Scoring-Modell hinzu."""
        self.models[model.id] = model
        logger.info(f"Custom Scoring-Modell '{model.name}' hinzugefügt")
    
    def get_model(self, model_id: str) -> Optional[ScoringModel]:
        """Gibt ein spezifisches Scoring-Modell zurück."""
        return self.models.get(model_id)

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/scoring_engine.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/semantic_grabber_loader.py
# ====================================================================


import yaml
from typing import List, Dict

class SemanticGrabberLibrary:
    def __init__(self, yaml_path: str):
        with open(yaml_path, 'r', encoding='utf-8') as f:
            self.library = yaml.safe_load(f)

    def get_grabber_ids(self) -> List[str]:
        return list(self.library.keys())

    def get_patterns_for_id(self, grabber_id: str) -> List[str]:
        return self.library.get(grabber_id, {}).get('patterns', [])

    def get_description_for_id(self, grabber_id: str) -> str:
        return self.library.get(grabber_id, {}).get('beschreibung', '')

    def match_text(self, text: str, threshold: float = 0.6) -> List[str]:
        """ Dummy logic – to be replaced with semantic similarity models """
        matches = []
        for grabber_id, data in self.library.items():
            for pattern in data.get('patterns', []):
                if pattern.lower() in text.lower():
                    matches.append(grabber_id)
                    break
        return matches


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/semantic_grabber_loader.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/text_chunker.py
# ====================================================================

"""Text Chunker für intelligente Text-Segmentierung."""

import re
import time
from datetime import datetime, timedelta
from typing import List, Optional, Tuple, Dict, Any
import logging
from uuid import uuid4

from .chunk_models import (
    TextChunk, ChunkType, Speaker, ChunkingConfig, ChunkingResult
)

logger = logging.getLogger(__name__)


class TextChunker:
    """Segmentiert Texte intelligent in analysierbare Chunks."""
    
    # Regex-Patterns für verschiedene Chat-Formate
    WHATSAPP_PATTERN = re.compile(
        r'(\d{1,2}[./]\d{1,2}[./]\d{2,4},?\s*\d{1,2}:\d{2}(?:\s*[AP]M)?)\s*-\s*([^:]+):\s*(.*)',
        re.MULTILINE
    )
    
    TELEGRAM_PATTERN = re.compile(
        r'\[(\d{1,2}\.\d{1,2}\.\d{2,4}\s+\d{1,2}:\d{2}(?::\d{2})?)\]\s*([^:]+):\s*(.*)',
        re.MULTILINE
    )
    
    GENERIC_PATTERN = re.compile(
        r'^([^:]+):\s*(.*)',
        re.MULTILINE
    )
    
    TIMESTAMP_PATTERNS = [
        # ISO format
        re.compile(r'\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}'),
        # Deutsch
        re.compile(r'\d{1,2}\.\d{1,2}\.\d{2,4}\s+\d{1,2}:\d{2}'),
        # US
        re.compile(r'\d{1,2}/\d{1,2}/\d{2,4}\s+\d{1,2}:\d{2}'),
    ]
    
    def __init__(self, config: Optional[ChunkingConfig] = None):
        self.config = config or ChunkingConfig()
        self._speaker_map: Dict[str, Speaker] = {}
        
    def chunk_text(
        self, 
        text: str, 
        format_hint: Optional[str] = None
    ) -> ChunkingResult:
        """Hauptmethode zum Chunking von Text.
        
        Args:
            text: Der zu segmentierende Text
            format_hint: Hinweis auf Format (whatsapp, telegram, etc.)
            
        Returns:
            ChunkingResult mit allen Chunks und Metadaten
        """
        start_time = time.time()
        result = ChunkingResult()
        
        try:
            # Format erkennen
            chat_format = format_hint or self._detect_format(text)
            logger.info(f"Erkanntes Format: {chat_format}")
            
            # Parse Messages
            messages = self._parse_messages(text, chat_format)
            
            if not messages:
                # Fallback: Als einzelnen Chunk behandeln
                chunk = self._create_chunk(
                    text=text,
                    chunk_type=ChunkType.PARAGRAPH,
                    start_pos=0,
                    end_pos=len(text)
                )
                result.chunks = [chunk]
            else:
                # Chunks aus Messages erstellen
                result.chunks = self._create_chunks_from_messages(messages)
            
            # Speakers sammeln
            result.speakers = list(self._speaker_map.values())
            
            # Statistiken
            result.statistics = self._calculate_statistics(result.chunks)
            
        except Exception as e:
            logger.error(f"Fehler beim Chunking: {e}")
            result.errors.append(str(e))
        
        result.processing_time = time.time() - start_time
        return result
    
    def _detect_format(self, text: str) -> str:
        """Erkennt das Chat-Format automatisch."""
        # Teste verschiedene Patterns
        if self.WHATSAPP_PATTERN.search(text):
            return "whatsapp"
        elif self.TELEGRAM_PATTERN.search(text):
            return "telegram"
        elif self.GENERIC_PATTERN.search(text):
            return "generic"
        else:
            return "plain"
    
    def _parse_messages(
        self, 
        text: str, 
        format_type: str
    ) -> List[Dict[str, Any]]:
        """Parst Messages aus dem Text basierend auf Format."""
        messages = []
        
        if format_type == "whatsapp":
            pattern = self.WHATSAPP_PATTERN
        elif format_type == "telegram":
            pattern = self.TELEGRAM_PATTERN
        elif format_type == "generic":
            pattern = self.GENERIC_PATTERN
        else:
            # Plain text - keine Messages
            return []
        
        last_end = 0
        
        for match in pattern.finditer(text):
            if format_type in ["whatsapp", "telegram"]:
                timestamp_str, speaker, message = match.groups()
                timestamp = self._parse_timestamp(timestamp_str)
            else:
                speaker, message = match.groups()
                timestamp = None
            
            # Multi-line Messages zusammenführen
            start = match.start()
            end = match.end()
            
            # Finde nächste Message oder Ende
            next_match = pattern.search(text, end)
            if next_match:
                message_end = next_match.start()
            else:
                message_end = len(text)
            
            # Erweitere Message bis zur nächsten
            full_message = text[match.end():message_end].strip()
            if full_message:
                message = message + "\n" + full_message
            
            messages.append({
                'speaker': speaker.strip(),
                'text': message.strip(),
                'timestamp': timestamp,
                'start_pos': start,
                'end_pos': message_end
            })
            
            last_end = message_end
        
        return messages
    
    def _parse_timestamp(self, timestamp_str: str) -> Optional[datetime]:
        """Versucht einen Zeitstempel zu parsen."""
        # Verschiedene Formate probieren
        formats = [
            "%d.%m.%Y %H:%M",
            "%d.%m.%Y %H:%M:%S",
            "%d/%m/%Y %H:%M",
            "%m/%d/%Y %H:%M",
            "%d.%m.%y, %H:%M",
            "%d/%m/%y, %H:%M",
            "%Y-%m-%d %H:%M:%S",
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(timestamp_str.strip(), fmt)
            except ValueError:
                continue
        
        logger.warning(f"Konnte Zeitstempel nicht parsen: {timestamp_str}")
        return None
    
    def _create_chunks_from_messages(
        self, 
        messages: List[Dict[str, Any]]
    ) -> List[TextChunk]:
        """Erstellt Chunks aus geparsten Messages."""
        chunks = []
        current_chunk_messages = []
        current_speaker = None
        last_timestamp = None
        
        for i, msg in enumerate(messages):
            speaker_name = msg['speaker']
            timestamp = msg['timestamp']
            
            # Entscheide ob neuer Chunk nötig
            need_new_chunk = False
            
            # Bei Sprecherwechsel
            if self.config.chunk_by_speaker and speaker_name != current_speaker:
                need_new_chunk = True
            
            # Bei Zeitsprung
            if (self.config.chunk_by_time and 
                last_timestamp and timestamp and
                (timestamp - last_timestamp).total_seconds() > self.config.time_gap_minutes * 60):
                need_new_chunk = True
            
            # Bei Größenlimit
            current_size = sum(len(m['text']) for m in current_chunk_messages)
            if current_size + len(msg['text']) > self.config.max_chunk_size:
                need_new_chunk = True
            
            # Erstelle neuen Chunk wenn nötig
            if need_new_chunk and current_chunk_messages:
                chunk = self._create_chunk_from_messages(current_chunk_messages)
                chunks.append(chunk)
                current_chunk_messages = []
            
            current_chunk_messages.append(msg)
            current_speaker = speaker_name
            last_timestamp = timestamp
        
        # Letzten Chunk erstellen
        if current_chunk_messages:
            chunk = self._create_chunk_from_messages(current_chunk_messages)
            chunks.append(chunk)
        
        # Chunk-IDs verlinken
        for i in range(len(chunks)):
            if i > 0:
                chunks[i].previous_chunk_id = chunks[i-1].id
            if i < len(chunks) - 1:
                chunks[i].next_chunk_id = chunks[i+1].id
        
        return chunks
    
    def _create_chunk_from_messages(
        self, 
        messages: List[Dict[str, Any]]
    ) -> TextChunk:
        """Erstellt einen Chunk aus einer Liste von Messages."""
        # Text zusammenführen
        texts = []
        for msg in messages:
            if msg['timestamp']:
                texts.append(f"[{msg['timestamp']}] {msg['speaker']}: {msg['text']}")
            else:
                texts.append(f"{msg['speaker']}: {msg['text']}")
        
        combined_text = "\n".join(texts)
        
        # Speaker bestimmen
        speakers = list(set(msg['speaker'] for msg in messages))
        if len(speakers) == 1:
            speaker = self._get_or_create_speaker(speakers[0])
        else:
            speaker = None  # Multiple speakers in chunk
        
        # Timestamps
        timestamps = [msg['timestamp'] for msg in messages if msg['timestamp']]
        timestamp = timestamps[0] if timestamps else None
        
        return self._create_chunk(
            text=combined_text,
            chunk_type=ChunkType.CONVERSATION if len(speakers) > 1 else ChunkType.MESSAGE,
            speaker=speaker,
            timestamp=timestamp,
            start_pos=messages[0]['start_pos'],
            end_pos=messages[-1]['end_pos'],
            metadata={
                'message_count': len(messages),
                'speakers': speakers
            }
        )
    
    def _create_chunk(
        self,
        text: str,
        chunk_type: ChunkType,
        start_pos: int,
        end_pos: int,
        speaker: Optional[Speaker] = None,
        timestamp: Optional[datetime] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> TextChunk:
        """Erstellt einen einzelnen Chunk."""
        chunk_id = f"chunk_{uuid4().hex[:8]}"
        
        # Text normalisieren wenn gewünscht
        if self.config.normalize_whitespace:
            text = ' '.join(text.split())
        
        return TextChunk(
            id=chunk_id,
            type=chunk_type,
            text=text,
            speaker=speaker,
            timestamp=timestamp,
            start_pos=start_pos,
            end_pos=end_pos,
            metadata=metadata or {}
        )
    
    def _get_or_create_speaker(self, name: str) -> Speaker:
        """Holt oder erstellt einen Speaker."""
        if name not in self._speaker_map:
            speaker_id = f"speaker_{len(self._speaker_map) + 1}"
            self._speaker_map[name] = Speaker(
                id=speaker_id,
                name=name
            )
        return self._speaker_map[name]
    
    def _calculate_statistics(self, chunks: List[TextChunk]) -> Dict[str, Any]:
        """Berechnet Statistiken über die Chunks."""
        if not chunks:
            return {}
        
        total_words = sum(c.word_count for c in chunks)
        total_chars = sum(c.char_count for c in chunks)
        
        # Zeitspanne
        timestamps = [c.timestamp for c in chunks if c.timestamp]
        if timestamps:
            time_span = max(timestamps) - min(timestamps)
            time_span_hours = time_span.total_seconds() / 3600
        else:
            time_span_hours = 0
        
        # Speaker-Statistiken
        speaker_stats = {}
        for chunk in chunks:
            if chunk.speaker:
                speaker_name = chunk.speaker.name
                if speaker_name not in speaker_stats:
                    speaker_stats[speaker_name] = {
                        'chunks': 0,
                        'words': 0,
                        'chars': 0
                    }
                speaker_stats[speaker_name]['chunks'] += 1
                speaker_stats[speaker_name]['words'] += chunk.word_count
                speaker_stats[speaker_name]['chars'] += chunk.char_count
        
        return {
            'total_chunks': len(chunks),
            'total_words': total_words,
            'total_chars': total_chars,
            'avg_chunk_size': total_chars / len(chunks) if chunks else 0,
            'time_span_hours': time_span_hours,
            'speaker_stats': speaker_stats,
            'chunk_types': {
                t.value: sum(1 for c in chunks if c.type == t)
                for t in ChunkType
            }
        }

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/text_chunker.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/time_series_aggregator.py
# ====================================================================

"""Time Series Aggregator für die Aggregation von Scores und Marker-Daten über Zeit."""

import logging
import time
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime, timedelta
from collections import defaultdict
import pandas as pd
import numpy as np

from .aggregation_models import (
    AggregationConfig, AggregationPeriod, AggregationResult,
    TimeSeriesData, TimeSeriesPoint, HeatmapData, ComparisonData
)
from ..scoring.score_models import ChunkScore, ScoreType
from ..matcher.marker_models import MarkerMatch, MarkerCategory

logger = logging.getLogger(__name__)


class TimeSeriesAggregator:
    """Aggregiert Scores und Marker-Daten über verschiedene Zeitfenster."""
    
    def __init__(self, config: Optional[AggregationConfig] = None):
        self.config = config or AggregationConfig()
    
    def aggregate_data(
        self,
        chunk_scores: List[ChunkScore],
        marker_matches: List[MarkerMatch],
        period: Optional[AggregationPeriod] = None
    ) -> AggregationResult:
        """Hauptmethode zur Aggregation von Daten.
        
        Args:
            chunk_scores: Liste von Chunk-Scores
            marker_matches: Liste von Marker-Matches
            period: Aggregationszeitraum (überschreibt config)
            
        Returns:
            AggregationResult mit allen aggregierten Daten
        """
        start_time = time.time()
        result = AggregationResult()
        
        # Verwende spezifizierten oder konfigurierten Zeitraum
        agg_period = period or self.config.period
        
        try:
            # Aggregiere Scores
            score_series = self._aggregate_scores(chunk_scores, agg_period)
            result.time_series.update(score_series)
            
            # Aggregiere Marker-Counts
            marker_series = self._aggregate_markers(marker_matches, agg_period)
            result.time_series.update(marker_series)
            
            # Erstelle Heatmaps
            result.heatmaps = self._create_heatmaps(
                chunk_scores,
                marker_matches,
                agg_period
            )
            
            # Erstelle Vergleiche
            result.comparisons = self._create_comparisons(result.time_series)
            
            # Berechne Summary Statistics
            result.summary_statistics = self._calculate_summary_stats(result.time_series)
            
            # Erstelle Export DataFrame
            result.export_data = self._create_export_dataframe(result.time_series)
            
        except Exception as e:
            logger.error(f"Fehler bei Aggregation: {e}")
            raise
        
        result.processing_time = time.time() - start_time
        logger.info(f"Aggregation abgeschlossen in {result.processing_time:.2f}s")
        
        return result
    
    def _aggregate_scores(
        self,
        chunk_scores: List[ChunkScore],
        period: AggregationPeriod
    ) -> Dict[str, TimeSeriesData]:
        """Aggregiert Chunk-Scores über Zeit."""
        series_dict = {}
        
        # Gruppiere Scores nach Typ
        scores_by_type = defaultdict(list)
        for score in chunk_scores:
            if score.timestamp:
                scores_by_type[score.score_type.value].append(score)
        
        # Erstelle Zeitreihe für jeden Score-Typ
        for score_type, scores in scores_by_type.items():
            if not scores:
                continue
            
            # Sortiere nach Zeit
            sorted_scores = sorted(scores, key=lambda s: s.timestamp)
            
            # Bestimme Zeitbereich
            start_time = sorted_scores[0].timestamp
            end_time = sorted_scores[-1].timestamp
            
            # Erstelle Zeitfenster
            time_windows = self._create_time_windows(start_time, end_time, period)
            
            # Aggregiere in Zeitfenster
            data_points = []
            for window_start, window_end in time_windows:
                window_scores = [
                    s for s in sorted_scores
                    if window_start <= s.timestamp < window_end
                ]
                
                if window_scores or self.config.include_zero_periods:
                    point = self._create_score_point(
                        window_scores,
                        window_start,
                        window_end,
                        score_type
                    )
                    data_points.append(point)
            
            # Glätte Daten wenn gewünscht
            if self.config.smooth_data and len(data_points) > self.config.smoothing_window:
                data_points = self._smooth_data_points(data_points, score_type)
            
            # Erstelle TimeSeriesData
            series = TimeSeriesData(
                series_id=f"scores_{score_type}",
                name=f"{score_type.replace('_', ' ').title()} Scores",
                metric_type="score",
                period=period,
                data_points=data_points,
                statistics=self._calculate_series_statistics(data_points, score_type)
            )
            
            series_dict[f"scores_{score_type}"] = series
        
        return series_dict
    
    def _aggregate_markers(
        self,
        marker_matches: List[MarkerMatch],
        period: AggregationPeriod
    ) -> Dict[str, TimeSeriesData]:
        """Aggregiert Marker-Matches über Zeit."""
        series_dict = {}
        
        # Filtere Matches mit Timestamps
        timed_matches = [m for m in marker_matches if m.timestamp]
        if not timed_matches:
            return series_dict
        
        # Sortiere nach Zeit
        sorted_matches = sorted(timed_matches, key=lambda m: m.timestamp)
        
        # Zeitbereich
        start_time = sorted_matches[0].timestamp
        end_time = sorted_matches[-1].timestamp
        
        # Zeitfenster
        time_windows = self._create_time_windows(start_time, end_time, period)
        
        # Aggregiere Marker-Counts gesamt
        total_points = []
        category_series = defaultdict(list)
        
        for window_start, window_end in time_windows:
            window_matches = [
                m for m in sorted_matches
                if window_start <= m.timestamp < window_end
            ]
            
            # Gesamt-Counts
            total_point = TimeSeriesPoint(
                timestamp=window_start,
                period_start=window_start,
                period_end=window_end,
                values={"marker_count": len(window_matches)},
                counts={"total": len(window_matches)}
            )
            
            # Counts nach Kategorie
            category_counts = defaultdict(int)
            for match in window_matches:
                category_counts[match.category.value] += 1
            
            total_point.counts.update(category_counts)
            total_points.append(total_point)
            
            # Separate Serien pro Kategorie
            for category in MarkerCategory:
                cat_count = category_counts.get(category.value, 0)
                cat_point = TimeSeriesPoint(
                    timestamp=window_start,
                    period_start=window_start,
                    period_end=window_end,
                    values={"count": cat_count},
                    counts={category.value: cat_count}
                )
                category_series[category.value].append(cat_point)
        
        # Erstelle Gesamt-Serie
        series_dict["markers_total"] = TimeSeriesData(
            series_id="markers_total",
            name="Total Marker Count",
            metric_type="count",
            period=period,
            data_points=total_points,
            statistics=self._calculate_series_statistics(total_points, "marker_count")
        )
        
        # Erstelle Kategorie-Serien
        for category, points in category_series.items():
            series_dict[f"markers_{category}"] = TimeSeriesData(
                series_id=f"markers_{category}",
                name=f"{category.replace('_', ' ').title()} Markers",
                metric_type="count",
                period=period,
                data_points=points,
                statistics=self._calculate_series_statistics(points, "count")
            )
        
        return series_dict
    
    def _create_time_windows(
        self,
        start: datetime,
        end: datetime,
        period: AggregationPeriod
    ) -> List[Tuple[datetime, datetime]]:
        """Erstellt Zeitfenster für Aggregation."""
        windows = []
        current = start
        
        while current < end:
            if period == AggregationPeriod.HOURLY:
                window_end = current + timedelta(hours=1)
            elif period == AggregationPeriod.DAILY:
                window_end = current + timedelta(days=1)
            elif period == AggregationPeriod.WEEKLY:
                window_end = current + timedelta(weeks=1)
            elif period == AggregationPeriod.MONTHLY:
                # Nächster Monat
                if current.month == 12:
                    window_end = current.replace(year=current.year + 1, month=1)
                else:
                    window_end = current.replace(month=current.month + 1)
            elif period == AggregationPeriod.QUARTERLY:
                # Nächstes Quartal
                quarter = (current.month - 1) // 3
                if quarter == 3:
                    window_end = current.replace(year=current.year + 1, month=1)
                else:
                    window_end = current.replace(month=(quarter + 1) * 3 + 1)
            elif period == AggregationPeriod.YEARLY:
                window_end = current.replace(year=current.year + 1)
            elif period == AggregationPeriod.CUSTOM:
                hours = self.config.custom_period_hours or 24
                window_end = current + timedelta(hours=hours)
            else:
                window_end = current + timedelta(days=1)
            
            windows.append((current, window_end))
            current = window_end
        
        return windows
    
    def _create_score_point(
        self,
        scores: List[ChunkScore],
        start: datetime,
        end: datetime,
        score_type: str
    ) -> TimeSeriesPoint:
        """Erstellt einen aggregierten Score-Punkt."""
        point = TimeSeriesPoint(
            timestamp=start,
            period_start=start,
            period_end=end
        )
        
        if scores:
            values = [s.normalized_score for s in scores]
            point.values = {
                "mean": np.mean(values),
                "min": min(values),
                "max": max(values),
                "std": np.std(values) if len(values) > 1 else 0,
                "median": np.median(values)
            }
            point.counts = {
                "chunk_count": len(scores),
                "confidence_avg": np.mean([s.confidence for s in scores])
            }
        else:
            # Keine Daten für diesen Zeitraum
            point.values = {
                "mean": 0,
                "min": 0,
                "max": 0,
                "std": 0,
                "median": 0
            }
            point.counts = {"chunk_count": 0}
        
        return point
    
    def _smooth_data_points(
        self,
        points: List[TimeSeriesPoint],
        metric: str
    ) -> List[TimeSeriesPoint]:
        """Glättet Datenpunkte mit Moving Average."""
        if len(points) <= self.config.smoothing_window:
            return points
        
        # Extrahiere Werte
        values = [p.values.get("mean", 0) for p in points]
        
        # Berechne Moving Average
        smoothed = []
        window = self.config.smoothing_window
        
        for i in range(len(values)):
            start_idx = max(0, i - window // 2)
            end_idx = min(len(values), i + window // 2 + 1)
            window_values = values[start_idx:end_idx]
            smoothed.append(np.mean(window_values))
        
        # Update Points
        for i, point in enumerate(points):
            point.values["mean_smoothed"] = smoothed[i]
        
        return points
    
    def _create_heatmaps(
        self,
        chunk_scores: List[ChunkScore],
        marker_matches: List[MarkerMatch],
        period: AggregationPeriod
    ) -> Dict[str, HeatmapData]:
        """Erstellt Heatmap-Daten."""
        heatmaps = {}
        
        # Marker-Kategorie Heatmap über Zeit
        category_heatmap = self._create_category_timeline_heatmap(
            marker_matches,
            period
        )
        if category_heatmap:
            heatmaps["marker_categories"] = category_heatmap
        
        # Score-Vergleich Heatmap
        score_heatmap = self._create_score_comparison_heatmap(
            chunk_scores,
            period
        )
        if score_heatmap:
            heatmaps["score_comparison"] = score_heatmap
        
        return heatmaps
    
    def _create_category_timeline_heatmap(
        self,
        matches: List[MarkerMatch],
        period: AggregationPeriod
    ) -> Optional[HeatmapData]:
        """Erstellt Heatmap für Marker-Kategorien über Zeit."""
        timed_matches = [m for m in matches if m.timestamp]
        if not timed_matches:
            return None
        
        # Zeitfenster
        start = min(m.timestamp for m in timed_matches)
        end = max(m.timestamp for m in timed_matches)
        windows = self._create_time_windows(start, end, period)
        
        # Matrix aufbauen
        categories = list(MarkerCategory)
        matrix = []
        x_labels = []
        
        for window_start, window_end in windows:
            window_matches = [
                m for m in timed_matches
                if window_start <= m.timestamp < window_end
            ]
            
            # Zähle pro Kategorie
            row = []
            for category in categories:
                count = sum(1 for m in window_matches if m.category == category)
                row.append(count)
            
            matrix.append(row)
            x_labels.append(window_start.strftime("%Y-%m-%d %H:%M"))
        
        # Transponiere für bessere Darstellung
        matrix = list(map(list, zip(*matrix)))
        
        return HeatmapData(
            title="Marker Categories Over Time",
            x_labels=x_labels,
            y_labels=[cat.value for cat in categories],
            values=matrix,
            color_scale="YlOrRd"
        )
    
    def _create_score_comparison_heatmap(
        self,
        chunk_scores: List[ChunkScore],
        period: AggregationPeriod
    ) -> Optional[HeatmapData]:
        """Erstellt Heatmap für Score-Vergleiche."""
        if not chunk_scores:
            return None
        
        # Gruppiere nach Score-Typ und Speaker
        score_matrix = defaultdict(lambda: defaultdict(list))
        
        for score in chunk_scores:
            if score.timestamp:
                speaker = score.metadata.get("speaker", "Unknown")
                score_matrix[score.score_type.value][speaker].append(score.normalized_score)
        
        if not score_matrix:
            return None
        
        # Erstelle Matrix
        score_types = list(score_matrix.keys())
        speakers = list(set(speaker for scores in score_matrix.values() for speaker in scores))
        
        matrix = []
        for score_type in score_types:
            row = []
            for speaker in speakers:
                scores = score_matrix[score_type].get(speaker, [])
                avg_score = np.mean(scores) if scores else 0
                row.append(avg_score)
            matrix.append(row)
        
        return HeatmapData(
            title="Average Scores by Type and Speaker",
            x_labels=speakers,
            y_labels=score_types,
            values=matrix,
            color_scale="RdYlGn_r"  # Reversed: Red=bad, Green=good
        )
    
    def _create_comparisons(
        self,
        time_series: Dict[str, TimeSeriesData]
    ) -> List[ComparisonData]:
        """Erstellt Vergleiche zwischen Zeitreihen."""
        comparisons = []
        
        # Vergleiche Score-Typen
        score_series = [
            ts for name, ts in time_series.items()
            if name.startswith("scores_")
        ]
        
        if len(score_series) > 1:
            comparison = ComparisonData(
                comparison_type="score_types",
                series=score_series
            )
            
            # Berechne Korrelationen
            if len(score_series[0].data_points) > 3:
                correlation_matrix = self._calculate_correlations(score_series)
                comparison.correlation_matrix = correlation_matrix
            
            comparisons.append(comparison)
        
        return comparisons
    
    def _calculate_correlations(
        self,
        series_list: List[TimeSeriesData]
    ) -> List[List[float]]:
        """Berechnet Korrelationsmatrix zwischen Zeitreihen."""
        # Extrahiere Werte
        value_arrays = []
        for series in series_list:
            values = [p.values.get("mean", 0) for p in series.data_points]
            value_arrays.append(values)
        
        # Berechne Korrelationen
        n = len(value_arrays)
        corr_matrix = [[0.0] * n for _ in range(n)]
        
        for i in range(n):
            for j in range(n):
                if len(value_arrays[i]) == len(value_arrays[j]):
                    corr = np.corrcoef(value_arrays[i], value_arrays[j])[0, 1]
                    corr_matrix[i][j] = float(corr) if not np.isnan(corr) else 0.0
                else:
                    corr_matrix[i][j] = 0.0
        
        return corr_matrix
    
    def _calculate_series_statistics(
        self,
        points: List[TimeSeriesPoint],
        main_metric: str
    ) -> Dict[str, Any]:
        """Berechnet Statistiken für eine Zeitreihe."""
        if not points:
            return {}
        
        # Extrahiere Hauptmetrik
        if main_metric in ["marker_count", "count"]:
            values = [p.values.get(main_metric, p.values.get("count", 0)) for p in points]
        else:
            values = [p.values.get("mean", 0) for p in points]
        
        non_zero_values = [v for v in values if v > 0]
        
        stats = {
            "total_periods": len(points),
            "non_zero_periods": len(non_zero_values),
            "average": np.mean(values) if values else 0,
            "std_dev": np.std(values) if len(values) > 1 else 0,
            "min": min(values) if values else 0,
            "max": max(values) if values else 0,
            "sum": sum(values) if values else 0
        }
        
        # Trend
        if len(values) > 2:
            first_third = np.mean(values[:len(values)//3])
            last_third = np.mean(values[-len(values)//3:])
            if last_third > first_third * 1.1:
                stats["trend"] = "increasing"
            elif last_third < first_third * 0.9:
                stats["trend"] = "decreasing"
            else:
                stats["trend"] = "stable"
        
        return stats
    
    def _calculate_summary_stats(
        self,
        time_series: Dict[str, TimeSeriesData]
    ) -> Dict[str, Dict[str, float]]:
        """Berechnet zusammenfassende Statistiken."""
        summary = {}
        
        for name, series in time_series.items():
            if series.statistics:
                summary[name] = {
                    "average": series.statistics.get("average", 0),
                    "trend": series.statistics.get("trend", "unknown"),
                    "periods": series.statistics.get("total_periods", 0),
                    "active_periods": series.statistics.get("non_zero_periods", 0)
                }
        
        return summary
    
    def _create_export_dataframe(
        self,
        time_series: Dict[str, TimeSeriesData]
    ) -> pd.DataFrame:
        """Erstellt DataFrame für Export."""
        if not time_series:
            return pd.DataFrame()
        
        # Sammle alle Datenpunkte
        all_data = []
        
        for series_name, series in time_series.items():
            for point in series.data_points:
                row = {
                    'timestamp': point.timestamp,
                    'period_start': point.period_start,
                    'period_end': point.period_end,
                    'series': series_name,
                    'metric_type': series.metric_type
                }
                
                # Füge Werte hinzu
                for key, value in point.values.items():
                    row[f"{series_name}_{key}"] = value
                
                # Füge Counts hinzu
                for key, count in point.counts.items():
                    row[f"{series_name}_{key}_count"] = count
                
                all_data.append(row)
        
        df = pd.DataFrame(all_data)
        
        # Pivotiere für bessere Darstellung
        if not df.empty:
            # Gruppiere nach Timestamp
            pivot_data = {}
            for _, row in df.iterrows():
                ts = row['timestamp']
                if ts not in pivot_data:
                    pivot_data[ts] = {'timestamp': ts}
                
                # Kopiere relevante Werte
                for col in row.index:
                    if col not in ['timestamp', 'period_start', 'period_end', 'series', 'metric_type']:
                        pivot_data[ts][col] = row[col]
            
            df = pd.DataFrame(list(pivot_data.values()))
            df.set_index('timestamp', inplace=True)
            df.sort_index(inplace=True)
        
        return df

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/time_series_aggregator.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/_python/yaml2json.py
# ====================================================================

#!/usr/bin/env python3
"""
yaml2json.py  ·  WordThread Marker Engine Helper
------------------------------------------------
Wandelt alle .yaml|.yml-Dateien eines Verzeichnisses (rekursiv) oder eine
einzelne Datei in gleichnamige .json-Dateien um.

⤷   python yaml2json.py  markers/          # ganze Mappe
⤷   python yaml2json.py  markers/A_WZ.yaml # einzelne Datei

Optionen:
    --out DIR        Zielordner (default: <eingabepfad>/json_out)
    --schema FILE    JSON-Schema zur Validierung (optional)
    --indent N       Einrückung im Output-JSON (default: 2)
"""

import argparse, json, sys
from pathlib import Path
from datetime import datetime

try:
    from ruamel.yaml import YAML
except ModuleNotFoundError:
    sys.exit("❌  ruamel.yaml fehlt:  pip install ruamel.yaml")

try:
    import jsonschema
except ModuleNotFoundError:
    jsonschema = None  # Validierung nur, wenn verfügbar

yaml = YAML(typ="safe")
yaml.preserve_quotes = True   # kein Verlust bei Strings

def load_yaml(path: Path):
    """Liest alle YAML-Dokumente (---) aus einer Datei."""
    with path.open("r", encoding="utf-8") as f:
        return list(yaml.load_all(f))

def validate(obj, schema):
    """Optional: JSON-Schema-Validierung."""
    if jsonschema is None:
        raise RuntimeError("jsonschema nicht installiert")
    jsonschema.validate(obj, schema)

def convert_file(path: Path, out_dir: Path, schema, indent: int):
    docs = load_yaml(path)
    # Datei kann mehrere YAML-Dokumente enthalten → Liste serialisieren
    out_data = docs[0] if len(docs) == 1 else docs
    if schema:
        validate(out_data, schema)
    out_path = out_dir / (path.stem + ".json")
    out_path.write_text(
        json.dumps(out_data, ensure_ascii=False, indent=indent),
        encoding="utf-8"
    )
    print(f"✔  {path.name}  →  {out_path.relative_to(out_dir.parent)}")

def walk_inputs(input_path: Path):
    return (
        [input_path] if input_path.is_file()
        else sorted(input_path.rglob("*.yml")) + sorted(input_path.rglob("*.yaml"))
    )

def main():
    ap = argparse.ArgumentParser(description="YAML → JSON Converter")
    ap.add_argument("src", type=Path, help="Datei oder Verzeichnis mit YAML")
    ap.add_argument("--out", type=Path, help="Zielordner")
    ap.add_argument("--schema", type=Path, help="JSON-Schema (optional)")
    ap.add_argument("--indent", type=int, default=2)
    args = ap.parse_args()

    if not args.src.exists():
        sys.exit("❌  Eingabepfad existiert nicht.")

    out_dir = args.out or args.src.parent / "json_out"
    out_dir.mkdir(parents=True, exist_ok=True)

    schema = None
    if args.schema:
        if jsonschema is None:
            sys.exit("❌  jsonschema-Paket nicht installiert.")
        schema = json.loads(args.schema.read_text(encoding="utf-8"))

    files = walk_inputs(args.src)
    if not files:
        sys.exit("⚠️  Keine YAML-Dateien gefunden.")

    print(f"🏁  Starte Konvertierung ({len(files)} Dateien)  –  {datetime.now():%H:%M:%S}")
    for f in files:
        try:
            convert_file(f, out_dir, schema, args.indent)
        except Exception as e:
            print(f"⚠️  Fehler in {f}: {e}")

    print(f"✅  Fertig. JSON-Dateien liegen in  {out_dir.resolve()}")

if __name__ == "__main__":
    main()


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/_python/yaml2json.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/chunk_analysis/chunk_models.py
# ====================================================================

"""Datenmodelle für Text-Chunks."""

from typing import List, Optional, Dict, Any
from datetime import datetime
from pydantic import BaseModel, Field, validator
from enum import Enum


class ChunkType(str, Enum):
    """Typ eines Text-Chunks."""
    
    MESSAGE = "message"
    PARAGRAPH = "paragraph"
    CONVERSATION = "conversation"
    TIME_WINDOW = "time_window"
    SPEAKER_TURN = "speaker_turn"


class Speaker(BaseModel):
    """Information über einen Sprecher/Autor."""
    
    id: str = Field(..., description="Eindeutige ID des Sprechers")
    name: str = Field(..., description="Name oder Alias")
    metadata: Dict[str, Any] = Field(default_factory=dict)


class TextChunk(BaseModel):
    """Ein segmentierter Text-Abschnitt zur Analyse."""
    
    id: str = Field(..., description="Eindeutige Chunk-ID")
    type: ChunkType = Field(..., description="Art des Chunks")
    
    text: str = Field(..., description="Der eigentliche Text-Inhalt")
    original_text: Optional[str] = Field(None, description="Original-Text vor Bereinigung")
    
    speaker: Optional[Speaker] = Field(None, description="Sprecher/Autor")
    timestamp: Optional[datetime] = Field(None, description="Zeitstempel")
    
    start_pos: int = Field(..., description="Startposition im Gesamttext")
    end_pos: int = Field(..., description="Endposition im Gesamttext")
    
    word_count: int = Field(0, description="Anzahl Wörter")
    char_count: int = Field(0, description="Anzahl Zeichen")
    
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="Zusätzliche Metadaten (z.B. Plattform, Thread-ID)"
    )
    
    previous_chunk_id: Optional[str] = Field(None, description="ID des vorherigen Chunks")
    next_chunk_id: Optional[str] = Field(None, description="ID des nächsten Chunks")
    
    @validator('word_count', always=True)
    def calculate_word_count(cls, v, values):
        """Berechnet Wortanzahl wenn nicht gesetzt."""
        if v == 0 and 'text' in values:
            return len(values['text'].split())
        return v
    
    @validator('char_count', always=True)
    def calculate_char_count(cls, v, values):
        """Berechnet Zeichenanzahl wenn nicht gesetzt."""
        if v == 0 and 'text' in values:
            return len(values['text'])
        return v
    
    def get_context(self, words_before: int = 5, words_after: int = 5) -> str:
        """Gibt Kontext um eine Position im Chunk zurück."""
        words = self.text.split()
        return ' '.join(words[:words_before + words_after + 1])


class ChunkingConfig(BaseModel):
    """Konfiguration für das Text-Chunking."""
    
    # Größen-Limits
    max_chunk_size: int = Field(
        default=1000,
        description="Maximale Chunk-Größe in Zeichen"
    )
    min_chunk_size: int = Field(
        default=50,
        description="Minimale Chunk-Größe in Zeichen"
    )
    
    # Chunking-Strategien
    chunk_by_speaker: bool = Field(
        default=True,
        description="Bei Sprecherwechsel neuen Chunk beginnen"
    )
    chunk_by_time: bool = Field(
        default=True,
        description="Bei Zeitsprüngen neuen Chunk beginnen"
    )
    time_gap_minutes: int = Field(
        default=30,
        description="Zeitlücke in Minuten für neuen Chunk"
    )
    
    # Text-Verarbeitung
    preserve_formatting: bool = Field(
        default=False,
        description="Original-Formatierung beibehalten"
    )
    normalize_whitespace: bool = Field(
        default=True,
        description="Mehrfache Leerzeichen normalisieren"
    )
    
    # Overlap für Kontext
    overlap_size: int = Field(
        default=50,
        description="Überlappung zwischen Chunks in Zeichen"
    )
    
    # Metadaten
    extract_timestamps: bool = Field(
        default=True,
        description="Zeitstempel aus Text extrahieren"
    )
    extract_speakers: bool = Field(
        default=True,
        description="Sprecher aus Text extrahieren"
    )


class ChunkingResult(BaseModel):
    """Ergebnis des Chunking-Prozesses."""
    
    chunks: List[TextChunk] = Field(default_factory=list)
    total_chunks: int = Field(0)
    
    speakers: List[Speaker] = Field(
        default_factory=list,
        description="Alle identifizierten Sprecher"
    )
    
    statistics: Dict[str, Any] = Field(
        default_factory=dict,
        description="Statistiken über das Chunking"
    )
    
    errors: List[str] = Field(
        default_factory=list,
        description="Aufgetretene Fehler/Warnungen"
    )
    
    processing_time: float = Field(
        0.0,
        description="Verarbeitungszeit in Sekunden"
    )
    
    @validator('total_chunks', always=True)
    def calculate_total(cls, v, values):
        """Berechnet Gesamtzahl wenn nicht gesetzt."""
        if v == 0 and 'chunks' in values:
            return len(values['chunks'])
        return v
    
    def get_chunks_by_speaker(self, speaker_id: str) -> List[TextChunk]:
        """Gibt alle Chunks eines Sprechers zurück."""
        return [c for c in self.chunks if c.speaker and c.speaker.id == speaker_id]
    
    def get_chunks_in_timerange(
        self, 
        start: datetime, 
        end: datetime
    ) -> List[TextChunk]:
        """Gibt Chunks in einem Zeitbereich zurück."""
        return [
            c for c in self.chunks 
            if c.timestamp and start <= c.timestamp <= end
        ]

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/chunk_analysis/chunk_models.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/ohne_json/PYTHON_MARKER.py
# ====================================================================

"""
PYTHON_MARKER - Semantic Marker
Sehr grobe Heuristik: je mehr Self-Disclosure-Marker, desto höher.
"""

import re

class PYTHON_MARKER:
    """
    Sehr grobe Heuristik: je mehr Self-Disclosure-Marker, desto höher.
    """
    
    examples = [
    ]
    
    patterns = [
        re.compile(r"(muster.*wird.*ergänzt)", re.IGNORECASE)
    ]
    
    semantic_grabber_id = "AUTO_GENERATED"
    
    def match(self, text):
        """Prüft ob der Text zum Marker passt"""
        for pattern in self.patterns:
            if pattern.search(text):
                return True
        return False


# ====================================================================
# END OF FILE: ALL_NEWMARKER01/ohne_json/PYTHON_MARKER.py
# ====================================================================





# ====================================================================
# START OF FILE: ALL_NEWMARKER01/ohne_json/_python/PYTHON_MARKER.py
# ====================================================================

# ==============================  pipeline.py  ==============================
"""
Minimal-Pipeline zum Erkennen von Flirt-/Ambivalenz-Drift.
1.   Daten-Ingest  – Chat-Records ◁ JSON / DB / API
2.   Marker-Scoring – Regex-/ML-Tagger → marker_lookup
3.   Detector-Stage – ruft unsere Detector-Funktionen
4.   Reporting      – schreibt Flags in DB oder sendet Webhook
"""
from datetime import datetime, timedelta
from detector_utils import linear_slope
from detectors import (
    detect_sustained_contact,
    detect_secret_bonding,
    detect_adaptive_polarization,
    detect_flirt_dance_drift,
)

# --------------------------------------------------------------------------
# 1)  Simulierter Chat-Stream (chronologisch)
# --------------------------------------------------------------------------
CHAT_LOG = [
    {"id": "m1", "ts": datetime(2025, 7, 1, 9,  0), "text": "Du bist süß ;)",     },
    {"id": "m2", "ts": datetime(2025, 7, 1, 9,  1), "text": "Nicht verraten!",     },
    {"id": "m3", "ts": datetime(2025, 7, 2, 22, 0), "text": "Gute Nacht 🌙",       },
    {"id": "m4", "ts": datetime(2025, 7, 3,  8, 0), "text": "Dir zum ersten Mal …"},
    {"id": "m5", "ts": datetime(2025, 7, 3, 22, 0), "text": "Lösch den Chat 😊",   },
    {"id": "m6", "ts": datetime(2025, 7, 4, 20, 0), "text": "Du fehlst mir – egal",},
    {"id": "m7", "ts": datetime(2025, 7, 5, 21, 0), "text": "VIP-Tattoo? 😏",      },
]

# --------------------------------------------------------------------------
# 2)  VERY simple Marker-Tagging (RegEx-Demo) -------------------------------
#     → in der Praxis ersetzt du das durch dein echtes Marker-Engine-API.
# --------------------------------------------------------------------------
import re
PATTERNS = {
    "A_SOFT_FLIRT": re.compile(r"\b(süß|😉|😊|😏)\b", re.I),
    "S_MENTION_OF_SECRECY": re.compile(r"(nicht verraten|unter uns|lösch)", re.I),
    "C_ADAPTIVE_POLARIZATION": re.compile(r"du fehlst mir.*egal", re.I),
    "C_RAPID_SELF_DISCLOSURE": re.compile(r"zum ersten mal|noch nie erzählt", re.I),
}
marker_lookup = {}

for m in CHAT_LOG:
    found = [mid for mid, rx in PATTERNS.items() if rx.search(m["text"])]
    marker_lookup[m["id"]] = found

# --------------------------------------------------------------------------
# 3)  Detector-Stage --------------------------------------------------------
# --------------------------------------------------------------------------
detected = {}

ok, info = detect_sustained_contact(CHAT_LOG)
detected["C_SUSTAINED_CONTACT"] = ok

ok, info = detect_secret_bonding(CHAT_LOG, marker_lookup)
detected["C_SECRET_BONDING"] = ok

ok, info = detect_adaptive_polarization(CHAT_LOG, marker_lookup)
detected["C_ADAPTIVE_POLARIZATION"] = ok

ok, span = detect_flirt_dance_drift(CHAT_LOG, marker_lookup)
detected["MM_FLIRT_DANCE_DRIFT"] = ok

# --------------------------------------------------------------------------
# 4)  Report / Action -------------------------------------------------------
# --------------------------------------------------------------------------
if detected["MM_FLIRT_DANCE_DRIFT"]:
    print("⚠️  FLIRT-DANCE-DRIFT erkannt!")
    for msg in span:
        print(f"  {msg['ts'].strftime('%Y-%m-%d %H:%M')} – {msg['text']}")
else:
    print("Keine kritische Flirt-Drift erkannt.")

# ============================  Ende pipeline.py  ===========================

# ====================================================================
# END OF FILE: ALL_NEWMARKER01/ohne_json/_python/PYTHON_MARKER.py
# ====================================================================



